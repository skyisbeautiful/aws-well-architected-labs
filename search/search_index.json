{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Home"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"Cost/","text":"AWS Well-Architected Cost Optimization Labs Introduction This repository contains documentation and code in the format of hands-on labs to help you learn, measure, and build using architectural best practices. For more information about cost optimization on AWS visit the Well-Architected tool in the AWS console, and read the AWS Well-Architected cost optimization whitepaper. Labs: Cost Fundamentals These labs must be done in order from 1 to 5, starting at the 100 level labs. 100 Level 100 #1 AWS Account Setup 100 #2 Governance 100 #4 Cost and Usage Analysis 100 #5 Cost Visualization 200 Level 200 #2 Governance 200 #3 Pricing Models 200 #4 Cost and Usage Analysis 200 #5 Cost Visualization Cost and Usage Analysis 100 and 200 Level These labs are from the fundamentals series and must be completed before you move onto the 300 series. - 100 #4 Cost and Usage Analysis - 200 #4 Cost and Usage Analysis 300 These labs focus on specific items within cost and usage analysis and are used to acheive a specific outcome. They must be done after the 100 and 200 labs, however they can be completed independantly of each other. - 300 Billing Analysis - Automated CUR Updates and Ingestion","title":"AWS Well-Architected Cost Optimization Labs"},{"location":"Cost/#aws-well-architected-cost-optimization-labs","text":"","title":"AWS Well-Architected Cost Optimization Labs"},{"location":"Cost/#introduction","text":"This repository contains documentation and code in the format of hands-on labs to help you learn, measure, and build using architectural best practices. For more information about cost optimization on AWS visit the Well-Architected tool in the AWS console, and read the AWS Well-Architected cost optimization whitepaper.","title":"Introduction"},{"location":"Cost/#labs","text":"","title":"Labs:"},{"location":"Cost/#cost-fundamentals","text":"These labs must be done in order from 1 to 5, starting at the 100 level labs.","title":"Cost Fundamentals"},{"location":"Cost/#100-level","text":"100 #1 AWS Account Setup 100 #2 Governance 100 #4 Cost and Usage Analysis 100 #5 Cost Visualization","title":"100 Level"},{"location":"Cost/#200-level","text":"200 #2 Governance 200 #3 Pricing Models 200 #4 Cost and Usage Analysis 200 #5 Cost Visualization","title":"200 Level"},{"location":"Cost/#cost-and-usage-analysis","text":"","title":"Cost and Usage Analysis"},{"location":"Cost/#100-and-200-level","text":"These labs are from the fundamentals series and must be completed before you move onto the 300 series. - 100 #4 Cost and Usage Analysis - 200 #4 Cost and Usage Analysis","title":"100 and 200 Level"},{"location":"Cost/#300","text":"These labs focus on specific items within cost and usage analysis and are used to acheive a specific outcome. They must be done after the 100 and 200 labs, however they can be completed independantly of each other. - 300 Billing Analysis - Automated CUR Updates and Ingestion","title":"300"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/","text":"Level 100: AWS Account Setup Introduction This hands-on lab will guide you through the steps to create and setup an initial account structure, and enable access to billing reports. This will ensure that you can complete the Well-Architected Cost workshops, and enable you to optimize your workloads inline with the Well-Architected Framework. Goals Implement an account structure Configure billing services Prerequisites Multiple AWS accounts (at least two) Root user access to the master account Overview Lab Guide.md the guide for this lab Checklist.md best practice checklist related to this lab /Code including CloudFormation templates related to this lab /Images referenced by this lab Permissions required Root user access to the master account ./Code/master_policy IAM policy required for Master account user ./Code/member_policy IAM policy required for Member account user NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required. License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 100: AWS Account Setup"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/#level-100-aws-account-setup","text":"","title":"Level 100: AWS Account Setup"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/#introduction","text":"This hands-on lab will guide you through the steps to create and setup an initial account structure, and enable access to billing reports. This will ensure that you can complete the Well-Architected Cost workshops, and enable you to optimize your workloads inline with the Well-Architected Framework.","title":"Introduction"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/#goals","text":"Implement an account structure Configure billing services","title":"Goals"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/#prerequisites","text":"Multiple AWS accounts (at least two) Root user access to the master account","title":"Prerequisites"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/#overview","text":"Lab Guide.md the guide for this lab Checklist.md best practice checklist related to this lab /Code including CloudFormation templates related to this lab /Images referenced by this lab","title":"Overview"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/#permissions-required","text":"Root user access to the master account ./Code/master_policy IAM policy required for Master account user ./Code/member_policy IAM policy required for Member account user NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required.","title":"Permissions required"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Checklist/","text":"Level 100: AWS Account Setup: Best Practice Checklist [ ] Create a basic account structure, with a master (payer) account and at least 1 member (linked) account [ ] Configure account parameters [ ] Configure IAM access to billing information [ ] Configure a Cost and Usage Report (CUR) [ ] Enable AWS Cost Explorer [ ] Enable AWS-Generated Cost Allocation Tags","title":"Level 100: AWS Account Setup: Best Practice Checklist"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Checklist/#level-100-aws-account-setup-best-practice-checklist","text":"[ ] Create a basic account structure, with a master (payer) account and at least 1 member (linked) account [ ] Configure account parameters [ ] Configure IAM access to billing information [ ] Configure a Cost and Usage Report (CUR) [ ] Enable AWS Cost Explorer [ ] Enable AWS-Generated Cost Allocation Tags","title":"Level 100: AWS Account Setup: Best Practice Checklist"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab Guide/","text":"Level 100: AWS Account Setup: Lab Guide Authors Nathan Besh, Cost Lead Well-Architected Spencer Marley, Commercial Architect Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Table of Contents Configure IAM access Create an account structure Configure account settings Configure Cost and Usage reports Enable AWS Cost Explorer Enable AWS-Generated Cost Allocation Tags Tear down Feedback survey 1. Configure IAM access to your billing NOTE : You will need to sign into the account with root account credentials to perform this action. You need to enter in the account email and password for root access. You need to enable IAM access to your billing so the correct IAM users can access the information. This allows other users (non-root) to access billing information in the master account. It is also required if you wish for member accounts to see their usage and billing information. This step will not provide access to the information, that is configured through IAM policies. Log in to your Master account as the root user, Click on the account name in the top right, and click on My Account from the menu: Scroll down to IAM User and Role Access to Billing Information , and click Edit : Select Activeate IAM Access and click on Update : Confirm that IAM user/role access to billing information is activated : You will now be able to provide access to non-root users to billing information via IAM policies. NOTE: Logout as the root user before continuing. 2. Create an account structure NOTE : Do NOT do this step if you already have an orgnization and consolidated billing setup. You will create an AWS Organization, and join one or more accounts to te master account. An organization will allow you to centrally manage multilpe AWS accounts efficiently and consistently. It is recommended to have a master account that is primarily used for billing and does not contain any resources, all resources and workloads will reside in the member accounts. You will need organizations:CreateOrganization access, and 2 or more AWS accounts. When you create a new master account, it will contain all billing information for member accounts, member accounts will no longer have any billing information, including historical billing information. Ensure you backup or export any reports or data. 2.1 Create an AWS Organization You will create an AWS Organization with the master account. Login to the AWS console as an IAM user with the required permissions, start typing AWS Organizations into the Find Services box and click on AWS Organizations : Click on Create organization : To create a fully featured organization, Click on Create organization You will receive a verification email, click on Verify your email address to verify your account: You will then see a verification message in the console for your organization: You now have an organization that you can join other accounts to. 2.2 Join member accounts You will now join other accounts to your organization. From the AWS Organizations console click on Add account : Click on Invite account : Enter in the Email or account ID , enter in any relevant Notes and click Invite : You will then have an open request: Log in to your member account , and go to AWS Organizations : You will see an invitation in the menu, click on Invitations : Verify the details in the request (they are blacked out here), and click on Accept : Verify the Organization ID (blacked out here), and click Confirm : You are shown that the account is now part of your organization: The member account will receive an email showing success: The master account will also receive email notification of success: Repeat the steps above (exercise 1.2) for each additional account in your organization. 3. Configure billing account settings It is important to ensure your account contacts are up to date and correct. This allows AWS to be able to contact the correct people in your organization if required. It is recommended to use a mailing list or shared email that is accessible by muptile team members for redudancy. Ensure the email accounts are actively monitored. Log in to your Master account as an IAM user with the required permissions, Click on the account name in the top right, and click on My Account from the menu: Scroll down to Alternate Contacts and click on Edit : Enter information into each of the fields for Billing , Operations and Security , and click Update : 4. Configure Cost and Usage Reports Cost and Usage Reports provide the most detailed information on your usage and bills. They can be configured to deliver 1 line per resource, for every hour of the day. They must be configured to enable you to access and analyze your usage and billing information. This will allow you to make modifications to your usage, and make your applications more efficient. 4.1 Configure a Cost and Usage Report If you configure multilpe Cost and Usage Reports (CURs), then it is recommended to have 1 CUR per bucket. If you must have multilpe CURs in a single bucket, ensure you use a different report path prefix so it is clear they are different reports. Log in to your Master account as an IAM user with the required permissions, and go to the Billing console: Select Cost Usage Reports from the left menu: Click on Create report : Enter a Report name (it can be any name), ensure you have selected Include resource IDs and Data refresh settings , then click on Next : Click on Configure : Enter a unique bucket name, and ensure the region is correct, click Next : Read and verify the policy, this will allow AWS to deliver billing reports to the bucket. Click on I have confirmed that this policy is correct , then click Save : Esure your bucket is a Valid Bucket (if not, verify the bucket policy). Enter a Report path prefix (it can be any word) without any '/' characters, ensure the Time Granularity is Hourly , Report Versioning is set to Overwrite existing report , under Enable report data integration for select Amazon Athena , and click Next : Review the configuration, scroll to the bottom and click on Review and Complete : You have successfully configured a Cost and Usage Report to be delivered. It may take up to 24hrs for the first report to be delivered. 4.2 Enable monthly billing report The monthly billing report contains estimated AWS charges for the month. It contains line items for each unique combination of AWS product, usage type, and operation that the account uses. Go to the billng console: Click on Billing preferences from the left menu: Scroll down, and click on Receive Billing Reports , then click on Configure : From the left dropdown, select your S3 billing bucket configured above: Click on Next : Read and verify the policy, this will allow AWS to deliver billing reports to the bucket. Click on I have confirmed that this policy is correct , then click Save : Ensure only Monthly report is selected, and uncheck all other boxes. Click on Save preferences : 5. Enable AWS Cost Explorer AWS Cost Explorer has an easy-to-use interface that lets you visualize, understand, and manage your AWS costs and usage over time. You must enable it before you can use it within your accounts. Log in to your Master account as an IAM user with the required permissions, and go to the Billing console: Select Cost Explorer from the left menu: Click on Enable Cost Explorer : You will receive notification that Cost Explorer has been enabled, and data will be populated: 6. Enable AWS-Generated Cost Allocation Tags Enabling AWS-Generated Cost Allocation Tags, generates a cost allocation tag containing resource creator information that is automatically applied to resources that are created within your account. This allows you to view and allocate costs based on who created a resource. Log in to your Master account as an IAM user with the required permissions, and go to the Billing console: Select Cost Allocation Tags from the left menu: Click on Activate to enable the tags: You will see that it is activated: 7. Tear down This exercise covered fundamental steps that are recommended for all AWS accounts to enable Cost Optimization. There is no tear down for exercises in this lab. Ensure you remove the IAM policies from the users/groups if they were used. 8. Survey Thanks for taking the lab, We hope that you can take this short survey ( 2 minutes), to share your insights and help us improve our content. This survey is hosted by an external company (Qualtrics) , so the link above does not lead to our website. Please note that AWS will own the data gathered via this survey and will not share the information/results collected with survey respondents. Your responses to this survey will be subject to Amazons Privacy Policy.","title":"Level 100: AWS Account Setup: Lab Guide"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab Guide/#level-100-aws-account-setup-lab-guide","text":"","title":"Level 100: AWS Account Setup: Lab Guide"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab Guide/#authors","text":"Nathan Besh, Cost Lead Well-Architected Spencer Marley, Commercial Architect","title":"Authors"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab Guide/#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab Guide/#table-of-contents","text":"Configure IAM access Create an account structure Configure account settings Configure Cost and Usage reports Enable AWS Cost Explorer Enable AWS-Generated Cost Allocation Tags Tear down Feedback survey","title":"Table of Contents"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab Guide/#1-configure-iam-access-to-your-billing","text":"NOTE : You will need to sign into the account with root account credentials to perform this action. You need to enter in the account email and password for root access. You need to enable IAM access to your billing so the correct IAM users can access the information. This allows other users (non-root) to access billing information in the master account. It is also required if you wish for member accounts to see their usage and billing information. This step will not provide access to the information, that is configured through IAM policies. Log in to your Master account as the root user, Click on the account name in the top right, and click on My Account from the menu: Scroll down to IAM User and Role Access to Billing Information , and click Edit : Select Activeate IAM Access and click on Update : Confirm that IAM user/role access to billing information is activated : You will now be able to provide access to non-root users to billing information via IAM policies. NOTE: Logout as the root user before continuing.","title":"1. Configure IAM access to your billing"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab Guide/#2-create-an-account-structure","text":"NOTE : Do NOT do this step if you already have an orgnization and consolidated billing setup. You will create an AWS Organization, and join one or more accounts to te master account. An organization will allow you to centrally manage multilpe AWS accounts efficiently and consistently. It is recommended to have a master account that is primarily used for billing and does not contain any resources, all resources and workloads will reside in the member accounts. You will need organizations:CreateOrganization access, and 2 or more AWS accounts. When you create a new master account, it will contain all billing information for member accounts, member accounts will no longer have any billing information, including historical billing information. Ensure you backup or export any reports or data.","title":"2. Create an account structure"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab Guide/#21-create-an-aws-organization","text":"You will create an AWS Organization with the master account. Login to the AWS console as an IAM user with the required permissions, start typing AWS Organizations into the Find Services box and click on AWS Organizations : Click on Create organization : To create a fully featured organization, Click on Create organization You will receive a verification email, click on Verify your email address to verify your account: You will then see a verification message in the console for your organization: You now have an organization that you can join other accounts to.","title":"2.1 Create an AWS Organization"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab Guide/#22-join-member-accounts","text":"You will now join other accounts to your organization. From the AWS Organizations console click on Add account : Click on Invite account : Enter in the Email or account ID , enter in any relevant Notes and click Invite : You will then have an open request: Log in to your member account , and go to AWS Organizations : You will see an invitation in the menu, click on Invitations : Verify the details in the request (they are blacked out here), and click on Accept : Verify the Organization ID (blacked out here), and click Confirm : You are shown that the account is now part of your organization: The member account will receive an email showing success: The master account will also receive email notification of success: Repeat the steps above (exercise 1.2) for each additional account in your organization.","title":"2.2 Join member accounts"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab Guide/#3-configure-billing-account-settings","text":"It is important to ensure your account contacts are up to date and correct. This allows AWS to be able to contact the correct people in your organization if required. It is recommended to use a mailing list or shared email that is accessible by muptile team members for redudancy. Ensure the email accounts are actively monitored. Log in to your Master account as an IAM user with the required permissions, Click on the account name in the top right, and click on My Account from the menu: Scroll down to Alternate Contacts and click on Edit : Enter information into each of the fields for Billing , Operations and Security , and click Update :","title":"3. Configure billing account settings"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab Guide/#4-configure-cost-and-usage-reports","text":"Cost and Usage Reports provide the most detailed information on your usage and bills. They can be configured to deliver 1 line per resource, for every hour of the day. They must be configured to enable you to access and analyze your usage and billing information. This will allow you to make modifications to your usage, and make your applications more efficient.","title":"4. Configure Cost and Usage Reports"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab Guide/#41-configure-a-cost-and-usage-report","text":"If you configure multilpe Cost and Usage Reports (CURs), then it is recommended to have 1 CUR per bucket. If you must have multilpe CURs in a single bucket, ensure you use a different report path prefix so it is clear they are different reports. Log in to your Master account as an IAM user with the required permissions, and go to the Billing console: Select Cost Usage Reports from the left menu: Click on Create report : Enter a Report name (it can be any name), ensure you have selected Include resource IDs and Data refresh settings , then click on Next : Click on Configure : Enter a unique bucket name, and ensure the region is correct, click Next : Read and verify the policy, this will allow AWS to deliver billing reports to the bucket. Click on I have confirmed that this policy is correct , then click Save : Esure your bucket is a Valid Bucket (if not, verify the bucket policy). Enter a Report path prefix (it can be any word) without any '/' characters, ensure the Time Granularity is Hourly , Report Versioning is set to Overwrite existing report , under Enable report data integration for select Amazon Athena , and click Next : Review the configuration, scroll to the bottom and click on Review and Complete : You have successfully configured a Cost and Usage Report to be delivered. It may take up to 24hrs for the first report to be delivered.","title":"4.1 Configure a Cost and Usage Report"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab Guide/#42-enable-monthly-billing-report","text":"The monthly billing report contains estimated AWS charges for the month. It contains line items for each unique combination of AWS product, usage type, and operation that the account uses. Go to the billng console: Click on Billing preferences from the left menu: Scroll down, and click on Receive Billing Reports , then click on Configure : From the left dropdown, select your S3 billing bucket configured above: Click on Next : Read and verify the policy, this will allow AWS to deliver billing reports to the bucket. Click on I have confirmed that this policy is correct , then click Save : Ensure only Monthly report is selected, and uncheck all other boxes. Click on Save preferences :","title":"4.2 Enable monthly billing report"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab Guide/#5-enable-aws-cost-explorer","text":"AWS Cost Explorer has an easy-to-use interface that lets you visualize, understand, and manage your AWS costs and usage over time. You must enable it before you can use it within your accounts. Log in to your Master account as an IAM user with the required permissions, and go to the Billing console: Select Cost Explorer from the left menu: Click on Enable Cost Explorer : You will receive notification that Cost Explorer has been enabled, and data will be populated:","title":"5. Enable AWS Cost Explorer"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab Guide/#6-enable-aws-generated-cost-allocation-tags","text":"Enabling AWS-Generated Cost Allocation Tags, generates a cost allocation tag containing resource creator information that is automatically applied to resources that are created within your account. This allows you to view and allocate costs based on who created a resource. Log in to your Master account as an IAM user with the required permissions, and go to the Billing console: Select Cost Allocation Tags from the left menu: Click on Activate to enable the tags: You will see that it is activated:","title":"6. Enable AWS-Generated Cost Allocation Tags"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab Guide/#7-tear-down","text":"This exercise covered fundamental steps that are recommended for all AWS accounts to enable Cost Optimization. There is no tear down for exercises in this lab. Ensure you remove the IAM policies from the users/groups if they were used.","title":"7. Tear down"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab Guide/#8-survey","text":"Thanks for taking the lab, We hope that you can take this short survey ( 2 minutes), to share your insights and help us improve our content. This survey is hosted by an external company (Qualtrics) , so the link above does not lead to our website. Please note that AWS will own the data gathered via this survey and will not share the information/results collected with survey respondents. Your responses to this survey will be subject to Amazons Privacy Policy.","title":"8. Survey "},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/","text":"Level 100: Cost and Usage Governance Introduction This hands-on lab will guide you through the steps to implement cost and usage governance. The skills you learn will help you control your cost and usage in alignment with your business requirements. Goals Create a Cost Optimization team to monitor usage and cost Implement AWS Budgets to notify on usage and spend Prerequisites An AWS Account AWS Account Setup has been completed Overview Lab Guide.md the guide for this lab Checklist.md best practice checklist related to this lab /Code Code related to this lab /Images referenced by this lab Permissions required ./Code/IAM_policy IAM policy required for this lab License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 100: Cost and Usage Governance"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/#level-100-cost-and-usage-governance","text":"","title":"Level 100: Cost and Usage Governance"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/#introduction","text":"This hands-on lab will guide you through the steps to implement cost and usage governance. The skills you learn will help you control your cost and usage in alignment with your business requirements.","title":"Introduction"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/#goals","text":"Create a Cost Optimization team to monitor usage and cost Implement AWS Budgets to notify on usage and spend","title":"Goals"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/#prerequisites","text":"An AWS Account AWS Account Setup has been completed","title":"Prerequisites"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/#overview","text":"Lab Guide.md the guide for this lab Checklist.md best practice checklist related to this lab /Code Code related to this lab /Images referenced by this lab","title":"Overview"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/#permissions-required","text":"./Code/IAM_policy IAM policy required for this lab","title":"Permissions required"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Checklist/","text":"Level 100: Cost and Usage Governance [ ] Create a cost optimizaion team, to manage cost optimization across your organization [ ] Create an AWS budget to notify on forecasted account cost [ ] Create an AWS budget to notify on actual cost of EC2 [ ] Create an AWS budget to notify on RI Coverage","title":"Level 100: Cost and Usage Governance"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Checklist/#level-100-cost-and-usage-governance","text":"[ ] Create a cost optimizaion team, to manage cost optimization across your organization [ ] Create an AWS budget to notify on forecasted account cost [ ] Create an AWS budget to notify on actual cost of EC2 [ ] Create an AWS budget to notify on RI Coverage","title":"Level 100: Cost and Usage Governance"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab Guide/","text":"Level 100: Cost and Usage Governance Authors Nathan Besh, Cost Lead Well-Architected Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Table of Contents Create a cost optimization team Create an AWS Budget - monthly forecast Create an AWS Budget - EC2 actual Create an AWS Budget - RI Coverage Tear down Feedback survey 1. Create a cost optimization team We are going to create a cost optimization team. Within your organization there needs to be a team of people that are focused around costs and usage. This exercise will create the users and the group, then assign all the access they need. This team will then be able to manage the organizations cost and usage, and start to implement optimization mechanisms. Log into the console as an IAM user with the required permissions, as per: - ./Code/IAM_policy IAM policy required for this lab 1.1 Create an IAM policy for the team This provides access to allow the cost optimization team to perform their work, namely the Labs in the 100 level fundamental series. This is the minimum access the team requires. Log in and go to the IAM Service page: Select Policies from the left menu: Select Create Policy : Select the JSON tab: Copy paste the following policy into the the field: NOTE : Ensure you copy the entire policy, everything including the first '{' and last '}' { Version : 2012-10-17 , Statement : [ { Sid : VisualEditor0 , Effect : Allow , Action : [ aws-portal:ViewUsage , aws-portal:ModifyBilling , aws-portal:ViewBilling , aws-portal:ViewAccount , budgets:* ], Resource : * } ] } Click Review policy : Enter a Name and Description for the policy and click Create policy : You have successfully created the cost optimization teams policy. 1.2 Create an IAM Group This group will bring together IAM users and apply the required policies. While in the IAM console, select Groups from the left menu: Click on Create New Group : Enter a Group Name and click Next Step : Click Policy Type and select Customer Managed : Select the CostOptimization_Summit policy (created previously): Click Create Group : You have now successfully created the cost optimization group, and attached the required policies. 1.3 Create an IAM User For this lab we will create a user and join them to the group above. In the IAM console, select Users from the left menu: Click Add user : Enter a User name , select AWS Management Console access , choose Custom Password , type a suitable password, deselect Require password reset , and click Next: Permissions : Select the CostOptimization group (created previously), and click Next: Tags : Click Next Review : Click Create user : Copy the link provided, and logout by clicking on your username in the top right, and selecting Sign Out :: Log back in as the username you just created, with the link you copied for the remainder of the Lab. You have successfully create a user, placed them in the cost optimization group and have applied policies. You can continue to expand this group by adding additional users from your organization. 2. Create and implement an AWS Budget for monthly forecasted usage Budgets allow you to manage cost and usage by providing notifications when usage or cost are outside of configured amounts. They cannot be used to restrict actions, only notify on usage after it has occurred. Create a monthly cost budget for your account We will create a monthly cost budget which will notify if the forecasted amount exceeds the budget. Go to the Billing console : Select Budgets from the left menu: Click on Create a budget : Ensure Cost Budget is selected, and click Set your budget : Create a cost budget, enter the following details: Name : (enter a name), Budgeted amount : (enter an amount a lot LESS than last months cost), Budget effective dates : Select Recurring Budget and start month is the current month, Other fields: leave as defaults: Scroll down and click Configure alerts : Select: Send alert based on : Forecasted Costs Alert threshold : 100% of budgeted amount Email contacts : (your email address) Click on Confirm budget : Review the configuration, and click Create : You can see the current forecast will exceed the budget (it is red, you may need to refresh your browser): 10: You will receive an email similar to this within a few minutes: You have created a forecasted budget, when your forecasted costs for the entire account are predicted to exceed the forecast, you will receive a notification. You can also create an actual budget, for when your current costs actually exceed a defined amount. 3. Create and implement an AWS Budget for EC2 actual cost We will create a monthly EC2 actual cost budget, which will notify if the actual costs of EC2 instances exceeds the specified amount. Click Create budget : Select Cost budget , and click Set your budget : Create a cost budget, enter the following details: Name : (enter a name), Budgeted amount : (enter an amount a lot LESS than last months cost), Budget effective dates : Select Recurring Budget and start month is the current month, Other fields: leave a defaults Under FILTERING click on Service: Type Elastic in the search field, then select the checkbox next to EC2-Instances(Elastic Compute Cloud - Compute) and Click Apply filters : De-select Upfront reservation fees , and click Configure alerts : Select: Send alert based on : Actual Costs Alert threshold : 100% of budgeted amount Email contacts : (your email address) Click on Confirm budget : Review the configuration, and click Create : You can see the current amount exceeds the budget (it is red, you may need to refresh your browser): You will receive an email similar to the previous budget within a few minutes. You have created an actual cost budget for EC2 usage. You can extend this budget by adding specific filters such as linked accounts, tags or instance types. You can also create budgets for other services than EC2. 4. Create and implement an AWS Budget for EC2 Instance RI coverage We will create a monthly RI coverage budget which will notify if the coverage of Reserved Instances for EC2 is below the specified amount. Click Create budget : Select Reservation budget , and click Set your budget : For Reservation budget type Select RI Coverage , enter a Name , select EC2-Instances as the Service , enter a Coverage threshold of 80% and click Configure alerts : Enter an address for Email contacts and click Confirm budget : Review the configuration, and click Create in the lower right: You have created an RI Coverage budget. High coverage is critical for cost optimization, as it ensures you are paying the lowest price for your resources. You will receive an email similar to this within a few minutes: 5. Tear down NOTE: The cost optimization user, group and policies are required for the completion of the fundamental labs. If you remove these resources you will not be able to complete the labs. There is no tear down for this component as it is best practices to have this group created in all organizations. Delete a budget We will delete all three budgets that were configured in sections 2,3 and 4. From the budgets homepage, click on the budget name CostBudget1 : Click on the 3 dot menu in the top right, select Delete : Click on the other budget name EC2_actual : Click on the 3 dot menu in the top right, select Delete : Click on the other budget name EC2_RI_Coverage : Click on the 3 dot menu in the top right, select Delete : ALl budgets should be deleted that were created in this workshop: 6. Survey Thanks for taking the lab, We hope that you can take this short survey ( 2 minutes), to share your insights and help us improve our content. This survey is hosted by an external company (Qualtrics) , so the link above does not lead to our website. Please note that AWS will own the data gathered via this survey and will not share the information/results collected with survey respondents. Your responses to this survey will be subject to Amazons Privacy Policy.","title":"Level 100: Cost and Usage Governance"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab Guide/#level-100-cost-and-usage-governance","text":"","title":"Level 100: Cost and Usage Governance"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab Guide/#authors","text":"Nathan Besh, Cost Lead Well-Architected","title":"Authors"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab Guide/#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab Guide/#table-of-contents","text":"Create a cost optimization team Create an AWS Budget - monthly forecast Create an AWS Budget - EC2 actual Create an AWS Budget - RI Coverage Tear down Feedback survey","title":"Table of Contents"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab Guide/#1-create-a-cost-optimization-team","text":"We are going to create a cost optimization team. Within your organization there needs to be a team of people that are focused around costs and usage. This exercise will create the users and the group, then assign all the access they need. This team will then be able to manage the organizations cost and usage, and start to implement optimization mechanisms. Log into the console as an IAM user with the required permissions, as per: - ./Code/IAM_policy IAM policy required for this lab","title":"1. Create a cost optimization team "},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab Guide/#11-create-an-iam-policy-for-the-team","text":"This provides access to allow the cost optimization team to perform their work, namely the Labs in the 100 level fundamental series. This is the minimum access the team requires. Log in and go to the IAM Service page: Select Policies from the left menu: Select Create Policy : Select the JSON tab: Copy paste the following policy into the the field: NOTE : Ensure you copy the entire policy, everything including the first '{' and last '}' { Version : 2012-10-17 , Statement : [ { Sid : VisualEditor0 , Effect : Allow , Action : [ aws-portal:ViewUsage , aws-portal:ModifyBilling , aws-portal:ViewBilling , aws-portal:ViewAccount , budgets:* ], Resource : * } ] } Click Review policy : Enter a Name and Description for the policy and click Create policy : You have successfully created the cost optimization teams policy.","title":"1.1 Create an IAM policy for the team"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab Guide/#12-create-an-iam-group","text":"This group will bring together IAM users and apply the required policies. While in the IAM console, select Groups from the left menu: Click on Create New Group : Enter a Group Name and click Next Step : Click Policy Type and select Customer Managed : Select the CostOptimization_Summit policy (created previously): Click Create Group : You have now successfully created the cost optimization group, and attached the required policies.","title":"1.2 Create an IAM Group"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab Guide/#13-create-an-iam-user","text":"For this lab we will create a user and join them to the group above. In the IAM console, select Users from the left menu: Click Add user : Enter a User name , select AWS Management Console access , choose Custom Password , type a suitable password, deselect Require password reset , and click Next: Permissions : Select the CostOptimization group (created previously), and click Next: Tags : Click Next Review : Click Create user : Copy the link provided, and logout by clicking on your username in the top right, and selecting Sign Out :: Log back in as the username you just created, with the link you copied for the remainder of the Lab. You have successfully create a user, placed them in the cost optimization group and have applied policies. You can continue to expand this group by adding additional users from your organization.","title":"1.3 Create an IAM User"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab Guide/#2-create-and-implement-an-aws-budget-for-monthly-forecasted-usage","text":"Budgets allow you to manage cost and usage by providing notifications when usage or cost are outside of configured amounts. They cannot be used to restrict actions, only notify on usage after it has occurred.","title":"2. Create and implement an AWS Budget for monthly forecasted usage"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab Guide/#create-a-monthly-cost-budget-for-your-account","text":"We will create a monthly cost budget which will notify if the forecasted amount exceeds the budget. Go to the Billing console : Select Budgets from the left menu: Click on Create a budget : Ensure Cost Budget is selected, and click Set your budget : Create a cost budget, enter the following details: Name : (enter a name), Budgeted amount : (enter an amount a lot LESS than last months cost), Budget effective dates : Select Recurring Budget and start month is the current month, Other fields: leave as defaults: Scroll down and click Configure alerts : Select: Send alert based on : Forecasted Costs Alert threshold : 100% of budgeted amount Email contacts : (your email address) Click on Confirm budget : Review the configuration, and click Create : You can see the current forecast will exceed the budget (it is red, you may need to refresh your browser): 10: You will receive an email similar to this within a few minutes: You have created a forecasted budget, when your forecasted costs for the entire account are predicted to exceed the forecast, you will receive a notification. You can also create an actual budget, for when your current costs actually exceed a defined amount.","title":"Create a monthly cost budget for your account"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab Guide/#3-create-and-implement-an-aws-budget-for-ec2-actual-cost","text":"We will create a monthly EC2 actual cost budget, which will notify if the actual costs of EC2 instances exceeds the specified amount. Click Create budget : Select Cost budget , and click Set your budget : Create a cost budget, enter the following details: Name : (enter a name), Budgeted amount : (enter an amount a lot LESS than last months cost), Budget effective dates : Select Recurring Budget and start month is the current month, Other fields: leave a defaults Under FILTERING click on Service: Type Elastic in the search field, then select the checkbox next to EC2-Instances(Elastic Compute Cloud - Compute) and Click Apply filters : De-select Upfront reservation fees , and click Configure alerts : Select: Send alert based on : Actual Costs Alert threshold : 100% of budgeted amount Email contacts : (your email address) Click on Confirm budget : Review the configuration, and click Create : You can see the current amount exceeds the budget (it is red, you may need to refresh your browser): You will receive an email similar to the previous budget within a few minutes. You have created an actual cost budget for EC2 usage. You can extend this budget by adding specific filters such as linked accounts, tags or instance types. You can also create budgets for other services than EC2.","title":"3. Create and implement an AWS Budget for EC2 actual cost"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab Guide/#4-create-and-implement-an-aws-budget-for-ec2-instance-ri-coverage","text":"We will create a monthly RI coverage budget which will notify if the coverage of Reserved Instances for EC2 is below the specified amount. Click Create budget : Select Reservation budget , and click Set your budget : For Reservation budget type Select RI Coverage , enter a Name , select EC2-Instances as the Service , enter a Coverage threshold of 80% and click Configure alerts : Enter an address for Email contacts and click Confirm budget : Review the configuration, and click Create in the lower right: You have created an RI Coverage budget. High coverage is critical for cost optimization, as it ensures you are paying the lowest price for your resources. You will receive an email similar to this within a few minutes:","title":"4. Create and implement an AWS Budget for EC2 Instance RI coverage"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab Guide/#5-tear-down","text":"NOTE: The cost optimization user, group and policies are required for the completion of the fundamental labs. If you remove these resources you will not be able to complete the labs. There is no tear down for this component as it is best practices to have this group created in all organizations.","title":"5. Tear down "},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab Guide/#delete-a-budget","text":"We will delete all three budgets that were configured in sections 2,3 and 4. From the budgets homepage, click on the budget name CostBudget1 : Click on the 3 dot menu in the top right, select Delete : Click on the other budget name EC2_actual : Click on the 3 dot menu in the top right, select Delete : Click on the other budget name EC2_RI_Coverage : Click on the 3 dot menu in the top right, select Delete : ALl budgets should be deleted that were created in this workshop:","title":"Delete a budget"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab Guide/#6-survey","text":"Thanks for taking the lab, We hope that you can take this short survey ( 2 minutes), to share your insights and help us improve our content. This survey is hosted by an external company (Qualtrics) , so the link above does not lead to our website. Please note that AWS will own the data gathered via this survey and will not share the information/results collected with survey respondents. Your responses to this survey will be subject to Amazons Privacy Policy.","title":"6. Survey "},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/","text":"Level 100: Cost and Usage Analysis Introduction This hands-on lab will guide you through the steps to perform analysis of your AWS cost and usage. The skills you learn will help you monitor your cost and usage, in alignment with the AWS Well-Architected Framework. Goals Perform basic analysis of your cost and usage Prerequisites A master AWS Account A linked AWS Account (preferred, not mandatory) Completed all previous labs in the Cost Fundamentals series Overview Lab Guide.md the guide for this lab Checklist.md best practice checklist related to this lab /Code including CloudFormation templates related to this lab /Images referenced by this lab Permissions required Log in as the Cost Optimization team, created in Cost_and_Usage_Governance NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required. License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 100: Cost and Usage Analysis"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/#level-100-cost-and-usage-analysis","text":"","title":"Level 100: Cost and Usage Analysis"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/#introduction","text":"This hands-on lab will guide you through the steps to perform analysis of your AWS cost and usage. The skills you learn will help you monitor your cost and usage, in alignment with the AWS Well-Architected Framework.","title":"Introduction"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/#goals","text":"Perform basic analysis of your cost and usage","title":"Goals"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/#prerequisites","text":"A master AWS Account A linked AWS Account (preferred, not mandatory) Completed all previous labs in the Cost Fundamentals series","title":"Prerequisites"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/#overview","text":"Lab Guide.md the guide for this lab Checklist.md best practice checklist related to this lab /Code including CloudFormation templates related to this lab /Images referenced by this lab","title":"Overview"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/#permissions-required","text":"Log in as the Cost Optimization team, created in Cost_and_Usage_Governance NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required.","title":"Permissions required"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/Checklist/","text":"Level 100: Cost and Usage Analysis [ ] View your AWS Invoices [ ] View your cost and usage in detail through the console [ ] Download your monthly cost and usage CSV file","title":"Level 100: Cost and Usage Analysis"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/Checklist/#level-100-cost-and-usage-analysis","text":"[ ] View your AWS Invoices [ ] View your cost and usage in detail through the console [ ] Download your monthly cost and usage CSV file","title":"Level 100: Cost and Usage Analysis"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/Lab Guide/","text":"Level 100: Cost and Usage Analysis Authors Nathan Besh, Cost Lead Well-Architected Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Table of Contents View your AWS Invoices View your cost and usage in detail Download your monthly cost and usage file Tear down Survey 1. View your AWS Invoices At the end of a billing cycle or at the time you choose to incur a one-time fee, AWS charges the payment method you have and issues your invoice as a PDF file. You can view these invoices through the AWS console, which will show summary information of all usage and cost incurred for that one off item, or billing period. Log into the console as an IAM user with the required permissions, go to the billing dashboard: Select Payment History from the menu on the left: Click on an Invoice/Receipt ID corresponding to the month you wish to view: It will download a PDF version of your invoice similar to below: 2. View your cost and usage in detail You can view past and present costs and usage through the console, which also provides more detailed information on cost and usage. We will go through accessing your cost and usage by service, and by linked account (if applicable). We will then drill down into a specific service. Go to the billing dashboard: Click on Bills from the left menu: Select the Date you require from the drop down menu, by clicking on the menu item: You will be shown Bill details by service , where you can dynamically drill down into the specific service cost and usage. Pick your largest cost service and look into the region and line items: Select Bill details by account to see cost and usage for each account separately. Select the Account name , then drill down into the specific service cost and usage: 3. Download your monthly cost and usage file It is possible to download a CSV version of your summary cost and usage information. This can be accessed by a spreadsheet application for ease of use. We will download your monthly usage file and view it. Go to the billing dashboard: Click on Bills from the left menu: Select the Date you require from the drop down menu, by clicking on the menu item: Click on Download CSV : It will download a CSV version of the bill you can use in a spreadsheet application. It is recommended to NOT use this data source for calculations and analysis, instead you should use the Cost and Usage Report, which is covered in 200_4_Billing ansalysis . 4. Tear down There is no configuration performed within this lab, so no teardown is required. 5. Survey Thanks for taking the lab, We hope that you can take this short survey ( 2 minutes), to share your insights and help us improve our content. This survey is hosted by an external company (Qualtrics) , so the link above does not lead to our website. Please note that AWS will own the data gathered via this survey and will not share the information/results collected with survey respondents. Your responses to this survey will be subject to Amazon\u2019s Privacy Policy.","title":"Level 100: Cost and Usage Analysis"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/Lab Guide/#level-100-cost-and-usage-analysis","text":"","title":"Level 100: Cost and Usage Analysis"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/Lab Guide/#authors","text":"Nathan Besh, Cost Lead Well-Architected","title":"Authors"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/Lab Guide/#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/Lab Guide/#table-of-contents","text":"View your AWS Invoices View your cost and usage in detail Download your monthly cost and usage file Tear down Survey","title":"Table of Contents"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/Lab Guide/#1-view-your-aws-invoices","text":"At the end of a billing cycle or at the time you choose to incur a one-time fee, AWS charges the payment method you have and issues your invoice as a PDF file. You can view these invoices through the AWS console, which will show summary information of all usage and cost incurred for that one off item, or billing period. Log into the console as an IAM user with the required permissions, go to the billing dashboard: Select Payment History from the menu on the left: Click on an Invoice/Receipt ID corresponding to the month you wish to view: It will download a PDF version of your invoice similar to below:","title":"1. View your AWS Invoices "},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/Lab Guide/#2-view-your-cost-and-usage-in-detail","text":"You can view past and present costs and usage through the console, which also provides more detailed information on cost and usage. We will go through accessing your cost and usage by service, and by linked account (if applicable). We will then drill down into a specific service. Go to the billing dashboard: Click on Bills from the left menu: Select the Date you require from the drop down menu, by clicking on the menu item: You will be shown Bill details by service , where you can dynamically drill down into the specific service cost and usage. Pick your largest cost service and look into the region and line items: Select Bill details by account to see cost and usage for each account separately. Select the Account name , then drill down into the specific service cost and usage:","title":"2. View your cost and usage in detail"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/Lab Guide/#3-download-your-monthly-cost-and-usage-file","text":"It is possible to download a CSV version of your summary cost and usage information. This can be accessed by a spreadsheet application for ease of use. We will download your monthly usage file and view it. Go to the billing dashboard: Click on Bills from the left menu: Select the Date you require from the drop down menu, by clicking on the menu item: Click on Download CSV : It will download a CSV version of the bill you can use in a spreadsheet application. It is recommended to NOT use this data source for calculations and analysis, instead you should use the Cost and Usage Report, which is covered in 200_4_Billing ansalysis .","title":"3. Download your monthly cost and usage file"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/Lab Guide/#4-tear-down","text":"There is no configuration performed within this lab, so no teardown is required.","title":"4. Tear down"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/Lab Guide/#5-survey","text":"Thanks for taking the lab, We hope that you can take this short survey ( 2 minutes), to share your insights and help us improve our content. This survey is hosted by an external company (Qualtrics) , so the link above does not lead to our website. Please note that AWS will own the data gathered via this survey and will not share the information/results collected with survey respondents. Your responses to this survey will be subject to Amazon\u2019s Privacy Policy.","title":"5. Survey "},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/","text":"Level 100: Billing Visualization Introduction This hands-on lab will guide you through the steps to perform visualization of your AWS cost and usage. The skills you learn will help you monitor your cost and usage, in alignment with the AWS Well-Architected Framework. Goals Perform basic analysis of your cost and usage Prerequisites A master AWS Account A linked AWS Account (preferred, not mandatory) Completed all previous labs in the Cost Fundamentals series Overview Lab Guide.md the guide for this lab Checklist.md best practice checklist related to this lab /Code including CloudFormation templates related to this lab /Images referenced by this lab Permissions required Log in as the Cost Optimization team, created in Cost_and_Usage_Governance NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required. License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 100: Billing Visualization"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/#level-100-billing-visualization","text":"","title":"Level 100: Billing Visualization"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/#introduction","text":"This hands-on lab will guide you through the steps to perform visualization of your AWS cost and usage. The skills you learn will help you monitor your cost and usage, in alignment with the AWS Well-Architected Framework.","title":"Introduction"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/#goals","text":"Perform basic analysis of your cost and usage","title":"Goals"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/#prerequisites","text":"A master AWS Account A linked AWS Account (preferred, not mandatory) Completed all previous labs in the Cost Fundamentals series","title":"Prerequisites"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/#overview","text":"Lab Guide.md the guide for this lab Checklist.md best practice checklist related to this lab /Code including CloudFormation templates related to this lab /Images referenced by this lab","title":"Overview"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/#permissions-required","text":"Log in as the Cost Optimization team, created in Cost_and_Usage_Governance NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required.","title":"Permissions required"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Checklist/","text":"Level 100: Cost Visualization [ ] View your cost and usage by service [ ] View your cost and usage by account [ ] View your Reserved Instance Coverage [ ] Create a custom EC2 report","title":"Level 100: Cost Visualization"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Checklist/#level-100-cost-visualization","text":"[ ] View your cost and usage by service [ ] View your cost and usage by account [ ] View your Reserved Instance Coverage [ ] Create a custom EC2 report","title":"Level 100: Cost Visualization"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Lab Guide/","text":"Level 100: Cost Visualization Authors Nathan Besh, Cost Lead Well-Architected Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Table of Contents View your cost and usage by service View your cost and usage by account View your Reserved Instance coverage Create a custom EC2 report Tear down Survey 1. View your cost and usage by service AWS Cost Explorer is a free built in tool to that lets you dive deeper into your cost and usage data to identify trends, pinpoint cost drivers, and detect anomalies. We will examine costs by service in this exercise. Log into the console as an IAM user with the required permissions, go to the billing dashboard: Select Cost Explorer from the menu on the left: Click on Launch Cost Explorer : Click on Saved reports from the left menu: You will be presented a list of pre-configured and saved reports. Click on Monthly costs by service : This is the monthly costs by service for the last 6 months, broken down by month (your usage will most likely be different): We will change to a daily view to highlight trends. Select the Monthly drop down and click on Daily : The bar graph is difficult to read, so we will switch to a line graph. Click on the Bar dropdown, then select Line : This is the same data with daily granularity and shows trends much more clearly. There are monthly peaks - these are monthly recurring reservation fees from Reserved Instances (Green line): We will remove the RI recurring fees. Click on the Charge Type filter on the right, click the checkbox next to Recurring reservation fee , select Exlude only to remove the data. Then click Apply filters : We have now excluded the monthly recurring fees and the peaks have been removed. We can see the largest cost for our usage during this period is Glue: We will remove the Glue service to show the other services with better clarity. Click on the Service filter from the right, click the checkbox next to Glue , select Exclude only , and click Apply filters : Glue has now been excluded, and all the other services can been seen easily: You have now viewed the costs by service and applied multilpe filters. You can continue to modify the report by timeframe and apply other filters. 2. View your cost and usage by account We will now view usage by account. This helps to highlight where the costs and usage are by linked account. NOTE: you will need one or more multilpe accounts for this exercise to be effective. Select Saved reports from the left menu: Click on Monthly costs by linked account : It will show the default last 6 months, with a monthly granularity. As above, change the graph to Daily granularity and from a bar graph to a Line graph: Here is the daily granularity line graph. You can see there is one account which has the most cost, so lets focus on that by applying a filter: On the right click on Linked Account , select the checkbox next to the account we want to focus on, then click Include only and Apply filters : You can now see this one accounts usage: Lets see the services breakdown for this account, click on Service to group by services: You can see the service breakdown for this account. Lets see the instance type breakdown for this account, click on Instance Type : You can see the instance type breakdown for this account. Lets see the usage type breakdown for this account, click on Usage Type : Here is the usage type breakdown: You have now viewed the costs by account and applied multilpe filters. You can continue to modify the report by timeframe and apply other filters. 3. View your Reserved Instance coverage To ensure you are paying the lowest prices for your resources, a high coverage of Reserved Instances (RI's) is required. A typical goal is to aim for approximately 80% of running instances covered by RI's, here is how you can check your coverage. In Cost Explorer, click on Saved reports on the left: Click on RI Coverage : You will see the default RI Coverage report. It is for the Last 3 Months , and is for the instances within the EC2 service: Scroll down below the graph and you can see a summary of the costs and usage. Note that depending on the instance type and size, the On-Demand costs will be different per hour: To help focus where you need to, click on the down arrow next to ON-DEMAND COST to sort by costs descending. This will put the highest on-demand costs at the top, which is where you should focus your RI purchases: You have now viewed your RI coverage, and have insight on where to increase your coverage. 4. Create custom EC2 reports We will now create some custom EC2 reports, which will help to show ongoing costs related to EC2 instances and their associated usage. From the left menu click Explore , and click Cost Usage : You will have the default breakdown by Service. Click on the Service filter on the right, select EC2-Instances (Elastic Compute Cloud - Compute) and EC2-Other , then click Apply filters : You will now have monthly EC2 Instance and Other costs: Change the Group by to Usage Type : Change it to a Daily Line graph, then select More filters : click on Purchase Option , select On Demand and click Apply filters , which will ensure we are only looking at On-Demand costs: These are your on-demand EC2 costs, you should setup a report like this for your services that have the highest usage or costs. We will now save this, click on Save as... : Enter a report name and click Save Report : Now click on the Service filter, and de-select EC2-Instances , so that only EC2-Other is selected: Now you can clearly see what makes up the Other charges, typically these are EBS volumes, Data Transfer and other costs associated with EC2 usage. Click Save as... (do NOT click Save): Enter a report name and click Save Report : You can access these by clicking on Saved Reports : Here you can see both reports that were saved, note they do not have a lock symbol - which is reserved for AWS configured reports: 5. Tear down We will delete both custom reports that were created. Click on Saved reports on the left menu: Select the checkbox next to the two custom reports that you created above. Click on Delete : Verify the names of the reports you are going to delete, click Delete : The reports are no longer listed in the reports available: 6. Survey Thanks for taking the lab, We hope that you can take this short survey ( 2 minutes), to share your insights and help us improve our content. This survey is hosted by an external company (Qualtrics) , so the link above does not lead to our website. Please note that AWS will own the data gathered via this survey and will not share the information/results collected with survey respondents. Your responses to this survey will be subject to Amazon\u2019s Privacy Policy.","title":"Level 100: Cost Visualization"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Lab Guide/#level-100-cost-visualization","text":"","title":"Level 100: Cost Visualization"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Lab Guide/#authors","text":"Nathan Besh, Cost Lead Well-Architected","title":"Authors"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Lab Guide/#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Lab Guide/#table-of-contents","text":"View your cost and usage by service View your cost and usage by account View your Reserved Instance coverage Create a custom EC2 report Tear down Survey","title":"Table of Contents"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Lab Guide/#1-view-your-cost-and-usage-by-service","text":"AWS Cost Explorer is a free built in tool to that lets you dive deeper into your cost and usage data to identify trends, pinpoint cost drivers, and detect anomalies. We will examine costs by service in this exercise. Log into the console as an IAM user with the required permissions, go to the billing dashboard: Select Cost Explorer from the menu on the left: Click on Launch Cost Explorer : Click on Saved reports from the left menu: You will be presented a list of pre-configured and saved reports. Click on Monthly costs by service : This is the monthly costs by service for the last 6 months, broken down by month (your usage will most likely be different): We will change to a daily view to highlight trends. Select the Monthly drop down and click on Daily : The bar graph is difficult to read, so we will switch to a line graph. Click on the Bar dropdown, then select Line : This is the same data with daily granularity and shows trends much more clearly. There are monthly peaks - these are monthly recurring reservation fees from Reserved Instances (Green line): We will remove the RI recurring fees. Click on the Charge Type filter on the right, click the checkbox next to Recurring reservation fee , select Exlude only to remove the data. Then click Apply filters : We have now excluded the monthly recurring fees and the peaks have been removed. We can see the largest cost for our usage during this period is Glue: We will remove the Glue service to show the other services with better clarity. Click on the Service filter from the right, click the checkbox next to Glue , select Exclude only , and click Apply filters : Glue has now been excluded, and all the other services can been seen easily: You have now viewed the costs by service and applied multilpe filters. You can continue to modify the report by timeframe and apply other filters.","title":"1. View your cost and usage by service "},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Lab Guide/#2-view-your-cost-and-usage-by-account","text":"We will now view usage by account. This helps to highlight where the costs and usage are by linked account. NOTE: you will need one or more multilpe accounts for this exercise to be effective. Select Saved reports from the left menu: Click on Monthly costs by linked account : It will show the default last 6 months, with a monthly granularity. As above, change the graph to Daily granularity and from a bar graph to a Line graph: Here is the daily granularity line graph. You can see there is one account which has the most cost, so lets focus on that by applying a filter: On the right click on Linked Account , select the checkbox next to the account we want to focus on, then click Include only and Apply filters : You can now see this one accounts usage: Lets see the services breakdown for this account, click on Service to group by services: You can see the service breakdown for this account. Lets see the instance type breakdown for this account, click on Instance Type : You can see the instance type breakdown for this account. Lets see the usage type breakdown for this account, click on Usage Type : Here is the usage type breakdown: You have now viewed the costs by account and applied multilpe filters. You can continue to modify the report by timeframe and apply other filters.","title":"2. View your cost and usage by account "},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Lab Guide/#3-view-your-reserved-instance-coverage","text":"To ensure you are paying the lowest prices for your resources, a high coverage of Reserved Instances (RI's) is required. A typical goal is to aim for approximately 80% of running instances covered by RI's, here is how you can check your coverage. In Cost Explorer, click on Saved reports on the left: Click on RI Coverage : You will see the default RI Coverage report. It is for the Last 3 Months , and is for the instances within the EC2 service: Scroll down below the graph and you can see a summary of the costs and usage. Note that depending on the instance type and size, the On-Demand costs will be different per hour: To help focus where you need to, click on the down arrow next to ON-DEMAND COST to sort by costs descending. This will put the highest on-demand costs at the top, which is where you should focus your RI purchases: You have now viewed your RI coverage, and have insight on where to increase your coverage.","title":"3. View your Reserved Instance coverage "},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Lab Guide/#4-create-custom-ec2-reports","text":"We will now create some custom EC2 reports, which will help to show ongoing costs related to EC2 instances and their associated usage. From the left menu click Explore , and click Cost Usage : You will have the default breakdown by Service. Click on the Service filter on the right, select EC2-Instances (Elastic Compute Cloud - Compute) and EC2-Other , then click Apply filters : You will now have monthly EC2 Instance and Other costs: Change the Group by to Usage Type : Change it to a Daily Line graph, then select More filters : click on Purchase Option , select On Demand and click Apply filters , which will ensure we are only looking at On-Demand costs: These are your on-demand EC2 costs, you should setup a report like this for your services that have the highest usage or costs. We will now save this, click on Save as... : Enter a report name and click Save Report : Now click on the Service filter, and de-select EC2-Instances , so that only EC2-Other is selected: Now you can clearly see what makes up the Other charges, typically these are EBS volumes, Data Transfer and other costs associated with EC2 usage. Click Save as... (do NOT click Save): Enter a report name and click Save Report : You can access these by clicking on Saved Reports : Here you can see both reports that were saved, note they do not have a lock symbol - which is reserved for AWS configured reports:","title":"4. Create custom EC2 reports "},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Lab Guide/#5-tear-down","text":"We will delete both custom reports that were created. Click on Saved reports on the left menu: Select the checkbox next to the two custom reports that you created above. Click on Delete : Verify the names of the reports you are going to delete, click Delete : The reports are no longer listed in the reports available:","title":"5. Tear down "},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Lab Guide/#6-survey","text":"Thanks for taking the lab, We hope that you can take this short survey ( 2 minutes), to share your insights and help us improve our content. This survey is hosted by an external company (Qualtrics) , so the link above does not lead to our website. Please note that AWS will own the data gathered via this survey and will not share the information/results collected with survey respondents. Your responses to this survey will be subject to Amazon\u2019s Privacy Policy.","title":"6. Survey "},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/","text":"Level 200: Cost and Usage Governance Introduction This hands-on lab will guide you through the steps to implement cost and usage governance. The skills you learn will help you control your cost and usage in alignment with your business requirements. Goals Create a Cost Optimization team to monitor usage, cost, and enforce policies Implement IAM Policies to control usage Prerequisites An AWS Account Completed all previous labs in the Cost Fundamentals series Overview Lab Guide.md the guide for this lab Checklist.md best practice checklist related to this lab /Code including CloudFormation templates related to this lab /Images referenced by this lab Permissions required ./Code/IAM_policy IAM policy required for this lab NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required. License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 200: Cost and Usage Governance"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/#level-200-cost-and-usage-governance","text":"","title":"Level 200: Cost and Usage Governance"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/#introduction","text":"This hands-on lab will guide you through the steps to implement cost and usage governance. The skills you learn will help you control your cost and usage in alignment with your business requirements.","title":"Introduction"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/#goals","text":"Create a Cost Optimization team to monitor usage, cost, and enforce policies Implement IAM Policies to control usage","title":"Goals"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/#prerequisites","text":"An AWS Account Completed all previous labs in the Cost Fundamentals series","title":"Prerequisites"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/#overview","text":"Lab Guide.md the guide for this lab Checklist.md best practice checklist related to this lab /Code including CloudFormation templates related to this lab /Images referenced by this lab","title":"Overview"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/#permissions-required","text":"./Code/IAM_policy IAM policy required for this lab NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required.","title":"Permissions required"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Checklist/","text":"Level 200: Cost and Usage Governance [ ] Create a cost optimizaion team, to manage cost optimization across your organization [ ] Create an IAM Policy to restrict EC2 usage by region [ ] Create an IAM Policy to restirct EC2 usage by family [ ] Extend an IAM Policy to restrict EC2 usage by instance size [ ] Create an IAM policy to restrict EBS Volume creation by volume type","title":"Level 200: Cost and Usage Governance"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Checklist/#level-200-cost-and-usage-governance","text":"[ ] Create a cost optimizaion team, to manage cost optimization across your organization [ ] Create an IAM Policy to restrict EC2 usage by region [ ] Create an IAM Policy to restirct EC2 usage by family [ ] Extend an IAM Policy to restrict EC2 usage by instance size [ ] Create an IAM policy to restrict EBS Volume creation by volume type","title":"Level 200: Cost and Usage Governance"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab Guide/","text":"Level 200: Cost and Usage Governance Authors Nathan Besh, Cost Lead Well-Architected Spencer Marley, Commercial Architect Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Table of Contents Create a cost optimization team Create an IAM Policy to restrict EC2 usage by region Create an IAM Policy to restirct EC2 usage by family Extend an IAM Policy to restrict EC2 usage by instance size Create an IAM policy to restrict EBS Volume creation by volume type Tear down Feedback survey 1. Create a cost optimization team We are going to create a cost optimization team. Within your organization there needs to be a team of people that are focused around costs and usage. This exercise will create the users and the group, then assign all the access they need. This team will then be able to manage the organizations cost and usage, and start to implement optimization mechanisms. 1.1 Create an IAM policy for the team This provides access to allow the cost optimization team to perform their work, namely the Labs in the 100 level fundamental series. This is the minimum access the team requires. Log into the console as an IAM user with the required permissions and go to the IAM Service page: Select Policies from the left menu: Select Create Policy : Select the JSON tab: Modify the policy below, replace -billing bucket- (2 replacements) with the name of the bucket your CUR files are delivered to. Then copy paste the policy into the the field: NOTE : Ensure you copy the entire policy, everything including the first '{' and last '}' { Version : 2012-10-17 , Statement : [ { Sid : VisualEditor0 , Effect : Allow , Action : [ s3:GetObject , s3:ListBucket ], Resource : [ arn:aws:s3:::-billing bucket- , arn:aws:s3:::-billing bucket-/* ] }, { Sid : VisualEditor1 , Effect : Allow , Action : [ iam:GetPolicyVersion , quicksight:CreateAdmin , iam:DeletePolicy , iam:CreateRole , iam:AttachRolePolicy , aws-portal:ViewUsage , iam:GetGroup , aws-portal:ModifyBilling , iam:DetachRolePolicy , iam:ListAttachedRolePolicies , ds:UnauthorizeApplication , aws-portal:ViewBilling , iam:DetachGroupPolicy , iam:ListAttachedGroupPolicies , iam:CreatePolicyVersion , ds:CheckAlias , quicksight:Subscribe , ds:DeleteDirectory , iam:ListPolicies , iam:GetRole , ds:CreateIdentityPoolDirectory , ds:DescribeTrusts , iam:GetPolicy , iam:ListGroupPolicies , aws-portal:ViewAccount , iam:ListEntitiesForPolicy , iam:AttachUserPolicy , iam:ListRoles , iam:DeleteRole , budgets:* , iam:CreatePolicy , quicksight:CreateUser , s3:ListAllMyBuckets , iam:ListPolicyVersions , iam:AttachGroupPolicy , quicksight:Unsubscribe , iam:ListAccountAliases , ds:DescribeDirectories , iam:ListGroups , iam:GetGroupPolicy , ds:CreateAlias , ds:AuthorizeApplication , iam:DeletePolicyVersion ], Resource : * } ] } Click Review policy : Enter a Name and Description for the policy and click Create policy : You have successfully created the cost optimization teams policy. 1.2 Create an IAM Group This group will bring together IAM users and apply the required policies. While in the IAM console, select Groups from the left menu: Click on Create New Group : Enter a Group Name and click Next Step : Click Policy Type and select Customer Managed : Select the CostOptimization_Summit policy (created previously): We will now add more policies, click on Customer Managed and select AWS Managed : Type Athena into the filter box, select both policies, and click Next Step : Click Create Group : You have now successfully created the cost optimization group, and attached the required policies. 1.3 Create an IAM User For this lab we will create a user and join them to the group above. NOTE: it is best practice to have Multi Factor Authentication (MFA) enabled for all users. We omit this step here for brevity and simplicity, and should only be implemented as a demonstration before being removed/rectified. In the IAM console, select Users from the left menu: Click Add user : Enter a User name , select AWS Management Console access , choose Custom Password , type a suitable password, deselect Require password reset , and click Next: Permissions : Select the CostOptimization group (created previously), and click Next: Tags : Click Next Review : Click Create user : Copy the link provided, and logout by clicking on your username in the top right, and selecting Sign Out :: Log back in as the username you just created, with the link you copied for the remainder of the Lab. You have successfully create a user, placed them in the cost optimization group and have applied policies. You can continue to expand this group by adding additional users from your organization. 2. Create an IAM Policy to restrict service usage by region To manage costs you need to manage and control your usage. AWS offers multilpe regions, so depending on your business requirements you can limit access to AWS services depending on the region. This can be used to ensure usage is only allowed in specific regions which are more cost effective, and minimize associated usage and cost, such as data transfer. We will create a policy that allows all EC2, RDS and S3 access in a single region only. NOTE: it is best practice to provide only the minimum access required, the policy used here is for brevity and simplicity, and should only be implemented as a demonstration before being removed. 2.1 Create the IAM Policy Go to the IAM service page: Select Policies from the left menu: Click Create Policy : Click the JSON tab: Open the following text file, copy and paste the policy into the console: NOTE Ensure you copy the entire policy, including the start '{' and end '}' Click Review policy : NOTE: the policy may be different from the image above Enter a Name and Description , and click Create policy : You have successfully created the Policy. 2.2 Apply it to a group Select Groups from the left menu: Click on the CostOptimization group (created previously): Select the Permissions tab: Click Attach Policy : Click Policy Type and select Customer Managed : Select the checkbox next to Region_Restrict (created above) and click Attach Policy : You have successfully attached the policy to the Cost Optimizaiton group. 2.3 Verify the policy is in effect Go to the EC2 Service dashboard: Click the current region in the top right, and select US West (N.California) : You will notice that there are authorization messages due to not having access in that region (the policy restricted EC2 usage to N. Virginia only): Try to launch an instance by clicking Launch Instance : Click on Select next to the Amazon Linux 2 AMI , You will receive an error when you select an AMI as you do not have permissions: You have successfully verified that you can not launch any instances outside of the N.Virginia region. We will now verify we have access in us-east-1 (N.Virginia): Change the region by clicking the current region, and selecting US East (N.Virginia) : Now attempt to launch an instance, choose the Amazon Linux 2 AMI , leave 64-bit (x86) selected, click Select : Scroll down and select a c5.large , and click Review and Launch : Take note of the security group created (as you need to delete it), Click Launch : Select Proceed without a key pair , and click I acknowledge.. checkbox, and click Launch Instances : You will get a success message, click on the instance id: Ensure the correct instance is selected, click Actions , then Instance State , then Terminate : Confirm the instance ID is correct, click Yes, Terminate : You have successfully implemented an IAM policy that restricts all EC2, RDS and S3 operations to a single region. 3. Create an IAM Policy to restirct EC2 usage by family AWS offers different instance families within EC2. Depending on your workload requirements - different types will be most cost effective. For non-specific environments such as testing or development, you can restrict the instance families in those accounts to the most cost effective generic types. It is also an effective way to increase RI utilization, by ensuring these accounts will consume any available Reserved Instances. We will create a policy that allows operations on specific instance families only. This will not only restrict launching an instance, but all other activities. NOTE: it is best practice to provide only the minimum access required, the policy used here is for brevity and simplicity, and should only be implemented as a demonstration before being removed. 3.1 Create the IAM Policy Go to the IAM service page: Select Policies from the left menu: Click Create Policy : Click on the JSON tab: Open the following text file, copy and paste the policy into the console: NOTE Ensure you copy the entire policy, including the start '{' and end '}' Click Review policy : Enter a Name , a Description , and click on Create Policy : 3.2 Attach the policy to the group Click on Groups from the left menu: Click on the CostOptimization group (created previously): We need to remove the RegionRestrict policy, as it permitted all EC2 actions. Click on Detach Policy for RegionRestrict : Click on Detach : Click on Attach Policy : Click on Policy Type , then click Customer Managed : Select the checkbox next to Ec2_FamilyRestrict , and click Attach Policy : 3.3 Verify the policy is in effect Click on Services and click EC2 : Click on Launch Instance : Click on Select next to the Amazon Linux 2 AMI: We will select an instance we are not able to launch first, so select a c5.large instance, click Review and Launch : Make note of the security group created, click Launch : Select Proceed without a key pair , and click I acknowledge that i will not be able to... , then click Launch Instances : You will receive an error, notice the failed step was Initiating launches . Click Back to Review Screen : Click Edit instance type : We will select an instance type we can launch (t3, a1 or m5) select t3.micro , and click Review and Launch : Select Yes, I want to continue with this instance type (t3.micro) , click Next : Click Launch : Select Proceed without a key pair , and click I acknowledge that i will not be able to... , then click Launch Instances : You will receive a success message. Click on the Instance ID and terminate the instance as above: 4. Extend an IAM Policy to restrict EC2 usage by instance size We can also restrict the size of instance that can be launched. This can be used to ensure only low cost instances can be created within an account. This is ideal for testing and development, where high capacity instances may not be required. We will extend the EC2 family policy above, and add restrictions by adding the sizes of instances allowed. 4.1 Extend the EC2Family_Restrict IAM Policy Go to the IAM service page: Click on Policies on the left menu: Click on Filter policies , then select Customer managed : Click on EC2_FamilyRestrict to modify it: Click on Edit policy : Click on the JSON tab: Modify the policy by adding in the sizes, be careful not to change the syntax and only remove the * characters. Click on Review policy : Click on Save changes : 4.2 Verify the policy is in effect Click on Services and go to the EC2 dashboard: Click on Launch Instance : Click on Select next to the Amazon Linux 2 AMI : We will attempt to launch a t3.micro which was successful before. Click on Review and Launch : Review the configuraion and take note of the security group created, click Launch : Select Proceed without a key pair , and click I acknowledge that i will not be able to... , then click Launch Instances : You will get a failure, as it wasnt a size we allowed in the policy. Click Back to Review Screen : Click Edit instance type : We will now select a t3.nano which will succeed. Click Review and Launch : Select Yes, I want to continue with this instance type (t3.nano) , and click Next : Review the configuration and click Launch : Select Proceed without a key pair , and click I acknowledge that i will not be able to... , then click Launch Instances : It will succeed. Click on the Instance ID and terminate the instance as above: You have successfully implemented an IAM policy that restricts all EC2 instance operations by family and size. 5. Create an IAM policy to restrict EBS Volume creation by volume type Extending cost optimization governance beyond compute instances will ensure overall higher levels of cost optimization. Similar to EC2 instances, there are different storage types. Governing the type of storage that can be created in an account can be effective to minimise cost. We will create an IAM policy that denies operations that contain provisioned IOPS (io1) EBS volume types. This will not only restrict creating a volume, but all other actions that attempt to use this volume type. NOTE: it is best practice to provide only the minimum access required, the policy used here is for brevity and simplicity, and should only be implemented as a demonstration before being removed. 5.1 Create the IAM Policy Go to the IAM service page: Click on Policies on the left menu: Click Create policy : Click on the JSON tab: Open the following text file, copy and paste the policy into the console: NOTE Ensure you copy the entire policy, including the start '{' and end '}' Click on Review Policy : Enter a Name and a Description , and click Create policy : 5.2 Attach the policy to the Cost Optimization group Click on Groups from the left menu: Click on the CostOptimization group: Click on Attach Policy : Click on Policy Type , then click Customer Managed : Select the checkbox next to EC2EBS_Restrict , and click Attach Policy : 5.3 Verify the policy is in effect Click on Services then click EC2 : Click Launch Instance : Click Select next to Aamzon Linux 2... : Select t3.nano (which is allowed as per our already applied policy, which we tested in the last exercise), click Next: Configure Instance Details : Click Next Add Storage : Click on Add New Volume , click on the dropdown , then select Provisioned IOPS SSD (io1) : Click Review and Launch : Take note of the security group created, and click Launch : Select Proceed without a key pair , and click I acknowledge that i will not be able to... , then click Launch Instances : The launch will fail, as it contained an io1 volume. Click Back to Review Screen : Click Edit storage : Click the dropdown and change it to General Purpose SSD(gp2) , click Review and Launch : Select Proceed without a key pair , and click I acknowledge that i will not be able to... , then click Launch Instances : It will now succeed, as it doesnt contain an io1 volume type. Click on the instance ID and terminate the instance as above: 6. Tear down NOTE: The cost optimization user, group and policies are required for the completion of the fundamental labs. If you remove these resources you will not be able to complete the labs. There is no tear down for this component as it is best practices to have this group created in all organizations. Delete a security group When you attempted to, and successfully launched instances above it created a launch-wizard security group automatically, which you will need to delete. Go to the EC2 Dashboard: Confirm the instances launched as part of this exercise are terminated. Click on Instances on the left and view the instances. You can use the Launch Time column to verify this. Select Security Groups under NETWORK AND SECURITY on the left: Click the checkbox next to the security group you need to delete: NOTE: you took note of the specific group in the exercise above, you can also use the Description column which will show when it was created. Click Actions , then select Delete Security Group : Click Yes, Delete : Remove a policy from a group We will remove the IAM policies from our cost optimization group. Go to the IAM Console: Select Groups from the left menu: Click on the CostOptimization group (that was created previously): Click on Permissions : Click on Detach Policy next to the EC2_FamilyRestrict and also EC2EBS_Restrict Policy Names: Click Detach : Repeat the steps above for EC2EBS_Restrict : Click Detach : Delete a policy We will delete the IAM policies created above, as they are no longer applied to any groups. Go to the IAM Console: Click on Policies on the left: 3.Click on Filter Policies and select Customer managed : Select the policy you want to delete Region_Restrict : Click on Policy actions , and select Delete : Click on Delete : Perform the same steps above to delete the Ec2_FamilyRestrict and EC2EBS_Restrict policies. 7. Survey Thanks for taking the lab, We hope that you can take this short survey ( 2 minutes), to share your insights and help us improve our content. This survey is hosted by an external company (Qualtrics) , so the link above does not lead to our website. Please note that AWS will own the data gathered via this survey and will not share the information/results collected with survey respondents. Your responses to this survey will be subject to Amazons Privacy Policy.","title":"Level 200: Cost and Usage Governance"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab Guide/#level-200-cost-and-usage-governance","text":"","title":"Level 200: Cost and Usage Governance"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab Guide/#authors","text":"Nathan Besh, Cost Lead Well-Architected Spencer Marley, Commercial Architect","title":"Authors"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab Guide/#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab Guide/#table-of-contents","text":"Create a cost optimization team Create an IAM Policy to restrict EC2 usage by region Create an IAM Policy to restirct EC2 usage by family Extend an IAM Policy to restrict EC2 usage by instance size Create an IAM policy to restrict EBS Volume creation by volume type Tear down Feedback survey","title":"Table of Contents"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab Guide/#1-create-a-cost-optimization-team","text":"We are going to create a cost optimization team. Within your organization there needs to be a team of people that are focused around costs and usage. This exercise will create the users and the group, then assign all the access they need. This team will then be able to manage the organizations cost and usage, and start to implement optimization mechanisms.","title":"1. Create a cost optimization team "},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab Guide/#11-create-an-iam-policy-for-the-team","text":"This provides access to allow the cost optimization team to perform their work, namely the Labs in the 100 level fundamental series. This is the minimum access the team requires. Log into the console as an IAM user with the required permissions and go to the IAM Service page: Select Policies from the left menu: Select Create Policy : Select the JSON tab: Modify the policy below, replace -billing bucket- (2 replacements) with the name of the bucket your CUR files are delivered to. Then copy paste the policy into the the field: NOTE : Ensure you copy the entire policy, everything including the first '{' and last '}' { Version : 2012-10-17 , Statement : [ { Sid : VisualEditor0 , Effect : Allow , Action : [ s3:GetObject , s3:ListBucket ], Resource : [ arn:aws:s3:::-billing bucket- , arn:aws:s3:::-billing bucket-/* ] }, { Sid : VisualEditor1 , Effect : Allow , Action : [ iam:GetPolicyVersion , quicksight:CreateAdmin , iam:DeletePolicy , iam:CreateRole , iam:AttachRolePolicy , aws-portal:ViewUsage , iam:GetGroup , aws-portal:ModifyBilling , iam:DetachRolePolicy , iam:ListAttachedRolePolicies , ds:UnauthorizeApplication , aws-portal:ViewBilling , iam:DetachGroupPolicy , iam:ListAttachedGroupPolicies , iam:CreatePolicyVersion , ds:CheckAlias , quicksight:Subscribe , ds:DeleteDirectory , iam:ListPolicies , iam:GetRole , ds:CreateIdentityPoolDirectory , ds:DescribeTrusts , iam:GetPolicy , iam:ListGroupPolicies , aws-portal:ViewAccount , iam:ListEntitiesForPolicy , iam:AttachUserPolicy , iam:ListRoles , iam:DeleteRole , budgets:* , iam:CreatePolicy , quicksight:CreateUser , s3:ListAllMyBuckets , iam:ListPolicyVersions , iam:AttachGroupPolicy , quicksight:Unsubscribe , iam:ListAccountAliases , ds:DescribeDirectories , iam:ListGroups , iam:GetGroupPolicy , ds:CreateAlias , ds:AuthorizeApplication , iam:DeletePolicyVersion ], Resource : * } ] } Click Review policy : Enter a Name and Description for the policy and click Create policy : You have successfully created the cost optimization teams policy.","title":"1.1 Create an IAM policy for the team"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab Guide/#12-create-an-iam-group","text":"This group will bring together IAM users and apply the required policies. While in the IAM console, select Groups from the left menu: Click on Create New Group : Enter a Group Name and click Next Step : Click Policy Type and select Customer Managed : Select the CostOptimization_Summit policy (created previously): We will now add more policies, click on Customer Managed and select AWS Managed : Type Athena into the filter box, select both policies, and click Next Step : Click Create Group : You have now successfully created the cost optimization group, and attached the required policies.","title":"1.2 Create an IAM Group"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab Guide/#13-create-an-iam-user","text":"For this lab we will create a user and join them to the group above. NOTE: it is best practice to have Multi Factor Authentication (MFA) enabled for all users. We omit this step here for brevity and simplicity, and should only be implemented as a demonstration before being removed/rectified. In the IAM console, select Users from the left menu: Click Add user : Enter a User name , select AWS Management Console access , choose Custom Password , type a suitable password, deselect Require password reset , and click Next: Permissions : Select the CostOptimization group (created previously), and click Next: Tags : Click Next Review : Click Create user : Copy the link provided, and logout by clicking on your username in the top right, and selecting Sign Out :: Log back in as the username you just created, with the link you copied for the remainder of the Lab. You have successfully create a user, placed them in the cost optimization group and have applied policies. You can continue to expand this group by adding additional users from your organization.","title":"1.3 Create an IAM User"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab Guide/#2-create-an-iam-policy-to-restrict-service-usage-by-region","text":"To manage costs you need to manage and control your usage. AWS offers multilpe regions, so depending on your business requirements you can limit access to AWS services depending on the region. This can be used to ensure usage is only allowed in specific regions which are more cost effective, and minimize associated usage and cost, such as data transfer. We will create a policy that allows all EC2, RDS and S3 access in a single region only. NOTE: it is best practice to provide only the minimum access required, the policy used here is for brevity and simplicity, and should only be implemented as a demonstration before being removed.","title":"2. Create an IAM Policy to restrict service usage by region "},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab Guide/#21-create-the-iam-policy","text":"Go to the IAM service page: Select Policies from the left menu: Click Create Policy : Click the JSON tab: Open the following text file, copy and paste the policy into the console: NOTE Ensure you copy the entire policy, including the start '{' and end '}' Click Review policy : NOTE: the policy may be different from the image above Enter a Name and Description , and click Create policy : You have successfully created the Policy.","title":"2.1 Create the IAM Policy"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab Guide/#22-apply-it-to-a-group","text":"Select Groups from the left menu: Click on the CostOptimization group (created previously): Select the Permissions tab: Click Attach Policy : Click Policy Type and select Customer Managed : Select the checkbox next to Region_Restrict (created above) and click Attach Policy : You have successfully attached the policy to the Cost Optimizaiton group.","title":"2.2 Apply it to a group"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab Guide/#23-verify-the-policy-is-in-effect","text":"Go to the EC2 Service dashboard: Click the current region in the top right, and select US West (N.California) : You will notice that there are authorization messages due to not having access in that region (the policy restricted EC2 usage to N. Virginia only): Try to launch an instance by clicking Launch Instance : Click on Select next to the Amazon Linux 2 AMI , You will receive an error when you select an AMI as you do not have permissions: You have successfully verified that you can not launch any instances outside of the N.Virginia region. We will now verify we have access in us-east-1 (N.Virginia): Change the region by clicking the current region, and selecting US East (N.Virginia) : Now attempt to launch an instance, choose the Amazon Linux 2 AMI , leave 64-bit (x86) selected, click Select : Scroll down and select a c5.large , and click Review and Launch : Take note of the security group created (as you need to delete it), Click Launch : Select Proceed without a key pair , and click I acknowledge.. checkbox, and click Launch Instances : You will get a success message, click on the instance id: Ensure the correct instance is selected, click Actions , then Instance State , then Terminate : Confirm the instance ID is correct, click Yes, Terminate : You have successfully implemented an IAM policy that restricts all EC2, RDS and S3 operations to a single region.","title":"2.3 Verify the policy is in effect"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab Guide/#3-create-an-iam-policy-to-restirct-ec2-usage-by-family","text":"AWS offers different instance families within EC2. Depending on your workload requirements - different types will be most cost effective. For non-specific environments such as testing or development, you can restrict the instance families in those accounts to the most cost effective generic types. It is also an effective way to increase RI utilization, by ensuring these accounts will consume any available Reserved Instances. We will create a policy that allows operations on specific instance families only. This will not only restrict launching an instance, but all other activities. NOTE: it is best practice to provide only the minimum access required, the policy used here is for brevity and simplicity, and should only be implemented as a demonstration before being removed.","title":"3. Create an IAM Policy to restirct EC2 usage by family "},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab Guide/#31-create-the-iam-policy","text":"Go to the IAM service page: Select Policies from the left menu: Click Create Policy : Click on the JSON tab: Open the following text file, copy and paste the policy into the console: NOTE Ensure you copy the entire policy, including the start '{' and end '}' Click Review policy : Enter a Name , a Description , and click on Create Policy :","title":"3.1 Create the IAM Policy"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab Guide/#32-attach-the-policy-to-the-group","text":"Click on Groups from the left menu: Click on the CostOptimization group (created previously): We need to remove the RegionRestrict policy, as it permitted all EC2 actions. Click on Detach Policy for RegionRestrict : Click on Detach : Click on Attach Policy : Click on Policy Type , then click Customer Managed : Select the checkbox next to Ec2_FamilyRestrict , and click Attach Policy :","title":"3.2 Attach the policy to the group"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab Guide/#33-verify-the-policy-is-in-effect","text":"Click on Services and click EC2 : Click on Launch Instance : Click on Select next to the Amazon Linux 2 AMI: We will select an instance we are not able to launch first, so select a c5.large instance, click Review and Launch : Make note of the security group created, click Launch : Select Proceed without a key pair , and click I acknowledge that i will not be able to... , then click Launch Instances : You will receive an error, notice the failed step was Initiating launches . Click Back to Review Screen : Click Edit instance type : We will select an instance type we can launch (t3, a1 or m5) select t3.micro , and click Review and Launch : Select Yes, I want to continue with this instance type (t3.micro) , click Next : Click Launch : Select Proceed without a key pair , and click I acknowledge that i will not be able to... , then click Launch Instances : You will receive a success message. Click on the Instance ID and terminate the instance as above:","title":"3.3 Verify the policy is in effect"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab Guide/#4-extend-an-iam-policy-to-restrict-ec2-usage-by-instance-size","text":"We can also restrict the size of instance that can be launched. This can be used to ensure only low cost instances can be created within an account. This is ideal for testing and development, where high capacity instances may not be required. We will extend the EC2 family policy above, and add restrictions by adding the sizes of instances allowed.","title":"4. Extend an IAM Policy to restrict EC2 usage by instance size "},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab Guide/#41-extend-the-ec2family_restrict-iam-policy","text":"Go to the IAM service page: Click on Policies on the left menu: Click on Filter policies , then select Customer managed : Click on EC2_FamilyRestrict to modify it: Click on Edit policy : Click on the JSON tab: Modify the policy by adding in the sizes, be careful not to change the syntax and only remove the * characters. Click on Review policy : Click on Save changes :","title":"4.1 Extend the EC2Family_Restrict IAM Policy"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab Guide/#42-verify-the-policy-is-in-effect","text":"Click on Services and go to the EC2 dashboard: Click on Launch Instance : Click on Select next to the Amazon Linux 2 AMI : We will attempt to launch a t3.micro which was successful before. Click on Review and Launch : Review the configuraion and take note of the security group created, click Launch : Select Proceed without a key pair , and click I acknowledge that i will not be able to... , then click Launch Instances : You will get a failure, as it wasnt a size we allowed in the policy. Click Back to Review Screen : Click Edit instance type : We will now select a t3.nano which will succeed. Click Review and Launch : Select Yes, I want to continue with this instance type (t3.nano) , and click Next : Review the configuration and click Launch : Select Proceed without a key pair , and click I acknowledge that i will not be able to... , then click Launch Instances : It will succeed. Click on the Instance ID and terminate the instance as above: You have successfully implemented an IAM policy that restricts all EC2 instance operations by family and size.","title":"4.2 Verify the policy is in effect"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab Guide/#5-create-an-iam-policy-to-restrict-ebs-volume-creation-by-volume-type","text":"Extending cost optimization governance beyond compute instances will ensure overall higher levels of cost optimization. Similar to EC2 instances, there are different storage types. Governing the type of storage that can be created in an account can be effective to minimise cost. We will create an IAM policy that denies operations that contain provisioned IOPS (io1) EBS volume types. This will not only restrict creating a volume, but all other actions that attempt to use this volume type. NOTE: it is best practice to provide only the minimum access required, the policy used here is for brevity and simplicity, and should only be implemented as a demonstration before being removed.","title":"5. Create an IAM policy to restrict EBS Volume creation by volume type "},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab Guide/#51-create-the-iam-policy","text":"Go to the IAM service page: Click on Policies on the left menu: Click Create policy : Click on the JSON tab: Open the following text file, copy and paste the policy into the console: NOTE Ensure you copy the entire policy, including the start '{' and end '}' Click on Review Policy : Enter a Name and a Description , and click Create policy :","title":"5.1 Create the IAM Policy"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab Guide/#52-attach-the-policy-to-the-cost-optimization-group","text":"Click on Groups from the left menu: Click on the CostOptimization group: Click on Attach Policy : Click on Policy Type , then click Customer Managed : Select the checkbox next to EC2EBS_Restrict , and click Attach Policy :","title":"5.2 Attach the policy to the Cost Optimization group"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab Guide/#53-verify-the-policy-is-in-effect","text":"Click on Services then click EC2 : Click Launch Instance : Click Select next to Aamzon Linux 2... : Select t3.nano (which is allowed as per our already applied policy, which we tested in the last exercise), click Next: Configure Instance Details : Click Next Add Storage : Click on Add New Volume , click on the dropdown , then select Provisioned IOPS SSD (io1) : Click Review and Launch : Take note of the security group created, and click Launch : Select Proceed without a key pair , and click I acknowledge that i will not be able to... , then click Launch Instances : The launch will fail, as it contained an io1 volume. Click Back to Review Screen : Click Edit storage : Click the dropdown and change it to General Purpose SSD(gp2) , click Review and Launch : Select Proceed without a key pair , and click I acknowledge that i will not be able to... , then click Launch Instances : It will now succeed, as it doesnt contain an io1 volume type. Click on the instance ID and terminate the instance as above:","title":"5.3 Verify the policy is in effect"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab Guide/#6-tear-down","text":"NOTE: The cost optimization user, group and policies are required for the completion of the fundamental labs. If you remove these resources you will not be able to complete the labs. There is no tear down for this component as it is best practices to have this group created in all organizations.","title":"6. Tear down "},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab Guide/#delete-a-security-group","text":"When you attempted to, and successfully launched instances above it created a launch-wizard security group automatically, which you will need to delete. Go to the EC2 Dashboard: Confirm the instances launched as part of this exercise are terminated. Click on Instances on the left and view the instances. You can use the Launch Time column to verify this. Select Security Groups under NETWORK AND SECURITY on the left: Click the checkbox next to the security group you need to delete: NOTE: you took note of the specific group in the exercise above, you can also use the Description column which will show when it was created. Click Actions , then select Delete Security Group : Click Yes, Delete :","title":"Delete a security group"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab Guide/#remove-a-policy-from-a-group","text":"We will remove the IAM policies from our cost optimization group. Go to the IAM Console: Select Groups from the left menu: Click on the CostOptimization group (that was created previously): Click on Permissions : Click on Detach Policy next to the EC2_FamilyRestrict and also EC2EBS_Restrict Policy Names: Click Detach : Repeat the steps above for EC2EBS_Restrict : Click Detach :","title":"Remove a policy from a group"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab Guide/#delete-a-policy","text":"We will delete the IAM policies created above, as they are no longer applied to any groups. Go to the IAM Console: Click on Policies on the left: 3.Click on Filter Policies and select Customer managed : Select the policy you want to delete Region_Restrict : Click on Policy actions , and select Delete : Click on Delete : Perform the same steps above to delete the Ec2_FamilyRestrict and EC2EBS_Restrict policies.","title":"Delete a policy"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab Guide/#7-survey","text":"Thanks for taking the lab, We hope that you can take this short survey ( 2 minutes), to share your insights and help us improve our content. This survey is hosted by an external company (Qualtrics) , so the link above does not lead to our website. Please note that AWS will own the data gathered via this survey and will not share the information/results collected with survey respondents. Your responses to this survey will be subject to Amazons Privacy Policy.","title":"7. Survey "},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/","text":"Level 200: Pricing Models Introduction This hands-on lab will guide you through the steps to perform a Reserved Instance analysis, and make low risk, high return RI purchases at scale. The skills you learn will help you ensure your workloads utilize different pricing models in alignment with the AWS Well-Architected Framework. Goals Perform a Reserved Instance analysis Filter and sort the recommendations Prerequisites An AWS Account Cost_and_Usage_Governance has been completed Overview Lab Guide.md the guide for this lab Checklist.md best practice checklist related to this lab /Code including CloudFormation templates related to this lab /Images referenced by this lab Permissions required Log in as the Cost Optimization team, created in Cost_and_Usage_Governance NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required. License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 200: Pricing Models"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/#level-200-pricing-models","text":"","title":"Level 200: Pricing Models"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/#introduction","text":"This hands-on lab will guide you through the steps to perform a Reserved Instance analysis, and make low risk, high return RI purchases at scale. The skills you learn will help you ensure your workloads utilize different pricing models in alignment with the AWS Well-Architected Framework.","title":"Introduction"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/#goals","text":"Perform a Reserved Instance analysis Filter and sort the recommendations","title":"Goals"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/#prerequisites","text":"An AWS Account Cost_and_Usage_Governance has been completed","title":"Prerequisites"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/#overview","text":"Lab Guide.md the guide for this lab Checklist.md best practice checklist related to this lab /Code including CloudFormation templates related to this lab /Images referenced by this lab","title":"Overview"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/#permissions-required","text":"Log in as the Cost Optimization team, created in Cost_and_Usage_Governance NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required.","title":"Permissions required"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Checklist/","text":"Level 200: Pricing Models Checklist [ ] Review RI Recommendations [ ] Sort and filter RI Recommendations across an account [ ] Prepare a final list of low risk, high return RI's based on usage patterns","title":"Level 200: Pricing Models Checklist"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Checklist/#level-200-pricing-models-checklist","text":"[ ] Review RI Recommendations [ ] Sort and filter RI Recommendations across an account [ ] Prepare a final list of low risk, high return RI's based on usage patterns","title":"Level 200: Pricing Models Checklist"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab Guide/","text":"Level 200: Pricing Models Authors Nathan Besh, Cost Lead, Well-Architected Spencer Marley, Commercial Architect Paul Lambden, Principal Technical Account Manager Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Table of Contents View an RI report Download and prepare the RI CSV files Sort and filter the RI CSV files Teardown Feedback survey 1. View an RI report We are going to view the RI reports within AWS Cost Explorer, to understand the recommendations and possible purchases we should make. NOTE: We do all analysis using All Up Front , as this allows the break even to show when the entire 12month term is paid off. After the analysis is performed, select the required RI type. Log into the console as an IAM user with the required permissions, go to the AWS Cost Explorer service page: In the left menu select Recommendations : On the right select the filters: RI term 1 year, Payment Option All upfront, Based on the past 7 days: The top section will show the estimated savings and number of recommendations, take note of the Purcahse Recommendations On the right select the filter: Based on the past 30 days: View the Purcahse Recommendations , if the 30 day recommendation is less than 7 days recommendation - your usage is increasing and the recommendations are lower risk. If the 7 days recommendation is less than 30 days, then your usage is decreasing and you need to look further into your usage patterns to see which RI's would be suitable. 2. Download and prepare the RI CSV files Download the CSV for both the 7 day and 30 day recommendation files, by selecting the filter 7 days or 30 days , and clicking on Download CSV : If you do not have sufficient usage, you can download the two sample files: Ctrl-click to open them in a new tab, then copy the text and paste it into a spreadsheet application. 7_day_EC2_R_Rec.csv 30_day_EC2_R_Rec.csv Open both files, copy the Recommended Instance Quantity Purchase column from the 7 day recommendaions, and insert it into the 30 day recommendations file: Delete the following columns as they are not necessary: Recommendation Date , Owner Account , Size Flexible Recommendation , Max hourly normalized unit usage in Historical Period , Min hourly normalized unit usage in Historical Period , Average hourly normalized unit usage in Historical Period , Offering Class , Term , Payment Option , Upfront Cost , Recurring Monthly Cost . You should be left with the following columns: You now have all the data you need to make RI recommendations, you can then take these recommendations and calculate upfront costs, accounts to purchase them in, and other RI attributes at a later date. 3. Sort and filter the RI CSV files RI purchases should be done frequently (bi-weekly or monthly), so for each cycle we want: low risk and high return purchases, and purchase the top 50-75% of recommendations. This will ensure you have sufficiently high coverage, while minimising the risk of unused RIs. 3.1 Filter out low risk, and high return RIs To get the lowest risk, we sort by Break Even Months smallest to largest, as these will be fully paid off in the shortest amount of time. You can see that the RI's below are fully paid off in less than 6months - so if they are used for 6 months - they have paid themselves off completely. We will separate the very low, low, and medium risk recommendtaions. Add in some empty lines between Break Even Months of 7, 10, and copy the header line across: We have categorized the risk, so we will now look for the highest return recommendations. Sort each of the three groups by Estimated Monthly Savings , largest to smallest: Depending on your usage and business, chose a minimum estimated monthly savings - a typical value for larger customers is in the range of $50-100. While they save money, these recommendations do not save enough - aim for the top 50-70% of recommendations. We have chosen $100, grey out anything less than this: 3.2 Filter out usage patterns It would be a large amount of effort to view the daily usage patterns over the month for every recommendation, but we can do this programatically. By looking at the columns, we can assess the underlying usage pattern. If the Max hourly usage is close to Min hourly usage , within 75-100% - then the usage would be relatively flat, with low variance. Go through and highlight these cells green. You could do this with a formula, but a very fast judgement is ok: If the Average hourly usage is close to the Max hourly usage , then the minimum was only a small duration, so highlight anything green where the Average is within 75-100% of the Max : Minimum utilization required varies by the discount level. The lowest discount level is approximately 20%, so we would look for a minimum of 80%. While this is reflected through the Break even (if utilization is low, break even would be very late), we'll double check filter out only the very high utilization. Highlight anything above 90% in green: Now we look for a declining usage pattern. If the recommendation for the last 7 days is less than the 30 days, usage is declining - and you should consult your business to determine if usage will continue to fall. If the 7day Recommended Instance Quantity is equal or more then the 30day Recommended Instance Quantity , and close to (within 5%) or above the Average Hourly usage in Historical Period , then highlight the cell green: The processed sample files are available here: You have successfully filtered and processed all the recommendations. If all cells are green, these are your highly suggested recommendations in each risk group. If some cells are not green you should look deeper into your usage via Cost Explorer, and consider purchasing a portion of the recommendation in this cycle. Other suggestions for the not fully green recommendations are: - Re-evaluate in another 7-14 days to observe the usage trend - Purchase a percentage of the 7 day recommendation - Purchase a lower percentage of the average hourly - Purchase a higher percentage of the minimum hourly You can then take those recommended numbers, and purchase the quantity in the required accounts, with the required payment option (All upfront, Partial upfront, No upfront), and class (standard or convertible). 4. Teardown There are no resources or configuration items that are created during this workshop. 5. Survey Thanks for taking the lab, We hope that you can take this short survey ( 2 minutes), to share your insights and help us improve our content. This survey is hosted by an external company (Qualtrics) , so the link above does not lead to our website. Please note that AWS will own the data gathered via this survey and will not share the information/results collected with survey respondents. Your responses to this survey will be subject to Amazons Privacy Policy.","title":"Level 200: Pricing Models"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab Guide/#level-200-pricing-models","text":"","title":"Level 200: Pricing Models"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab Guide/#authors","text":"Nathan Besh, Cost Lead, Well-Architected Spencer Marley, Commercial Architect Paul Lambden, Principal Technical Account Manager","title":"Authors"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab Guide/#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab Guide/#table-of-contents","text":"View an RI report Download and prepare the RI CSV files Sort and filter the RI CSV files Teardown Feedback survey","title":"Table of Contents"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab Guide/#1-view-an-ri-report","text":"We are going to view the RI reports within AWS Cost Explorer, to understand the recommendations and possible purchases we should make. NOTE: We do all analysis using All Up Front , as this allows the break even to show when the entire 12month term is paid off. After the analysis is performed, select the required RI type. Log into the console as an IAM user with the required permissions, go to the AWS Cost Explorer service page: In the left menu select Recommendations : On the right select the filters: RI term 1 year, Payment Option All upfront, Based on the past 7 days: The top section will show the estimated savings and number of recommendations, take note of the Purcahse Recommendations On the right select the filter: Based on the past 30 days: View the Purcahse Recommendations , if the 30 day recommendation is less than 7 days recommendation - your usage is increasing and the recommendations are lower risk. If the 7 days recommendation is less than 30 days, then your usage is decreasing and you need to look further into your usage patterns to see which RI's would be suitable.","title":"1. View an RI report"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab Guide/#2-download-and-prepare-the-ri-csv-files","text":"Download the CSV for both the 7 day and 30 day recommendation files, by selecting the filter 7 days or 30 days , and clicking on Download CSV : If you do not have sufficient usage, you can download the two sample files: Ctrl-click to open them in a new tab, then copy the text and paste it into a spreadsheet application. 7_day_EC2_R_Rec.csv 30_day_EC2_R_Rec.csv Open both files, copy the Recommended Instance Quantity Purchase column from the 7 day recommendaions, and insert it into the 30 day recommendations file: Delete the following columns as they are not necessary: Recommendation Date , Owner Account , Size Flexible Recommendation , Max hourly normalized unit usage in Historical Period , Min hourly normalized unit usage in Historical Period , Average hourly normalized unit usage in Historical Period , Offering Class , Term , Payment Option , Upfront Cost , Recurring Monthly Cost . You should be left with the following columns: You now have all the data you need to make RI recommendations, you can then take these recommendations and calculate upfront costs, accounts to purchase them in, and other RI attributes at a later date.","title":"2. Download and prepare the RI CSV files"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab Guide/#3-sort-and-filter-the-ri-csv-files","text":"RI purchases should be done frequently (bi-weekly or monthly), so for each cycle we want: low risk and high return purchases, and purchase the top 50-75% of recommendations. This will ensure you have sufficiently high coverage, while minimising the risk of unused RIs.","title":"3. Sort and filter the RI CSV files"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab Guide/#31-filter-out-low-risk-and-high-return-ris","text":"To get the lowest risk, we sort by Break Even Months smallest to largest, as these will be fully paid off in the shortest amount of time. You can see that the RI's below are fully paid off in less than 6months - so if they are used for 6 months - they have paid themselves off completely. We will separate the very low, low, and medium risk recommendtaions. Add in some empty lines between Break Even Months of 7, 10, and copy the header line across: We have categorized the risk, so we will now look for the highest return recommendations. Sort each of the three groups by Estimated Monthly Savings , largest to smallest: Depending on your usage and business, chose a minimum estimated monthly savings - a typical value for larger customers is in the range of $50-100. While they save money, these recommendations do not save enough - aim for the top 50-70% of recommendations. We have chosen $100, grey out anything less than this:","title":"3.1 Filter out low risk, and high return RIs"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab Guide/#32-filter-out-usage-patterns","text":"It would be a large amount of effort to view the daily usage patterns over the month for every recommendation, but we can do this programatically. By looking at the columns, we can assess the underlying usage pattern. If the Max hourly usage is close to Min hourly usage , within 75-100% - then the usage would be relatively flat, with low variance. Go through and highlight these cells green. You could do this with a formula, but a very fast judgement is ok: If the Average hourly usage is close to the Max hourly usage , then the minimum was only a small duration, so highlight anything green where the Average is within 75-100% of the Max : Minimum utilization required varies by the discount level. The lowest discount level is approximately 20%, so we would look for a minimum of 80%. While this is reflected through the Break even (if utilization is low, break even would be very late), we'll double check filter out only the very high utilization. Highlight anything above 90% in green: Now we look for a declining usage pattern. If the recommendation for the last 7 days is less than the 30 days, usage is declining - and you should consult your business to determine if usage will continue to fall. If the 7day Recommended Instance Quantity is equal or more then the 30day Recommended Instance Quantity , and close to (within 5%) or above the Average Hourly usage in Historical Period , then highlight the cell green: The processed sample files are available here: You have successfully filtered and processed all the recommendations. If all cells are green, these are your highly suggested recommendations in each risk group. If some cells are not green you should look deeper into your usage via Cost Explorer, and consider purchasing a portion of the recommendation in this cycle. Other suggestions for the not fully green recommendations are: - Re-evaluate in another 7-14 days to observe the usage trend - Purchase a percentage of the 7 day recommendation - Purchase a lower percentage of the average hourly - Purchase a higher percentage of the minimum hourly You can then take those recommended numbers, and purchase the quantity in the required accounts, with the required payment option (All upfront, Partial upfront, No upfront), and class (standard or convertible).","title":"3.2 Filter out usage patterns"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab Guide/#4-teardown","text":"There are no resources or configuration items that are created during this workshop.","title":"4. Teardown"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab Guide/#5-survey","text":"Thanks for taking the lab, We hope that you can take this short survey ( 2 minutes), to share your insights and help us improve our content. This survey is hosted by an external company (Qualtrics) , so the link above does not lead to our website. Please note that AWS will own the data gathered via this survey and will not share the information/results collected with survey respondents. Your responses to this survey will be subject to Amazons Privacy Policy.","title":"5. Survey "},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/","text":"Level 200: Cost and Usage Analysis Introduction This hands-on lab will guide you through the steps to setup a platform to analyze your cost and usage reports. The skills you learn will help you perform analysis on your cost and usage, in alignment with the AWS Well-Architected Framework. Goals Setup an analysis platform for your cost and usage data Perform basic analysis of your cost and usage Prerequisites A master AWS Account A linked AWS Account (preferred, not mandatory) Have your Cost and Usage Report (CUR) enabled as per 100_1_Account Setup Cost_and_Usage_Governance has been completed Have usage that is tagged (preferred, not mandatory) Overview Lab Guide.md the guide for this lab Checklist.md best practice checklist related to this lab /Code including CloudFormation templates related to this lab /Images referenced by this lab Permissions required Log in as the Cost Optimization team, created in Cost_and_Usage_Governance NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required. License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 200: Cost and Usage Analysis"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/#level-200-cost-and-usage-analysis","text":"","title":"Level 200: Cost and Usage Analysis"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/#introduction","text":"This hands-on lab will guide you through the steps to setup a platform to analyze your cost and usage reports. The skills you learn will help you perform analysis on your cost and usage, in alignment with the AWS Well-Architected Framework.","title":"Introduction"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/#goals","text":"Setup an analysis platform for your cost and usage data Perform basic analysis of your cost and usage","title":"Goals"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/#prerequisites","text":"A master AWS Account A linked AWS Account (preferred, not mandatory) Have your Cost and Usage Report (CUR) enabled as per 100_1_Account Setup Cost_and_Usage_Governance has been completed Have usage that is tagged (preferred, not mandatory)","title":"Prerequisites"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/#overview","text":"Lab Guide.md the guide for this lab Checklist.md best practice checklist related to this lab /Code including CloudFormation templates related to this lab /Images referenced by this lab","title":"Overview"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/#permissions-required","text":"Log in as the Cost Optimization team, created in Cost_and_Usage_Governance NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required.","title":"Permissions required"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Checklist/","text":"Level 200: Cost and Usage Analysis [ ] Load your CUR files into Athena for analysis [ ] Create a separate table to contain only a specific member accounts usage [ ] Analyze your cost and usage by executing queries against your CUR files","title":"Level 200: Cost and Usage Analysis"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Checklist/#level-200-cost-and-usage-analysis","text":"[ ] Load your CUR files into Athena for analysis [ ] Create a separate table to contain only a specific member accounts usage [ ] Analyze your cost and usage by executing queries against your CUR files","title":"Level 200: Cost and Usage Analysis"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab Guide/","text":"Level 200: Cost and Usage Analysis Authors Nathan Besh, Cost Lead, Well-Architected Spencer Marley, Commercial Architect Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Table of Contents Verify your CUR files are being delivered Setup Amazon Athena and load the CUR files Cost and Usage analysis Tear down Feedback survey 1. Verify your CUR files are being delivered We will verify the CUR files are being delivered, they are in the correct format and the region they are in. Log into the console as an IAM user with the required permissions, go to the Billing console, and view the CUR report you created in AWS Account Setup , confirm the S3 bucket , and Report path prefix : Go to the S3 Service console: Verify the region where the bucket is located (here it is US East N.Virginia ), and click on the bucket name where the report is delivered(it is blacked out here): You should see a aws-programmatic-access-test-object which was put there to verify AWS can deliver reports, and also the folder which is the report prefix - cur . Click on the folder name for the prefix (here it is cur): Click on the folder name which is also part of the prefix (here it is WorkshopCUR): You should see a folder for each month, which contains the sql and manifest files. You will also have a folder from the prefix, labelled WorkshopCUR here. Click on the folder for the current month, for December 2018 it will be: 20181201-20190101 You will see the create-table.sql file, open this text file copy it to a text editor for later. This is delivered as you chose Athena support for your CUR. Here is a sample of the SQL file: Go back up a level in the S3 console, and click on the prefix folder, here it is WorkshopCUR, then drill down in the current year and month: You can see the delivered CUR file, it is in the parquet format: You have successfully verified that the CUR files are being delivered and in the correct format. You have also verified that Athena support was enabled through the delivery of the sql file. Sample Files You may not have substantial or interesting usage, in this case there are sample files that you can use in the code section. You will need to create the required structure in S3, download these files and then upload them into s3. NOTE : Do not save the links below, open them in a new window and download the files. They should be approximately 1Mb in size each, if you have files that are 65kb - then you have downloaded the web page and not the parquet files. Create a folder structure, such as -bucket name-/cur/WorkshopCUR/WorkshopCUR/year=2018/month=12 and copy the parquet files below into each months folder. You will need to edit the SQL file with the correct name. - Workshop.sql - October 2018 Usage - November 2018 Usage - December 2018 Usage 2. Setup Amazon Athena and load the CUR files We will configure Athena to access and view our CUR files via SQL. Athena is a serverless solution to be able to execute SQL queries across very large amounts of data. Athena is only charged for data that is scanned, and there are no ongoing costs if data is not being queried, unlike a traditional database solution. Go to the Athena console: If prompted, click on Get Started : Go to your text editor, view the first line in the SQL file and check the name of the database, here it is WorkshopCUR In the Athena console, check the region in the top right is the same as the S3 bucket (here it is N.Virginia ), then create the database in Athena - with the exact name above (check the case matches), by pasting the following code into the query window, and click on Run query : CREATE DATABASE IF NOT EXISTS WorkshopCUR COMMENT 'Database for CUR files'; You will see in the lower Results window Query successful , and on the left - a new workshopcur database has been created: Select the workshopcur database on the left, copy and paste the entire .sql file you have in your text editor into the query window, and click Run query : A new table called workshop_c_u_r will have been created, we will now load the partitions. Click on the 3 dot menu and select Load partitions : You will see it execute the command MSCK REPAIR TABLE , and in the results it will add partitions to the metastore for each month that has a billing file: NOTE: If it did not add partitions, then there is an error and there will be no data. Check The database name is correct the same as the SQL file The folder names year and month are in S3 and the case matches There are parquet files in each of the month folders We will now preview the data. Click on the 3 dot menu and select Preview table : It will execute a Select * from query, and in the results you will see the first 10 lines of your CUR file: (Optional if you have a linked account) We will create a member account table, this is for large organizations or partners - that want only a single accounts usage to be visible to them. Copy and paste the following code: CREATE TABLE linked_AccountID WITH ( format = 'Parquet', parquet_compression = 'SNAPPY') AS SELECT * FROM workshopcur . workshop_c_u_r where line_item_usage_account_id = 'AccountID' NOTE: replace AccountID with the 12 digit account ID of your member account. You will see a new table created on the left: NOTE: You can restrict and grant access to this specific member account table through IAM policies. This will be covered in the 300 level billing analysis lab You have successfully setup your CUR file to be analyzed. You can now query your usage and costs via SQL. 3. Cost and Usage analysis We will now perform some common analysis of your usage through SQL queries. You will be charged for Athena usage by the amount of data that is scanned - the source files are monthly, and in parquet format - which is compressed and partitioned to minimise cost. Be careful to include limit 10 or similar at the end of your queries to limit the amount of data that comes back. For each of the queries below, copy and paste each query into the query window, click Run query and view the results. We will restrict the queries to a single month (December, 2018) by including the following line: where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 3.1 What data is available in the CUR file? We will learn how to find out what data is available for querying in the CUR files, this will show what columns there are and some sample data in those columns. What columns and data are in the CUR table? select * from workshopcur . workshop_c_u_r where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 limit 10; What are all the different values in a column? (the column we use is line_item_line_item_description ) select distinct line_item_line_item_description from workshopcur . workshop_c_u_r where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 limit 10; 3 Give me all columns from the CUR, where a specific value is in a column (here the column line_item_line_item_type contains the word Usage somewhere, note the capital 'U'): select * from workshopcur . workshop_c_u_r where line_item_line_item_type like '%Usage%' and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 limit 10; What billing periods are available? SELECT distinct bill_billing_period_start_date FROM workshopcur . workshop_c_u_r limit 10; 3.2 Top Costs To efficiently optimize its useful to view the top costs in different categories, such as service, description or tags. Top10 Costs by AccountID: select line_item_usage_account_id , round(sum( line_item_unblended_cost ),2) as cost from workshopcur . workshop_c_u_r where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by line_item_usage_account_id order by cost desc limit 10; Top10 Costs by Product: select line_item_product_code , round(sum( line_item_unblended_cost ),2) as cost from workshopcur . workshop_c_u_r where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by line_item_product_code order by cost desc limit 10; Top Costs by Line Item Description select line_item_product_code , line_item_line_item_description , round(sum( line_item_unblended_cost ),2) as cost from workshopcur . workshop_c_u_r where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by line_item_product_code , line_item_line_item_description order by cost desc limit 10; Top EC2 Costs select line_item_product_code , line_item_line_item_description , round(sum( line_item_unblended_cost ),2) as cost from workshopcur . workshop_c_u_r where line_item_product_code like '%AmazonEC2%' and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by line_item_product_code , line_item_line_item_description order by cost desc limit 10; Top EC2 OnDemand Costs select line_item_product_code , line_item_line_item_description , round(sum( line_item_unblended_cost ),2) as cost from workshopcur . workshop_c_u_r where line_item_product_code like '%AmazonEC2%' and line_item_usage_type like '%BoxUsage%' and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by line_item_product_code , line_item_line_item_description order by cost desc limit 10; 3.3 Tagging and Chargeback Common in large organizations is the requirement to allocate costs back to specific business units. It is also critical for optimization to be able to allocate costs to workloads, to measure workload efficiency. NOTE : This will only work if you have tags enabled in your billng files, and they are the same as the examples here - resource_tags_user_cost_center Top 20 Costs by line item description and CostCenter Tag SELECT bill_payer_account_id , product_product_name , line_item_usage_type , line_item_line_item_description , resource_tags_user_cost_center, round(sum(line_item_unblended_cost),2) as cost FROM workshopcur . workshop_c_u_r where length( resource_tags_user_cost_center ) 0 and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by resource_tags_user_cost_center , bill_payer_account_id , product_product_name , line_item_usage_type , line_item_line_item_description order by cost desc limit 20 Top 20 costs by line item description, without a CostCenter Tag SELECT bill_payer_account_id , product_product_name , line_item_usage_type , line_item_line_item_description , round(sum(line_item_unblended_cost),2) as cost FROM workshopcur . workshop_c_u_r where length( resource_tags_user_cost_center ) = 0 and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by bill_payer_account_id , product_product_name , line_item_usage_type , line_item_line_item_description order by cost desc limit 20 3.4 Reserved Instance, On Demand and Spot Usage To improve the use of pricing models across a business, these queries can assist to highlight the top opportunities for Reserved Instance (top On Demand cost), and also identify who is successful with pricing models (Top users of spot). NOTE : You will need specific usage in your account that matches the instance types below, for this to work correctly. Who used Reserved Instances Identify which accounts used the available RIs, and what they would have paid with public pricing. Ideal for chargeback within an organization. select bill_payer_account_id , bill_billing_period_start_date , line_item_usage_account_id , reservation_reservation_a_r_n , line_item_product_code , line_item_usage_type , sum( line_item_usage_amount ) as Usage, line_item_unblended_rate , sum( line_item_unblended_cost ) as Cost, line_item_line_item_description , pricing_public_on_demand_rate , sum( pricing_public_on_demand_cost ) as PublicCost from workshopcur . workshop_c_u_r where line_item_line_item_Type like '%DiscountedUsage%' group by bill_payer_account_id , bill_billing_period_start_date , line_item_usage_account_id , reservation_reservation_a_r_n , line_item_product_code , line_item_usage_type , line_item_unblended_rate , line_item_line_item_description , pricing_public_on_demand_rate T2 family instance usage Observe how much is being spent on each different family (usage type) and how much is covered by Reserved instances. select line_item_usage_type , sum( line_item_usage_amount ) as usage, round(sum( line_item_unblended_cost ),2) as cost from workshopcur . workshop_c_u_r where line_item_usage_type like '%t2.%' and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by line_item_usage_type order by line_item_usage_type Costs By running type Divide the cost by usage (hrs), and see how much is being spent per hour on each of the usage types. Compare BoxUsgae (On Demand), to HeavyUsage (Reserved instance), to SpotUsage (Spot). select line_item_usage_type , round(sum( line_item_usage_amount ),2) as usage, round(sum( line_item_unblended_cost ),2) as cost, round(avg( line_item_unblended_cost / line_item_usage_amount ),4) as hourly_rate from workshopcur . workshop_c_u_r where line_item_product_code like '%AmazonEC2%' and line_item_usage_type like '%Usage%' and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by line_item_usage_type order by line_item_usage_type Show unused Reserved Instances This will show how much of your reserved instances are not being used, and sorts it via cost of unused portion (recurring fee). You can use this in two ways: See where you have spare RI's and modify instances to match, so they will use the RIs Convert your existing RI's if possible select bill_billing_period_start_date, product_region, line_item_usage_type, reservation_reservation_a_r_n, reservation_unused_quantity, reservation_unused_recurring_fee from workshopcur . workshop_c_u_r where length(reservation_reservation_a_r_n) 0 and reservation_unused_quantity 0 order by bill_billing_period_start_date, reservation_unused_recurring_fee desc 4. Tear down Amazon Athena only charges when it is being used, i.e. data is being scanned - so if it is not being actively queried, there are no charges. It is also best practice to regularly analyze your usage and cost, so there is no teardown for this lab. 5. Survey Thanks for taking the lab, We hope that you can take this short survey ( 2 minutes), to share your insights and help us improve our content. This survey is hosted by an external company (Qualtrics) , so the link above does not lead to our website. Please note that AWS will own the data gathered via this survey and will not share the information/results collected with survey respondents. Your responses to this survey will be subject to Amazons Privacy Policy.","title":"Level 200: Cost and Usage Analysis"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab Guide/#level-200-cost-and-usage-analysis","text":"","title":"Level 200: Cost and Usage Analysis"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab Guide/#authors","text":"Nathan Besh, Cost Lead, Well-Architected Spencer Marley, Commercial Architect","title":"Authors"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab Guide/#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab Guide/#table-of-contents","text":"Verify your CUR files are being delivered Setup Amazon Athena and load the CUR files Cost and Usage analysis Tear down Feedback survey","title":"Table of Contents"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab Guide/#1-verify-your-cur-files-are-being-delivered","text":"We will verify the CUR files are being delivered, they are in the correct format and the region they are in. Log into the console as an IAM user with the required permissions, go to the Billing console, and view the CUR report you created in AWS Account Setup , confirm the S3 bucket , and Report path prefix : Go to the S3 Service console: Verify the region where the bucket is located (here it is US East N.Virginia ), and click on the bucket name where the report is delivered(it is blacked out here): You should see a aws-programmatic-access-test-object which was put there to verify AWS can deliver reports, and also the folder which is the report prefix - cur . Click on the folder name for the prefix (here it is cur): Click on the folder name which is also part of the prefix (here it is WorkshopCUR): You should see a folder for each month, which contains the sql and manifest files. You will also have a folder from the prefix, labelled WorkshopCUR here. Click on the folder for the current month, for December 2018 it will be: 20181201-20190101 You will see the create-table.sql file, open this text file copy it to a text editor for later. This is delivered as you chose Athena support for your CUR. Here is a sample of the SQL file: Go back up a level in the S3 console, and click on the prefix folder, here it is WorkshopCUR, then drill down in the current year and month: You can see the delivered CUR file, it is in the parquet format: You have successfully verified that the CUR files are being delivered and in the correct format. You have also verified that Athena support was enabled through the delivery of the sql file. Sample Files You may not have substantial or interesting usage, in this case there are sample files that you can use in the code section. You will need to create the required structure in S3, download these files and then upload them into s3. NOTE : Do not save the links below, open them in a new window and download the files. They should be approximately 1Mb in size each, if you have files that are 65kb - then you have downloaded the web page and not the parquet files. Create a folder structure, such as -bucket name-/cur/WorkshopCUR/WorkshopCUR/year=2018/month=12 and copy the parquet files below into each months folder. You will need to edit the SQL file with the correct name. - Workshop.sql - October 2018 Usage - November 2018 Usage - December 2018 Usage","title":"1. Verify your CUR files are being delivered "},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab Guide/#2-setup-amazon-athena-and-load-the-cur-files","text":"We will configure Athena to access and view our CUR files via SQL. Athena is a serverless solution to be able to execute SQL queries across very large amounts of data. Athena is only charged for data that is scanned, and there are no ongoing costs if data is not being queried, unlike a traditional database solution. Go to the Athena console: If prompted, click on Get Started : Go to your text editor, view the first line in the SQL file and check the name of the database, here it is WorkshopCUR In the Athena console, check the region in the top right is the same as the S3 bucket (here it is N.Virginia ), then create the database in Athena - with the exact name above (check the case matches), by pasting the following code into the query window, and click on Run query : CREATE DATABASE IF NOT EXISTS WorkshopCUR COMMENT 'Database for CUR files'; You will see in the lower Results window Query successful , and on the left - a new workshopcur database has been created: Select the workshopcur database on the left, copy and paste the entire .sql file you have in your text editor into the query window, and click Run query : A new table called workshop_c_u_r will have been created, we will now load the partitions. Click on the 3 dot menu and select Load partitions : You will see it execute the command MSCK REPAIR TABLE , and in the results it will add partitions to the metastore for each month that has a billing file: NOTE: If it did not add partitions, then there is an error and there will be no data. Check The database name is correct the same as the SQL file The folder names year and month are in S3 and the case matches There are parquet files in each of the month folders We will now preview the data. Click on the 3 dot menu and select Preview table : It will execute a Select * from query, and in the results you will see the first 10 lines of your CUR file: (Optional if you have a linked account) We will create a member account table, this is for large organizations or partners - that want only a single accounts usage to be visible to them. Copy and paste the following code: CREATE TABLE linked_AccountID WITH ( format = 'Parquet', parquet_compression = 'SNAPPY') AS SELECT * FROM workshopcur . workshop_c_u_r where line_item_usage_account_id = 'AccountID' NOTE: replace AccountID with the 12 digit account ID of your member account. You will see a new table created on the left: NOTE: You can restrict and grant access to this specific member account table through IAM policies. This will be covered in the 300 level billing analysis lab You have successfully setup your CUR file to be analyzed. You can now query your usage and costs via SQL.","title":"2. Setup Amazon Athena and load the CUR files "},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab Guide/#3-cost-and-usage-analysis","text":"We will now perform some common analysis of your usage through SQL queries. You will be charged for Athena usage by the amount of data that is scanned - the source files are monthly, and in parquet format - which is compressed and partitioned to minimise cost. Be careful to include limit 10 or similar at the end of your queries to limit the amount of data that comes back. For each of the queries below, copy and paste each query into the query window, click Run query and view the results. We will restrict the queries to a single month (December, 2018) by including the following line: where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018","title":"3. Cost and Usage analysis "},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab Guide/#31-what-data-is-available-in-the-cur-file","text":"We will learn how to find out what data is available for querying in the CUR files, this will show what columns there are and some sample data in those columns. What columns and data are in the CUR table? select * from workshopcur . workshop_c_u_r where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 limit 10; What are all the different values in a column? (the column we use is line_item_line_item_description ) select distinct line_item_line_item_description from workshopcur . workshop_c_u_r where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 limit 10; 3 Give me all columns from the CUR, where a specific value is in a column (here the column line_item_line_item_type contains the word Usage somewhere, note the capital 'U'): select * from workshopcur . workshop_c_u_r where line_item_line_item_type like '%Usage%' and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 limit 10; What billing periods are available? SELECT distinct bill_billing_period_start_date FROM workshopcur . workshop_c_u_r limit 10;","title":"3.1 What data is available in the CUR file?"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab Guide/#32-top-costs","text":"To efficiently optimize its useful to view the top costs in different categories, such as service, description or tags. Top10 Costs by AccountID: select line_item_usage_account_id , round(sum( line_item_unblended_cost ),2) as cost from workshopcur . workshop_c_u_r where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by line_item_usage_account_id order by cost desc limit 10; Top10 Costs by Product: select line_item_product_code , round(sum( line_item_unblended_cost ),2) as cost from workshopcur . workshop_c_u_r where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by line_item_product_code order by cost desc limit 10; Top Costs by Line Item Description select line_item_product_code , line_item_line_item_description , round(sum( line_item_unblended_cost ),2) as cost from workshopcur . workshop_c_u_r where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by line_item_product_code , line_item_line_item_description order by cost desc limit 10; Top EC2 Costs select line_item_product_code , line_item_line_item_description , round(sum( line_item_unblended_cost ),2) as cost from workshopcur . workshop_c_u_r where line_item_product_code like '%AmazonEC2%' and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by line_item_product_code , line_item_line_item_description order by cost desc limit 10; Top EC2 OnDemand Costs select line_item_product_code , line_item_line_item_description , round(sum( line_item_unblended_cost ),2) as cost from workshopcur . workshop_c_u_r where line_item_product_code like '%AmazonEC2%' and line_item_usage_type like '%BoxUsage%' and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by line_item_product_code , line_item_line_item_description order by cost desc limit 10;","title":"3.2 Top Costs"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab Guide/#33-tagging-and-chargeback","text":"Common in large organizations is the requirement to allocate costs back to specific business units. It is also critical for optimization to be able to allocate costs to workloads, to measure workload efficiency. NOTE : This will only work if you have tags enabled in your billng files, and they are the same as the examples here - resource_tags_user_cost_center Top 20 Costs by line item description and CostCenter Tag SELECT bill_payer_account_id , product_product_name , line_item_usage_type , line_item_line_item_description , resource_tags_user_cost_center, round(sum(line_item_unblended_cost),2) as cost FROM workshopcur . workshop_c_u_r where length( resource_tags_user_cost_center ) 0 and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by resource_tags_user_cost_center , bill_payer_account_id , product_product_name , line_item_usage_type , line_item_line_item_description order by cost desc limit 20 Top 20 costs by line item description, without a CostCenter Tag SELECT bill_payer_account_id , product_product_name , line_item_usage_type , line_item_line_item_description , round(sum(line_item_unblended_cost),2) as cost FROM workshopcur . workshop_c_u_r where length( resource_tags_user_cost_center ) = 0 and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by bill_payer_account_id , product_product_name , line_item_usage_type , line_item_line_item_description order by cost desc limit 20","title":"3.3 Tagging and Chargeback"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab Guide/#34-reserved-instance-on-demand-and-spot-usage","text":"To improve the use of pricing models across a business, these queries can assist to highlight the top opportunities for Reserved Instance (top On Demand cost), and also identify who is successful with pricing models (Top users of spot). NOTE : You will need specific usage in your account that matches the instance types below, for this to work correctly. Who used Reserved Instances Identify which accounts used the available RIs, and what they would have paid with public pricing. Ideal for chargeback within an organization. select bill_payer_account_id , bill_billing_period_start_date , line_item_usage_account_id , reservation_reservation_a_r_n , line_item_product_code , line_item_usage_type , sum( line_item_usage_amount ) as Usage, line_item_unblended_rate , sum( line_item_unblended_cost ) as Cost, line_item_line_item_description , pricing_public_on_demand_rate , sum( pricing_public_on_demand_cost ) as PublicCost from workshopcur . workshop_c_u_r where line_item_line_item_Type like '%DiscountedUsage%' group by bill_payer_account_id , bill_billing_period_start_date , line_item_usage_account_id , reservation_reservation_a_r_n , line_item_product_code , line_item_usage_type , line_item_unblended_rate , line_item_line_item_description , pricing_public_on_demand_rate T2 family instance usage Observe how much is being spent on each different family (usage type) and how much is covered by Reserved instances. select line_item_usage_type , sum( line_item_usage_amount ) as usage, round(sum( line_item_unblended_cost ),2) as cost from workshopcur . workshop_c_u_r where line_item_usage_type like '%t2.%' and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by line_item_usage_type order by line_item_usage_type Costs By running type Divide the cost by usage (hrs), and see how much is being spent per hour on each of the usage types. Compare BoxUsgae (On Demand), to HeavyUsage (Reserved instance), to SpotUsage (Spot). select line_item_usage_type , round(sum( line_item_usage_amount ),2) as usage, round(sum( line_item_unblended_cost ),2) as cost, round(avg( line_item_unblended_cost / line_item_usage_amount ),4) as hourly_rate from workshopcur . workshop_c_u_r where line_item_product_code like '%AmazonEC2%' and line_item_usage_type like '%Usage%' and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by line_item_usage_type order by line_item_usage_type Show unused Reserved Instances This will show how much of your reserved instances are not being used, and sorts it via cost of unused portion (recurring fee). You can use this in two ways: See where you have spare RI's and modify instances to match, so they will use the RIs Convert your existing RI's if possible select bill_billing_period_start_date, product_region, line_item_usage_type, reservation_reservation_a_r_n, reservation_unused_quantity, reservation_unused_recurring_fee from workshopcur . workshop_c_u_r where length(reservation_reservation_a_r_n) 0 and reservation_unused_quantity 0 order by bill_billing_period_start_date, reservation_unused_recurring_fee desc","title":"3.4 Reserved Instance, On Demand and Spot Usage"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab Guide/#4-tear-down","text":"Amazon Athena only charges when it is being used, i.e. data is being scanned - so if it is not being actively queried, there are no charges. It is also best practice to regularly analyze your usage and cost, so there is no teardown for this lab.","title":"4. Tear down "},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab Guide/#5-survey","text":"Thanks for taking the lab, We hope that you can take this short survey ( 2 minutes), to share your insights and help us improve our content. This survey is hosted by an external company (Qualtrics) , so the link above does not lead to our website. Please note that AWS will own the data gathered via this survey and will not share the information/results collected with survey respondents. Your responses to this survey will be subject to Amazons Privacy Policy.","title":"5. Survey "},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/","text":"Level 200: Cost Visualization Introduction This hands-on lab will guide you through the steps to visualize your cost and usage. The skills you learn will help you analyze your cost and usage, in alignment with the AWS Well-Architected Framework. Goals Setup Amazon QuickSight Configure QuickSight to view your Cost and Usage reports Create a dashboard of cost and usage Prerequisites A master AWS Account Have your Cost and Usage Report (CUR) enabled as per 100_1_Account Setup Cost_and_Usage_Governance has been completed Overview Lab Guide.md the guide for this lab Checklist.md best practice checklist related to this lab /Code including CloudFormation templates related to this lab /Images referenced by this lab Permissions required Log in as the Cost Optimization team, created in Cost_and_Usage_Governance NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required. License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 200: Cost Visualization"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/#level-200-cost-visualization","text":"","title":"Level 200: Cost Visualization"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/#introduction","text":"This hands-on lab will guide you through the steps to visualize your cost and usage. The skills you learn will help you analyze your cost and usage, in alignment with the AWS Well-Architected Framework.","title":"Introduction"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/#goals","text":"Setup Amazon QuickSight Configure QuickSight to view your Cost and Usage reports Create a dashboard of cost and usage","title":"Goals"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/#prerequisites","text":"A master AWS Account Have your Cost and Usage Report (CUR) enabled as per 100_1_Account Setup Cost_and_Usage_Governance has been completed","title":"Prerequisites"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/#overview","text":"Lab Guide.md the guide for this lab Checklist.md best practice checklist related to this lab /Code including CloudFormation templates related to this lab /Images referenced by this lab","title":"Overview"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/#permissions-required","text":"Log in as the Cost Optimization team, created in Cost_and_Usage_Governance NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required.","title":"Permissions required"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Checklist/","text":"Level 200: Cost Visualization [ ] Load your CUR files into Amazon QuickSight [ ] Analyze Cost and Usage data visually","title":"Level 200: Cost Visualization"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Checklist/#level-200-cost-visualization","text":"[ ] Load your CUR files into Amazon QuickSight [ ] Analyze Cost and Usage data visually","title":"Level 200: Cost Visualization"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab Guide/","text":"Level 200: Cost Visualization Authors Spencer Marley, Commercial Architect Nathan Besh, Cost Lead, Well-Architected Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Table of Contents Setup Amazon QuickSight Create a data set Create visualizations Share your Analysis and Dashboard Tear down Feedback survey 1. Setup Amazon QuickSight The first step is to setup Amazon QuickSight, so that you can use the service in your account, and it has access to all the resources in your account. Log into the console as an IAM user with the required permissions, go to the Amazon QuickSight console: 1.1 Setup QuickSight for the first time If you havent used QuickSight before click on Sign up for QuickSight , otherwise login and go to step 5 : Select the Standard edition, and click Continue : Enter your QuickSight account name , Notification email address , select the QuickSight region (which matches your S3 bucket and Athena setup), select Amazon Athena and click Finish : Click Go to Amazon QuickSight : Click on your profile icon in the top right, and select Manage QuickSight : Click Account settings , then click Manage QuickSight permissions Click Choose S3 buckets : Select your billing bucket where the CUR files are delivered, and click Select buckets : Click Apply : Click on the QuickSight logo in the top left: 2. Create a data set We will create a data set so that QuickSight can access our Athena data set, and visualize our CUR data. Click Manage data in the top right: Click New data set : Click Athena : Enter a Data source name , and click Create data source : Select the workshpocur database (or the name you setup previously), and then the workshop_c_u_r table you created in Athena, and click Select : Select Directly query your data , and click Visualize : You have now configured QuickSight to access your Athena data set, and have access to your CUR data. 3. Create visualizations We will now start to visualize our costs and usage, and create a dashboard. 3.1 Cost by account and product The first visualization of the dashboard will do is a visualization of costs by linkedaccountID, and product. This will highlight top spend by account and product. Select line_item_unblendedcost from the Fields list, and it will show Sum of Line_item_unblended_cost: Select line_item_usage_account_id , which will add it to the graph: Expand the field wells by clicking on the two arrows in the top right. Drag line_item_product_code into the Group/Color field: Select the dropdown next to the title, and chose Format visual : Click on the down arrows under format visual , change: Y-Axis label : Linked Account ID X-Axis label : Cost Double click the title to set it: Title: Cost by Account and Product Modify the graph so that all elements are visible, with the lower corner and vertical bars : (you may need to increase the size of the graph) Sort the accounts by cost, click the dropdown under the X-Axis (Cost label), and select Sort by descending : The visualization is complete and the layout should look similar to: Click on the highest usage bar, in this example it is AWSGlue , and select Exclude AWSGlue : You will notice that AWSGlue (or the service you selected) is no longer showing, and on the left it has automatically created and applied a filter : 3.2 Elasticity The next visualization on the dashboard we will create is a visualization that shows usage for every hour, by purchase type (On Demand, Spot, Reserved Instance). In the CUR file there is no single field which shows the purchase type for EC2 Instances \u2013 so we\u2019ll make one with a calculated field. Click Add in the top left corner, then select Add calculated field : Copy and paste this formula into the Formula box: ifelse(split({line_item_usage_type},':',1) = 'SpotUsage','Spot',ifelse(right(split({product_usagetype},':',1), 8) = 'BoxUsage',{pricing_term},'other')) Description : - Ifelse( , , ) If statement evaluated and returns if true, otherwise - Right( , ) Returns the right most characters from a string - Split( , , ) Returns the substring when is split by , position is the index of the array starting at 1 Formula Logic : If the first part of \u2018lineitem/usagetype\u2019 is \u2018SpotUsage\u2019 then PurchaseOption = \u2018Spot\u2019, otherwise check part of \u2018product/usagetype\u2019 is \u2018BoxUsage\u2019, if it is then PurchaseOption = \u2018pricing/term\u2019, otherwise PurchaseOption = \u2018other\u2019. Enter a Calculated field name of PurchaseOption , and click Create : The new field will appear in the list of fields in the data source Click Add then select Add visual from the top left: Click the field line_item_usage_amount to add it to the visualization: Click line_item_usage_start_date to add it to the visualization x-axis: Change the aggregation of time to hourly , expand the field wells wih the arrows at the top right , click the down arrow* next to line_item_usage_start_date , click the arrow next to Aggregate: Day , and click Hour**: Click and drag PurchaseOption to the Color field: Now we will filter out other , click Filter on the left, and click Create One... : Select PurchaseOption : Click on the filter name PurchaseOption to edit it: Change the filter type to Custom filter list , enter other and click the + , change the Current list to Exclude : Click Apply : Select the empty line, and right click and select exclude : Update the title to Usage Elasticity , and you now have your elasticity graph, showing hourly usage by purchase option: NOTE : In the top left it states SHOWING TOP 200, and on the x-axis it has changed the range from Nov 10th to Nov 18th (most recent data points). Line charts show up to 2500 data points on the X axis when no color field is selected. When color is populated, line charts show up to 200 data points on the X axis and up to 25 data points for color. To work within this limitation, you can to add a filter to see each purchase option (OnDemand, Reserved, Spot) and remove the color field, we will do that next. We will now add instance type to the visualization, to be able to further drill down on usage. We will use another calculated field to get the instance type. Click on Add , and click Add calculated field : Copy and paste the following formula: split({line_item_usage_type},':',2) Name the field InstanceType , click Create : Drag InstanceType across to the Color field, the bottom of the box so it says Add drill-down layer: Select InstanceType and it will display the hourly usage by instance type (which is all usage regardless of purchase option): Now select PurchaseOption : Now we\u2019ll focus only on ondemand . Click on the blue line select Focus only on OnDemand : You can see it automatically added a filter on the left , now click InstanceType : It will now only show hourly usage of OnDemand instances : You can enable/disable the filter to quickly cycle through the different options, by clicking on the checkbox next to the filter : This is also useful to work within the limitations of the number of data points on visuals. Remove the color field enable/disable the filters to switch between data. Hourly usage of on demand instances is useful when making Reserved Instance purchase decisions and verifying usage to confirm if a purchase should be made. 3.3 Cost by line item description The previous visual showed instance usage, however instances vary in cost and your organization may have significant spend in other services and other components of EC2. So now we\u2019ll create a visualization that looks at daily costs by line_item_line_item_descrption, this will help to identify exactly where your costs are by within each service, across all services on a daily basis. Click Add and select Add visual : Click on line_item_unblendedcost to add it to the visualization: Click on line_item_usage_start_date to add it to the visualization, and you will have the Sum of Line_item_unblended_cost by line_item_usage_start_date : The data source for our workshop is 3 months of data, so we\u2019ll narrow that down with a filter to make it faster. Click on Filter and click Create one\u2026 Select bill_billing_period_start_date : Click on the filter name, bill_billing_period_start_date : Select a Relative dates filter, by Months and select This month , then click Apply : Click Visualize : Drag line_item_line_item_description to the Color field well , to add it to the visualization: You may have a visualization similar to below, which doesn\u2019t look very meaningful: Click on the Vertical stacked bar chart icon under Visual Types : You should get a graph similar to below which highlights cost more efficiently: Hover over the large usage and you can see the actual costs. To use this graph, observe the top costs, then exclude them and continue to drill down on the highest cost visible. 3.4 Dashboard Complete Your dashboard is now complete, you should have a similar dashboard to below: 4. Share your Analysis and Dashboard Now that your QuickSight Analysis is complete, it is time to share the Analysis or publish a Dashboard. An Analysis is a read and write copy of the Visuals and Data Set that you created. A dashboard is a read-only version, allowing the user to apply filters but not make any changes to the Visuals or Data Set. 4.1 Share an analysis To share an analysis, click on Share on the top right, then select Share analysis : Share with Authors and Admins in your QuickSight account by searching by email address. Once you have added all the users, click Share : The users will then receive an email similar to the one below. When they click on Click to View they\u2019ll be taken straight to the analysis, and they will have full access to modify the analysis as we have been doing in this workshop.: 4.2 Publish a dashboard To publish a dashboard click on Share in the upper right, and select Publish dashboard : Enter a name for the dashboard , and click Publish dashboard : Share with users in your QuickSight account by searching by email address. Once you have added all the users, select their permission levels and click Share . For the permissions, Viewer: can view, filter and sort the dashboard data, they can also use controls. Co-owner: can edit and share the dashboard. Click the x button in the top right to close the Manage dashboard sharing dialog: You will then have the dashboard on screen: All users will receive an email: 5. Tear down It is best practice to regularly analyze your usage and cost, so you should not tear down this lab unless you have an alternative visualization solution. 5.1 Cancel your QuickSight subscription Click on your profile icon in the top left, select Manage QuickSight : Click on Account settings : Cluck on Unsubscribe : Review the notifications, click Unsubscribe : 6. Survey Thanks for taking the lab, We hope that you can take this short survey ( 2 minutes), to share your insights and help us improve our content. This survey is hosted by an external company (Qualtrics) , so the link above does not lead to our website. Please note that AWS will own the data gathered via this survey and will not share the information/results collected with survey respondents. Your responses to this survey will be subject to Amazons Privacy Policy.","title":"Level 200: Cost Visualization"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab Guide/#level-200-cost-visualization","text":"","title":"Level 200: Cost Visualization"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab Guide/#authors","text":"Spencer Marley, Commercial Architect Nathan Besh, Cost Lead, Well-Architected","title":"Authors"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab Guide/#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab Guide/#table-of-contents","text":"Setup Amazon QuickSight Create a data set Create visualizations Share your Analysis and Dashboard Tear down Feedback survey","title":"Table of Contents"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab Guide/#1-setup-amazon-quicksight","text":"The first step is to setup Amazon QuickSight, so that you can use the service in your account, and it has access to all the resources in your account. Log into the console as an IAM user with the required permissions, go to the Amazon QuickSight console:","title":"1. Setup Amazon QuickSight "},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab Guide/#11-setup-quicksight-for-the-first-time","text":"If you havent used QuickSight before click on Sign up for QuickSight , otherwise login and go to step 5 : Select the Standard edition, and click Continue : Enter your QuickSight account name , Notification email address , select the QuickSight region (which matches your S3 bucket and Athena setup), select Amazon Athena and click Finish : Click Go to Amazon QuickSight : Click on your profile icon in the top right, and select Manage QuickSight : Click Account settings , then click Manage QuickSight permissions Click Choose S3 buckets : Select your billing bucket where the CUR files are delivered, and click Select buckets : Click Apply : Click on the QuickSight logo in the top left:","title":"1.1 Setup QuickSight for the first time"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab Guide/#2-create-a-data-set","text":"We will create a data set so that QuickSight can access our Athena data set, and visualize our CUR data. Click Manage data in the top right: Click New data set : Click Athena : Enter a Data source name , and click Create data source : Select the workshpocur database (or the name you setup previously), and then the workshop_c_u_r table you created in Athena, and click Select : Select Directly query your data , and click Visualize : You have now configured QuickSight to access your Athena data set, and have access to your CUR data.","title":"2. Create a data set "},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab Guide/#3-create-visualizations","text":"We will now start to visualize our costs and usage, and create a dashboard.","title":"3. Create visualizations "},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab Guide/#31-cost-by-account-and-product","text":"The first visualization of the dashboard will do is a visualization of costs by linkedaccountID, and product. This will highlight top spend by account and product. Select line_item_unblendedcost from the Fields list, and it will show Sum of Line_item_unblended_cost: Select line_item_usage_account_id , which will add it to the graph: Expand the field wells by clicking on the two arrows in the top right. Drag line_item_product_code into the Group/Color field: Select the dropdown next to the title, and chose Format visual : Click on the down arrows under format visual , change: Y-Axis label : Linked Account ID X-Axis label : Cost Double click the title to set it: Title: Cost by Account and Product Modify the graph so that all elements are visible, with the lower corner and vertical bars : (you may need to increase the size of the graph) Sort the accounts by cost, click the dropdown under the X-Axis (Cost label), and select Sort by descending : The visualization is complete and the layout should look similar to: Click on the highest usage bar, in this example it is AWSGlue , and select Exclude AWSGlue : You will notice that AWSGlue (or the service you selected) is no longer showing, and on the left it has automatically created and applied a filter :","title":"3.1 Cost by account and product"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab Guide/#32-elasticity","text":"The next visualization on the dashboard we will create is a visualization that shows usage for every hour, by purchase type (On Demand, Spot, Reserved Instance). In the CUR file there is no single field which shows the purchase type for EC2 Instances \u2013 so we\u2019ll make one with a calculated field. Click Add in the top left corner, then select Add calculated field : Copy and paste this formula into the Formula box: ifelse(split({line_item_usage_type},':',1) = 'SpotUsage','Spot',ifelse(right(split({product_usagetype},':',1), 8) = 'BoxUsage',{pricing_term},'other')) Description : - Ifelse( , , ) If statement evaluated and returns if true, otherwise - Right( , ) Returns the right most characters from a string - Split( , , ) Returns the substring when is split by , position is the index of the array starting at 1 Formula Logic : If the first part of \u2018lineitem/usagetype\u2019 is \u2018SpotUsage\u2019 then PurchaseOption = \u2018Spot\u2019, otherwise check part of \u2018product/usagetype\u2019 is \u2018BoxUsage\u2019, if it is then PurchaseOption = \u2018pricing/term\u2019, otherwise PurchaseOption = \u2018other\u2019. Enter a Calculated field name of PurchaseOption , and click Create : The new field will appear in the list of fields in the data source Click Add then select Add visual from the top left: Click the field line_item_usage_amount to add it to the visualization: Click line_item_usage_start_date to add it to the visualization x-axis: Change the aggregation of time to hourly , expand the field wells wih the arrows at the top right , click the down arrow* next to line_item_usage_start_date , click the arrow next to Aggregate: Day , and click Hour**: Click and drag PurchaseOption to the Color field: Now we will filter out other , click Filter on the left, and click Create One... : Select PurchaseOption : Click on the filter name PurchaseOption to edit it: Change the filter type to Custom filter list , enter other and click the + , change the Current list to Exclude : Click Apply : Select the empty line, and right click and select exclude : Update the title to Usage Elasticity , and you now have your elasticity graph, showing hourly usage by purchase option: NOTE : In the top left it states SHOWING TOP 200, and on the x-axis it has changed the range from Nov 10th to Nov 18th (most recent data points). Line charts show up to 2500 data points on the X axis when no color field is selected. When color is populated, line charts show up to 200 data points on the X axis and up to 25 data points for color. To work within this limitation, you can to add a filter to see each purchase option (OnDemand, Reserved, Spot) and remove the color field, we will do that next. We will now add instance type to the visualization, to be able to further drill down on usage. We will use another calculated field to get the instance type. Click on Add , and click Add calculated field : Copy and paste the following formula: split({line_item_usage_type},':',2) Name the field InstanceType , click Create : Drag InstanceType across to the Color field, the bottom of the box so it says Add drill-down layer: Select InstanceType and it will display the hourly usage by instance type (which is all usage regardless of purchase option): Now select PurchaseOption : Now we\u2019ll focus only on ondemand . Click on the blue line select Focus only on OnDemand : You can see it automatically added a filter on the left , now click InstanceType : It will now only show hourly usage of OnDemand instances : You can enable/disable the filter to quickly cycle through the different options, by clicking on the checkbox next to the filter : This is also useful to work within the limitations of the number of data points on visuals. Remove the color field enable/disable the filters to switch between data. Hourly usage of on demand instances is useful when making Reserved Instance purchase decisions and verifying usage to confirm if a purchase should be made.","title":"3.2 Elasticity"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab Guide/#33-cost-by-line-item-description","text":"The previous visual showed instance usage, however instances vary in cost and your organization may have significant spend in other services and other components of EC2. So now we\u2019ll create a visualization that looks at daily costs by line_item_line_item_descrption, this will help to identify exactly where your costs are by within each service, across all services on a daily basis. Click Add and select Add visual : Click on line_item_unblendedcost to add it to the visualization: Click on line_item_usage_start_date to add it to the visualization, and you will have the Sum of Line_item_unblended_cost by line_item_usage_start_date : The data source for our workshop is 3 months of data, so we\u2019ll narrow that down with a filter to make it faster. Click on Filter and click Create one\u2026 Select bill_billing_period_start_date : Click on the filter name, bill_billing_period_start_date : Select a Relative dates filter, by Months and select This month , then click Apply : Click Visualize : Drag line_item_line_item_description to the Color field well , to add it to the visualization: You may have a visualization similar to below, which doesn\u2019t look very meaningful: Click on the Vertical stacked bar chart icon under Visual Types : You should get a graph similar to below which highlights cost more efficiently: Hover over the large usage and you can see the actual costs. To use this graph, observe the top costs, then exclude them and continue to drill down on the highest cost visible.","title":"3.3 Cost by line item description"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab Guide/#34-dashboard-complete","text":"Your dashboard is now complete, you should have a similar dashboard to below:","title":"3.4 Dashboard Complete"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab Guide/#4-share-your-analysis-and-dashboard","text":"Now that your QuickSight Analysis is complete, it is time to share the Analysis or publish a Dashboard. An Analysis is a read and write copy of the Visuals and Data Set that you created. A dashboard is a read-only version, allowing the user to apply filters but not make any changes to the Visuals or Data Set.","title":"4. Share your Analysis and Dashboard "},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab Guide/#41-share-an-analysis","text":"To share an analysis, click on Share on the top right, then select Share analysis : Share with Authors and Admins in your QuickSight account by searching by email address. Once you have added all the users, click Share : The users will then receive an email similar to the one below. When they click on Click to View they\u2019ll be taken straight to the analysis, and they will have full access to modify the analysis as we have been doing in this workshop.:","title":"4.1 Share an analysis"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab Guide/#42-publish-a-dashboard","text":"To publish a dashboard click on Share in the upper right, and select Publish dashboard : Enter a name for the dashboard , and click Publish dashboard : Share with users in your QuickSight account by searching by email address. Once you have added all the users, select their permission levels and click Share . For the permissions, Viewer: can view, filter and sort the dashboard data, they can also use controls. Co-owner: can edit and share the dashboard. Click the x button in the top right to close the Manage dashboard sharing dialog: You will then have the dashboard on screen: All users will receive an email:","title":"4.2 Publish a dashboard"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab Guide/#5-tear-down","text":"It is best practice to regularly analyze your usage and cost, so you should not tear down this lab unless you have an alternative visualization solution.","title":"5. Tear down "},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab Guide/#51-cancel-your-quicksight-subscription","text":"Click on your profile icon in the top left, select Manage QuickSight : Click on Account settings : Cluck on Unsubscribe : Review the notifications, click Unsubscribe :","title":"5.1 Cancel your QuickSight subscription"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab Guide/#6-survey","text":"Thanks for taking the lab, We hope that you can take this short survey ( 2 minutes), to share your insights and help us improve our content. This survey is hosted by an external company (Qualtrics) , so the link above does not lead to our website. Please note that AWS will own the data gathered via this survey and will not share the information/results collected with survey respondents. Your responses to this survey will be subject to Amazons Privacy Policy.","title":"6. Survey "},{"location":"Cost/Cost_and_Usage_Analysis/","text":"AWS Well-Architected Cost Optimization Labs Cost and Usage Analysis Critical to Cost Optimization is cost and usage analysis. The first step in being able to optimize is understanding your usage, and therefore costs. After analysis is performed, you will gain insights and be able to modify your usage, leading to increasing optimization of your workloads. For more information about cost optimization on AWS visit the Well-Architected tool in the AWS console, and read the AWS Well-Architected cost optimization whitepaper. Labs: 100 and 200 These labs are from the fundamentals series and must be completed before you move onto the 300 series. - 100 #4 Cost and Usage Analysis - 200 #4 Cost and Usage Analysis 300 These labs focus on specific items within cost and usage analysis and are used to acheive a specific outcome. They must be done after the 100 and 200 labs, however they can be completed independantly of each other. - 300 Billing Analysis - Automated CUR Updates and Ingestion","title":"AWS Well-Architected Cost Optimization Labs"},{"location":"Cost/Cost_and_Usage_Analysis/#aws-well-architected-cost-optimization-labs","text":"","title":"AWS Well-Architected Cost Optimization Labs"},{"location":"Cost/Cost_and_Usage_Analysis/#cost-and-usage-analysis","text":"Critical to Cost Optimization is cost and usage analysis. The first step in being able to optimize is understanding your usage, and therefore costs. After analysis is performed, you will gain insights and be able to modify your usage, leading to increasing optimization of your workloads. For more information about cost optimization on AWS visit the Well-Architected tool in the AWS console, and read the AWS Well-Architected cost optimization whitepaper.","title":"Cost and Usage Analysis"},{"location":"Cost/Cost_and_Usage_Analysis/#labs","text":"","title":"Labs:"},{"location":"Cost/Cost_and_Usage_Analysis/#100-and-200","text":"These labs are from the fundamentals series and must be completed before you move onto the 300 series. - 100 #4 Cost and Usage Analysis - 200 #4 Cost and Usage Analysis","title":"100 and 200"},{"location":"Cost/Cost_and_Usage_Analysis/#300","text":"These labs focus on specific items within cost and usage analysis and are used to acheive a specific outcome. They must be done after the 100 and 200 labs, however they can be completed independantly of each other. - 300 Billing Analysis - Automated CUR Updates and Ingestion","title":"300"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/","text":"Level 300: Automated CUR Updates and Ingestion Introduction This hands-on lab will guide you through the steps to enable automated updates of your CUR files into Athena. The skills you learn will help you perform cost and usage analysis in alignment with the AWS Well-Architected Framework. Goals Automatically update the CUR table in Athena/Glue when a new report arrives Automatically update the CUR table for multilpe Cost and Usage Reports in the same bucket Prerequisites An AWS Account CUR enabled and delivered into S3, with Athena integration 6-12 months AWS experience, able to navigate the console, and have an understanding of the underlying services and features Overview Lab Guide.md the guide for this lab Checklist.md best practice checklist related to this lab /Code including CloudFormation templates related to this lab /Images referenced by this lab License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 300: Automated CUR Updates and Ingestion"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/#level-300-automated-cur-updates-and-ingestion","text":"","title":"Level 300: Automated CUR Updates and Ingestion"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/#introduction","text":"This hands-on lab will guide you through the steps to enable automated updates of your CUR files into Athena. The skills you learn will help you perform cost and usage analysis in alignment with the AWS Well-Architected Framework.","title":"Introduction"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/#goals","text":"Automatically update the CUR table in Athena/Glue when a new report arrives Automatically update the CUR table for multilpe Cost and Usage Reports in the same bucket","title":"Goals"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/#prerequisites","text":"An AWS Account CUR enabled and delivered into S3, with Athena integration 6-12 months AWS experience, able to navigate the console, and have an understanding of the underlying services and features","title":"Prerequisites"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/#overview","text":"Lab Guide.md the guide for this lab Checklist.md best practice checklist related to this lab /Code including CloudFormation templates related to this lab /Images referenced by this lab","title":"Overview"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/Checklist/","text":"Level 300: Automated CUR Updates and Ingestion [ ] Run the CloudFormation template to update a single CUR in AWS Glue/Athena [ ] Modify and run a CloudFormation template to update multilpe CURs in AWS Glue/Athena","title":"Level 300: Automated CUR Updates and Ingestion"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/Checklist/#level-300-automated-cur-updates-and-ingestion","text":"[ ] Run the CloudFormation template to update a single CUR in AWS Glue/Athena [ ] Modify and run a CloudFormation template to update multilpe CURs in AWS Glue/Athena","title":"Level 300: Automated CUR Updates and Ingestion"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/Lab Guide/","text":"Level 300: Automated CUR Updates and Ingestion Authors Nathan Besh, Cost Lead, Well-Architected Derrick Gold, Software Development Engineer, AWS Insights Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Table of Contents Create the CloudFormation stack Multilpe CURs Tear down Survey 1. Create the CloudFormation Stack This step is used when there is a single CUR being delivered, and have it automatically update Athena/Glue when there are new versions and new months data. We will follow the steps here: https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/setting-up-athena.html#use-athena-cf to implement the CloudFormation template, which will automatically update existing CURs, and include new CURs when they are delivered. NOTE: IAM roles will be created, these are used to: - Add event notification to existing S3 buckets - Create s3 buckets and upload objects - Create and run a Glue crawler - Create and update a Glue database and tables Please review the CloudFormation template with your security team. We will build the following solution: Log into the console as an IAM user with the required permissions. Go to the S3 dashboard, go to the bucket and folders which contain your CUR file. Open the CloudFormation(CF) file and save it locally: Here is a sample of the CF file: Go to the CloudFormation dashboard and create a stack: Load the template and click Next : Specify the details for the stack and click Next : Review the configuration, click I acknowledge that AWS CloudFormation might create IAM resources , and click Create stack : You will see the stack will start in CREATE_IN_PROGRESS : Once complete, the stack will show CREATE_COMPLETE : Click on Resources to view the resources that it will create: Go to the AWS Glue dashboard: Click on Databases and click the database starting with athenacurcfn : View the table within that database and its properties: You will see that the table is populated, the recordCount should be greater than 0. You can now go to Athena and load the partitions and view the cost and usage reports. 2. Multiple CURs This step is used when there are multilpe CURs being delivered into the same bucket - for example a CUR with hourly granularity and one with daily granularity. This will automatically update Athena/Glue when there are new versions and new months data for both reports. The easiest way to work with multiple CURs is to deliver each CUR to a different S3 bucket, and follow the previous process. If you must deliver to a single bucket, configure your CURs with different prefixes or folders and follow this process. Log into the console as an IAM user with the required permissions, verify you have multiple CURs with different prefixes being delivered into the same bucket. We will have the following configuration: Format: bucket name / prefix / report_name / Configuration: bucket name /DailyCUR/daily/ bucket name /HourlyCUR/hourly/ Open the S3 console, and navigate to one of the directories where CURs are stored. Open and save the crawler-cfn.yml file: Open the file in your favourite text editor Modify the following lines to remove all references to the prefix or report name. Replace the first line with the second in each case: Under AWSCurDatabase: Name: 'athenacurcfn_daily' Name: 'athenacurcfn' Under AWSCURCrawlerComponentFunction: Resource: arn:aws:s3::: bucket name /DailyCUR/daily/daily* Resource: arn:aws:s3::: bucket name * Under AWSCURCrawler: Name: AWSCURCrawler-daily Name: AWSCURCrawler and Path: 's3:// bucket name /DailyCUR/daily/daily' Path: 's3:// bucket name ' and under Exclusions after .zip add: 'aws-programmatic-access-test-object' Under AWSPutS3CURNotification: ReportKey: 'DailyCUR/daily/daily' ReportKey: '' Under AWSCURReportStatusTable: DatabaseName: athenacurcfn_daily DatabaseName: athenacurcfn and Location: 's3:// bucket name /DailyCUR/daily/cost_and_usage_data_status/' Location: 's3:// bucket name /cost_and_usage_data_status/' A modified sample is provided here: Code/crawler-cfn.yml Look for the comments: ### New line Save the template file. Go to the CloudFormation dashboard and execute the template you just created Go to the Glue dashbaord and verify that there is a single database, containing multilpe tables: 3. Tear down Delete the Glue database, select the database name, click Action and click Delete database : Delete the CloudFormation stack, select the stack, click Actions and click Delete stack : 4. Survey Thanks for taking the lab, We hope that you can take this short survey ( 2 minutes), to share your insights and help us improve our content. This survey is hosted by an external company (Qualtrics) , so the link above does not lead to our website. Please note that AWS will own the data gathered via this survey and will not share the information/results collected with survey respondents. Your responses to this survey will be subject to Amazon\u2019s Privacy Policy.","title":"Level 300: Automated CUR Updates and Ingestion"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/Lab Guide/#level-300-automated-cur-updates-and-ingestion","text":"","title":"Level 300: Automated CUR Updates and Ingestion"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/Lab Guide/#authors","text":"Nathan Besh, Cost Lead, Well-Architected Derrick Gold, Software Development Engineer, AWS Insights","title":"Authors"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/Lab Guide/#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/Lab Guide/#table-of-contents","text":"Create the CloudFormation stack Multilpe CURs Tear down Survey","title":"Table of Contents"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/Lab Guide/#1-create-the-cloudformation-stack","text":"This step is used when there is a single CUR being delivered, and have it automatically update Athena/Glue when there are new versions and new months data. We will follow the steps here: https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/setting-up-athena.html#use-athena-cf to implement the CloudFormation template, which will automatically update existing CURs, and include new CURs when they are delivered. NOTE: IAM roles will be created, these are used to: - Add event notification to existing S3 buckets - Create s3 buckets and upload objects - Create and run a Glue crawler - Create and update a Glue database and tables Please review the CloudFormation template with your security team. We will build the following solution: Log into the console as an IAM user with the required permissions. Go to the S3 dashboard, go to the bucket and folders which contain your CUR file. Open the CloudFormation(CF) file and save it locally: Here is a sample of the CF file: Go to the CloudFormation dashboard and create a stack: Load the template and click Next : Specify the details for the stack and click Next : Review the configuration, click I acknowledge that AWS CloudFormation might create IAM resources , and click Create stack : You will see the stack will start in CREATE_IN_PROGRESS : Once complete, the stack will show CREATE_COMPLETE : Click on Resources to view the resources that it will create: Go to the AWS Glue dashboard: Click on Databases and click the database starting with athenacurcfn : View the table within that database and its properties: You will see that the table is populated, the recordCount should be greater than 0. You can now go to Athena and load the partitions and view the cost and usage reports.","title":"1. Create the CloudFormation Stack"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/Lab Guide/#2-multiple-curs","text":"This step is used when there are multilpe CURs being delivered into the same bucket - for example a CUR with hourly granularity and one with daily granularity. This will automatically update Athena/Glue when there are new versions and new months data for both reports. The easiest way to work with multiple CURs is to deliver each CUR to a different S3 bucket, and follow the previous process. If you must deliver to a single bucket, configure your CURs with different prefixes or folders and follow this process. Log into the console as an IAM user with the required permissions, verify you have multiple CURs with different prefixes being delivered into the same bucket. We will have the following configuration: Format: bucket name / prefix / report_name / Configuration: bucket name /DailyCUR/daily/ bucket name /HourlyCUR/hourly/ Open the S3 console, and navigate to one of the directories where CURs are stored. Open and save the crawler-cfn.yml file: Open the file in your favourite text editor Modify the following lines to remove all references to the prefix or report name. Replace the first line with the second in each case: Under AWSCurDatabase: Name: 'athenacurcfn_daily' Name: 'athenacurcfn' Under AWSCURCrawlerComponentFunction: Resource: arn:aws:s3::: bucket name /DailyCUR/daily/daily* Resource: arn:aws:s3::: bucket name * Under AWSCURCrawler: Name: AWSCURCrawler-daily Name: AWSCURCrawler and Path: 's3:// bucket name /DailyCUR/daily/daily' Path: 's3:// bucket name ' and under Exclusions after .zip add: 'aws-programmatic-access-test-object' Under AWSPutS3CURNotification: ReportKey: 'DailyCUR/daily/daily' ReportKey: '' Under AWSCURReportStatusTable: DatabaseName: athenacurcfn_daily DatabaseName: athenacurcfn and Location: 's3:// bucket name /DailyCUR/daily/cost_and_usage_data_status/' Location: 's3:// bucket name /cost_and_usage_data_status/' A modified sample is provided here: Code/crawler-cfn.yml Look for the comments: ### New line Save the template file. Go to the CloudFormation dashboard and execute the template you just created Go to the Glue dashbaord and verify that there is a single database, containing multilpe tables:","title":"2. Multiple CURs"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/Lab Guide/#3-tear-down","text":"Delete the Glue database, select the database name, click Action and click Delete database : Delete the CloudFormation stack, select the stack, click Actions and click Delete stack :","title":"3. Tear down"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/Lab Guide/#4-survey","text":"Thanks for taking the lab, We hope that you can take this short survey ( 2 minutes), to share your insights and help us improve our content. This survey is hosted by an external company (Qualtrics) , so the link above does not lead to our website. Please note that AWS will own the data gathered via this survey and will not share the information/results collected with survey respondents. Your responses to this survey will be subject to Amazon\u2019s Privacy Policy.","title":"4. Survey "},{"location":"Reliability/","text":"AWS Well-Architected Reliability Labs Introduction This repository contains documentation and code in the format of hands-on-labs to help you learn how to learn, measure, and build using architectural best practices. The labs are categorized into levels, where 100 is introductory, 200/300 is intermediate and 400 is advanced. For more information about Reliability, read the AWS Well-Architected Reliability whitepaper . Labs: Level 300: Testing for Resiliency of EC2, RDS, and S3 License Documentation License Licensed under the Creative Commons Share Alike 4.0 license. Code LicenseLicensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"AWS Well-Architected Reliability Labs"},{"location":"Reliability/#aws-well-architected-reliability-labs","text":"","title":"AWS Well-Architected Reliability Labs"},{"location":"Reliability/#introduction","text":"This repository contains documentation and code in the format of hands-on-labs to help you learn how to learn, measure, and build using architectural best practices. The labs are categorized into levels, where 100 is introductory, 200/300 is intermediate and 400 is advanced. For more information about Reliability, read the AWS Well-Architected Reliability whitepaper .","title":"Introduction"},{"location":"Reliability/#labs","text":"Level 300: Testing for Resiliency of EC2, RDS, and S3","title":"Labs:"},{"location":"Reliability/#license","text":"","title":"License"},{"location":"Reliability/#documentation-license","text":"Licensed under the Creative Commons Share Alike 4.0 license.","title":"Documentation License"},{"location":"Reliability/#code-licenselicensed-under-the-apache-20-and-mitnoattr-license","text":"Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Code LicenseLicensed under the Apache 2.0 and MITnoAttr License."},{"location":"Reliability/300 - Testing for Resiliency of EC2, RDS, and S3/","text":"Level 300: Testing for Resiliency of EC2, RDS, and S3 Introduction The purpose if this lab is to teach you the fundamentals of using tests to ensure your implementation is resilient to failure by injecting failure modes into your application. This may be a familiar concept to companies that practice Failure Mode Engineering Analysis (FMEA). One primary capability that AWS provides is the ability to test your systems at a production scale, under load. It is not sufficient to only design for failure, you must also test to ensure that you understand how the failure will cause your systems to behave. The act of conducting these tests will also give you the ability to create playbooks how to investigate failures. You will also be able to create playbooks for identifying root causes. If you conduct these tests regularly, then you will identify changes to your application that are not resilient to failure and also create the skills to react to unexpected failures in a calm and predictable manner. In this lab, you will deploy a 3-tier resource, with a reverse proxy (Application Load Balancer), Web Application on Amazon Elastic Compute Cloud (EC2), and MySQL database using Amazon Relational Database Service (RDS). There is also an option to deploy the same stack into a different region, then using MySQL Read Replicas in the other region deployed with Amazon RDS, and then using AWS Database Migration Service to synchronize the data from the primary region into the secondary region. This will provide you the ability to progress from simpler failure testing of an application to failure testing under a simulated AWS regional failure. The skills you learn will help you build resilient workloads in alignment with the AWS Well-Architected Framework If you wish to build this code in this lab, the follow the instructions in the Builders Guide document. Goals: Reduce fear of implementing resiliency testing by providing examples in common development and scripting languages Resilience testing of EC2 instances Resilience testing of RDS Multi-AZ instances Resilience testing of S3 objects Learn how to implement resiliency using those tests Learn how to think about what a failure will cause within your infrastructure Learn how common AWS services can reduce mean time to recovery (MTTR) Prequisites: An AWS Account that you are able to use for tesintg, that is not used for production or other purposes. An Identity and Access Management (IAM) user or federated credentials into that account that has permissions to create Amazon Virtual Private Cloud(s) (VPCs), including subnets, security groups, internet gateways, NAT Gateways, Elastic IP Addresses, and route tables. The credentials must also be able to create the database subnet group needed for a Multi-AZ RDS instance. The credential will need permissions to create IAM Role, instance profiles, AWS Auto Scaling lanch configurations, application load balancers, auto scaling group, and EC2 instances. An IAM user or federated credentials into that account that has permissions to deploy the deployment automation, which consists of IAM service linked roles, AWS Lambda functions, and an AWS Step Functions state machine to execute the deployment. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Overview: Lab Guide for this lab Troubleshooting Guide for common problems encountered while deploying and conducting this lab Builders Guide for building the AWS Lambda functions and the web server and where to make changes in the lab guide to use the code you built instead of the publicly available executables. /Code Code including CloudFormation templates, Lambda functions, and resiliency tests /Images referenced by the guides License Documentation License Licensed under the Creative Commons Share Alike 4.0 license. Code LicenseLicensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 300: Testing for Resiliency of EC2, RDS, and S3"},{"location":"Reliability/300 - Testing for Resiliency of EC2, RDS, and S3/#level-300-testing-for-resiliency-of-ec2-rds-and-s3","text":"","title":"Level 300: Testing for Resiliency of EC2, RDS, and S3"},{"location":"Reliability/300 - Testing for Resiliency of EC2, RDS, and S3/#introduction","text":"The purpose if this lab is to teach you the fundamentals of using tests to ensure your implementation is resilient to failure by injecting failure modes into your application. This may be a familiar concept to companies that practice Failure Mode Engineering Analysis (FMEA). One primary capability that AWS provides is the ability to test your systems at a production scale, under load. It is not sufficient to only design for failure, you must also test to ensure that you understand how the failure will cause your systems to behave. The act of conducting these tests will also give you the ability to create playbooks how to investigate failures. You will also be able to create playbooks for identifying root causes. If you conduct these tests regularly, then you will identify changes to your application that are not resilient to failure and also create the skills to react to unexpected failures in a calm and predictable manner. In this lab, you will deploy a 3-tier resource, with a reverse proxy (Application Load Balancer), Web Application on Amazon Elastic Compute Cloud (EC2), and MySQL database using Amazon Relational Database Service (RDS). There is also an option to deploy the same stack into a different region, then using MySQL Read Replicas in the other region deployed with Amazon RDS, and then using AWS Database Migration Service to synchronize the data from the primary region into the secondary region. This will provide you the ability to progress from simpler failure testing of an application to failure testing under a simulated AWS regional failure. The skills you learn will help you build resilient workloads in alignment with the AWS Well-Architected Framework If you wish to build this code in this lab, the follow the instructions in the Builders Guide document.","title":"Introduction"},{"location":"Reliability/300 - Testing for Resiliency of EC2, RDS, and S3/#goals","text":"Reduce fear of implementing resiliency testing by providing examples in common development and scripting languages Resilience testing of EC2 instances Resilience testing of RDS Multi-AZ instances Resilience testing of S3 objects Learn how to implement resiliency using those tests Learn how to think about what a failure will cause within your infrastructure Learn how common AWS services can reduce mean time to recovery (MTTR)","title":"Goals:"},{"location":"Reliability/300 - Testing for Resiliency of EC2, RDS, and S3/#prequisites","text":"An AWS Account that you are able to use for tesintg, that is not used for production or other purposes. An Identity and Access Management (IAM) user or federated credentials into that account that has permissions to create Amazon Virtual Private Cloud(s) (VPCs), including subnets, security groups, internet gateways, NAT Gateways, Elastic IP Addresses, and route tables. The credentials must also be able to create the database subnet group needed for a Multi-AZ RDS instance. The credential will need permissions to create IAM Role, instance profiles, AWS Auto Scaling lanch configurations, application load balancers, auto scaling group, and EC2 instances. An IAM user or federated credentials into that account that has permissions to deploy the deployment automation, which consists of IAM service linked roles, AWS Lambda functions, and an AWS Step Functions state machine to execute the deployment. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prequisites:"},{"location":"Reliability/300 - Testing for Resiliency of EC2, RDS, and S3/#overview","text":"Lab Guide for this lab Troubleshooting Guide for common problems encountered while deploying and conducting this lab Builders Guide for building the AWS Lambda functions and the web server and where to make changes in the lab guide to use the code you built instead of the publicly available executables. /Code Code including CloudFormation templates, Lambda functions, and resiliency tests /Images referenced by the guides","title":"Overview:"},{"location":"Reliability/300 - Testing for Resiliency of EC2, RDS, and S3/#license","text":"","title":"License"},{"location":"Reliability/300 - Testing for Resiliency of EC2, RDS, and S3/#documentation-license","text":"Licensed under the Creative Commons Share Alike 4.0 license.","title":"Documentation License"},{"location":"Reliability/300 - Testing for Resiliency of EC2, RDS, and S3/#code-licenselicensed-under-the-apache-20-and-mitnoattr-license","text":"Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Code LicenseLicensed under the Apache 2.0 and MITnoAttr License."},{"location":"Reliability/300 - Testing for Resiliency of EC2, RDS, and S3/Builders Guide/","text":"Builders Guide for 300 - Testing for Resiliency of EC2, RDS, and S3 Introduction This guide contains the instructions for how to build the Lambda functions, the web application, and the modifications needed for the AWS CloudFormation templates' parameters as well as the JSON passed to the AWS Step Functions state machine to perform the deployment. This guide will also give some specific instructions on the limitations of how you can deploy and what AWS regions it can be run in. Prerequisites An AWS Account that you are able to use for tesintg, that is not used for production or other purposes. Python installer program (pip) Go language development environment Comfort with JSON License Documentation License Licensed under the Creative Commons Share Alike 4.0 license. Building and uploading the AWS Lambda Functions Each function also has a makefile included. This make file will use pip to install dependent packages, then zip the entire directory's contents into a zip file that will be located one directory up. You can deploy these to the region you wish to run the Lambda functions using the AWS Command Line Interface (CLI) as follows: % cd LambdaDirectory % make % cd .. % aws s3 cp lambda .zip s3:// S3 bucket / directory prefix / lambda .zip Debugging the AWS Lambda Functions The Lambda functions are all written in Python. They can be run on the command line with the python debugger, pdb, as follows: % python -m pdb lambda_function .py The lambda functions all have an event that is passed in the main function that can be used to test your environment. The parameters are the same as they are to the AWS Step Functions state machine: log_level: This is the python logger logging level. To make it verbose in the logs, use the value \"DEBUG\" region_name: This is the region that the infrastructure is going to be deployed to secondary_region_name: This is the region where the red replica for this region will be deployed. (optional) workshop: A name to be added to the tags of the deployed infrastructure cfn_region: This is the region where the bucket that contains the AWS CloudFormation template is located cfn_bucket: This is the name of the S3 bucket where the AWS CloudFormation template is stored. folder: This is the apparent \"folder\" (actually a key prefix) where the CloudFormation template is located in the cfn_bucket. boot_bucket: This is the bucket in the region_name where the boot scripts and executables are located. boot_prefix: This is the apparent \"folder\" (actually a key prefix) where the boot scripts and executables are located. boot_object: This is the script executed on the instances to bootstrap the application. This is an JSON string that looks like the following: { 'log_level' : 'DEBUG', 'region_name' : 'us-west-2', 'secondary_region_name' : 'us-east-2', 'workshop' : '300 - Testing for Resiliency', 'cfn_region' : 'us-east-2', 'cfn_bucket' : 'aws-well-architected-labs-ohio', 'folder' : 'Reliability/', 'boot_bucket' : 'aws-well-architected-labs-ohio', 'boot_prefix' : 'Reliability/', 'boot_object' : 'bootstrap300Reliability.sh', 'websiteimage' : 'https://s3.us-east-2.amazonaws.com/arc327-well-architected-for-reliability/Cirque_of_the_Towers.jpg' } There is considerable \"shared knowledge\" between the state machine functions that is all hard-coded, like stack names. The state machine passes state of stacks between functions to indicate if the stack has been deployed or not. These take the form of a nested JSON object: { 'vpc' : { 'stackname' : 'ResiliencyVPC', 'status' : 'CREATE_COMPLETE' } } There will be a status for each stack as they deploy to prevent any attempt to deploy when a previous stack is either not present, or not complete. The applications all have the relevant nested stacks passed in the debug event, so you need to ensure you test them in the same order that the state machine deploys them within. The Troubleshooting guide has additional details on how to debug the function when it is executing in AWS Lambda. Building and Uploading the Web Application The web application is written in the Go programming langauage. You must have the go language installed where you are building the executable. There is also a makefile to build this application. You can also upload the executable using the same method as follows: % cd go % make % aws s3 cp FragileWebApp s3:// S3 bucket / diretory prefix /FragileWebapp The web application is very fragile in that it will always write an entry on every hit it receives. This will cause the application to be tightly coupled to the database (a violation of the AWS Well-Architected Reliability Pillar!). However, it is small and easy to understand and deploy. The Bootstrapping Script The bootstrapping script assumes 4 things: The name of the SQL to run to create the table used is hardcoded to \"createIPTable.sql\" The password is hardcoded to match the hardcoded password in the CloudFormation template that creates the RDS instance. The name of the Executable is \"FragileWebApp\" The bucket location(s) should really be passed as a 5th and/or 6th command line variable and is marked as TODOs. The SQL in the Bootstrapping Script The database and table are hard coded to match what the executable is expecting. There are also commands required to support AWS Database Migration Service (DMS) replication to set the retention configuration of the binlog, and add permisssions for the user that AWS DMS uses. Deploying the State Machine The AWS Step Functions state machine must be deployed in the same region as the bucket where you uploaded the zipped code. This is because the Lambda functions can only be created in the same AWS Region as the location of the bucket. In addition, the Lambda functions must be in the same AWS Region as the state machine in order for the state machine to invoke it. CloudFormation templates The CloudFomation templates and the bootstrapping scripts need to be deployed in the same region. This is not a limitation, except for the fact that the parameters built in the Lambda function make this assumption. Also, the Amazon Machine Images (AMIs) for the web servers are only mapped into us-east-2 (Ohio) and us-west-2 (Oregon).","title":"Builders Guide for 300 - Testing for Resiliency of EC2, RDS, and S3"},{"location":"Reliability/300 - Testing for Resiliency of EC2, RDS, and S3/Builders Guide/#builders-guide-for-300-testing-for-resiliency-of-ec2-rds-and-s3","text":"","title":"Builders Guide for 300 - Testing for Resiliency of EC2, RDS, and S3"},{"location":"Reliability/300 - Testing for Resiliency of EC2, RDS, and S3/Builders Guide/#introduction","text":"This guide contains the instructions for how to build the Lambda functions, the web application, and the modifications needed for the AWS CloudFormation templates' parameters as well as the JSON passed to the AWS Step Functions state machine to perform the deployment. This guide will also give some specific instructions on the limitations of how you can deploy and what AWS regions it can be run in.","title":"Introduction"},{"location":"Reliability/300 - Testing for Resiliency of EC2, RDS, and S3/Builders Guide/#prerequisites","text":"An AWS Account that you are able to use for tesintg, that is not used for production or other purposes. Python installer program (pip) Go language development environment Comfort with JSON","title":"Prerequisites"},{"location":"Reliability/300 - Testing for Resiliency of EC2, RDS, and S3/Builders Guide/#license","text":"","title":"License"},{"location":"Reliability/300 - Testing for Resiliency of EC2, RDS, and S3/Builders Guide/#documentation-license","text":"Licensed under the Creative Commons Share Alike 4.0 license.","title":"Documentation License"},{"location":"Reliability/300 - Testing for Resiliency of EC2, RDS, and S3/Builders Guide/#building-and-uploading-the-aws-lambda-functions","text":"Each function also has a makefile included. This make file will use pip to install dependent packages, then zip the entire directory's contents into a zip file that will be located one directory up. You can deploy these to the region you wish to run the Lambda functions using the AWS Command Line Interface (CLI) as follows: % cd LambdaDirectory % make % cd .. % aws s3 cp lambda .zip s3:// S3 bucket / directory prefix / lambda .zip","title":"Building and uploading the AWS Lambda Functions"},{"location":"Reliability/300 - Testing for Resiliency of EC2, RDS, and S3/Builders Guide/#debugging-the-aws-lambda-functions","text":"The Lambda functions are all written in Python. They can be run on the command line with the python debugger, pdb, as follows: % python -m pdb lambda_function .py The lambda functions all have an event that is passed in the main function that can be used to test your environment. The parameters are the same as they are to the AWS Step Functions state machine: log_level: This is the python logger logging level. To make it verbose in the logs, use the value \"DEBUG\" region_name: This is the region that the infrastructure is going to be deployed to secondary_region_name: This is the region where the red replica for this region will be deployed. (optional) workshop: A name to be added to the tags of the deployed infrastructure cfn_region: This is the region where the bucket that contains the AWS CloudFormation template is located cfn_bucket: This is the name of the S3 bucket where the AWS CloudFormation template is stored. folder: This is the apparent \"folder\" (actually a key prefix) where the CloudFormation template is located in the cfn_bucket. boot_bucket: This is the bucket in the region_name where the boot scripts and executables are located. boot_prefix: This is the apparent \"folder\" (actually a key prefix) where the boot scripts and executables are located. boot_object: This is the script executed on the instances to bootstrap the application. This is an JSON string that looks like the following: { 'log_level' : 'DEBUG', 'region_name' : 'us-west-2', 'secondary_region_name' : 'us-east-2', 'workshop' : '300 - Testing for Resiliency', 'cfn_region' : 'us-east-2', 'cfn_bucket' : 'aws-well-architected-labs-ohio', 'folder' : 'Reliability/', 'boot_bucket' : 'aws-well-architected-labs-ohio', 'boot_prefix' : 'Reliability/', 'boot_object' : 'bootstrap300Reliability.sh', 'websiteimage' : 'https://s3.us-east-2.amazonaws.com/arc327-well-architected-for-reliability/Cirque_of_the_Towers.jpg' } There is considerable \"shared knowledge\" between the state machine functions that is all hard-coded, like stack names. The state machine passes state of stacks between functions to indicate if the stack has been deployed or not. These take the form of a nested JSON object: { 'vpc' : { 'stackname' : 'ResiliencyVPC', 'status' : 'CREATE_COMPLETE' } } There will be a status for each stack as they deploy to prevent any attempt to deploy when a previous stack is either not present, or not complete. The applications all have the relevant nested stacks passed in the debug event, so you need to ensure you test them in the same order that the state machine deploys them within. The Troubleshooting guide has additional details on how to debug the function when it is executing in AWS Lambda.","title":"Debugging the AWS Lambda Functions"},{"location":"Reliability/300 - Testing for Resiliency of EC2, RDS, and S3/Builders Guide/#building-and-uploading-the-web-application","text":"The web application is written in the Go programming langauage. You must have the go language installed where you are building the executable. There is also a makefile to build this application. You can also upload the executable using the same method as follows: % cd go % make % aws s3 cp FragileWebApp s3:// S3 bucket / diretory prefix /FragileWebapp The web application is very fragile in that it will always write an entry on every hit it receives. This will cause the application to be tightly coupled to the database (a violation of the AWS Well-Architected Reliability Pillar!). However, it is small and easy to understand and deploy.","title":"Building and Uploading the Web Application"},{"location":"Reliability/300 - Testing for Resiliency of EC2, RDS, and S3/Builders Guide/#the-bootstrapping-script","text":"The bootstrapping script assumes 4 things: The name of the SQL to run to create the table used is hardcoded to \"createIPTable.sql\" The password is hardcoded to match the hardcoded password in the CloudFormation template that creates the RDS instance. The name of the Executable is \"FragileWebApp\" The bucket location(s) should really be passed as a 5th and/or 6th command line variable and is marked as TODOs.","title":"The Bootstrapping Script"},{"location":"Reliability/300 - Testing for Resiliency of EC2, RDS, and S3/Builders Guide/#the-sql-in-the-bootstrapping-script","text":"The database and table are hard coded to match what the executable is expecting. There are also commands required to support AWS Database Migration Service (DMS) replication to set the retention configuration of the binlog, and add permisssions for the user that AWS DMS uses.","title":"The SQL in the Bootstrapping Script"},{"location":"Reliability/300 - Testing for Resiliency of EC2, RDS, and S3/Builders Guide/#deploying-the-state-machine","text":"The AWS Step Functions state machine must be deployed in the same region as the bucket where you uploaded the zipped code. This is because the Lambda functions can only be created in the same AWS Region as the location of the bucket. In addition, the Lambda functions must be in the same AWS Region as the state machine in order for the state machine to invoke it.","title":"Deploying the State Machine"},{"location":"Reliability/300 - Testing for Resiliency of EC2, RDS, and S3/Builders Guide/#cloudformation-templates","text":"The CloudFomation templates and the bootstrapping scripts need to be deployed in the same region. This is not a limitation, except for the fact that the parameters built in the Lambda function make this assumption. Also, the Amazon Machine Images (AMIs) for the web servers are only mapped into us-east-2 (Ohio) and us-west-2 (Oregon).","title":"CloudFormation templates"},{"location":"Reliability/300 - Testing for Resiliency of EC2, RDS, and S3/Lab Guide/","text":"Level 300: Performing Resiliency Testing for EC2, RDS, and S3 Authors Rodney Lester, Reliability Lead, Well-Architected, AWS Table of Contents Deploying the infrastructure Discussion and Example Failure Scenarios Failure modes Tear Down 1. Deploying the infrastructure AWS requires \u201cService-Linked\u201d Roles for AWS Auto Scaling, Elastic Load Balancing, and Amazon RDS to create the services and metrics they manage. In the past, these Roles may have been created automatically for you, or we may need to create them. Here is how to find which roles you need to create. 1.1 Checking for Existing Service-Linked Roles Sign in to the AWS Management Console as an IAM user with MFA enabled or in a federated Role, and open the IAM console at https://console.aws.amazon.com/iam/ . In the navigation pane, click Roles . In the filter box, type \u201cService\u201d to find the service linked roles that exist in your account and look for \u201cAutoScaling,\u201d \u201cELB\u201d or \u201cElasticLoadBalancing,\u201d and \u201cRDS.\u201d In this screenshot, the service linked role for AutoScaling exists, but the roles for ELB and RDS do not. Note which roles will need to be created as you will use this information when performing the next step. In the AWS Services Search Box, type \u201cCloudFormation\u201d and click enter. You will need to download the CloudFormation template that will deploy the lambda functions and step functions state machine. Change the region to Ohio and navigate to the CloudFormation console. On the CloudFormation console, click \u201cCreate Stack:\u201d. There are two versions that can be deployed. You can deploy in one AWS region, which will allow you start testing sooner, or you can deploy into 2 AWS regions, which will enable you to test some additional aspects of S3, as well as as simulation a regional failure of your application For a single region deployment, select the option to \u201cSpecify an Amazon S3 template\" and enter https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/lambda_functions_for_deploy.json For a two region deployment, select the option to \u201cSpecify an Amazon S3 template\" and enter https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/lambda_functions_for_deploy_two_regions.json Click the \u201cNext\u201d button. On this page you will enter the following information: For the single region deployment: Stack name: \u201cDeployResiliencyWorkshop\u201d -No spaces! EnableAutoScalingServiceRole: \u201cfalse\u201d or \u201ctrue,\u201d depending on whether it exists. In the example, this will be false since it already exists. EnableELBServiceRole: \u201cfalse\u201d or \u201ctrue,\u201d depending on whether it exists. In the example, this will be true because it does not exist. EnableRDSServiceRole: \u201cfalse\u201d or \u201ctrue,\u201d depending on whether it exists. In the example, this will be true because it does not exist. LambdaFunctionsBucket: \u201caws-well-architected-labs-ohio\u201d -Case sensitive! RDSLambdaKey: \u201cReliability/RDSLambda.zip\u201d -Case sensitive! VPCLambdaKey: \u201cReliability/VPCLambda.zip\u201d -Case sensitive! WaitForStackLambdaKey: \u201cReliability/WaitForStack.zip\u201d -Case sensitive! WebAppLambdaKey: \u201cReliability/WebAppLambda.zip\u201d -Case sensitive! For the two region deployment: Stack name: \u201cDeployResiliencyWorkshop\u201d -No spaces! DMSLambdaKey: \u201cReliability/DMSLambda.zip\u201d -Case sensitive! EnableAutoScalingServiceRole: \u201cfalse\u201d or \u201ctrue,\u201d depending on whether it exists. In the example, this will be false since it already exists. EnableELBServiceRole: \u201cfalse\u201d or \u201ctrue,\u201d depending on whether it exists. In the example, this will be true because it does not exist. EnableRDSServiceRole: \u201cfalse\u201d or \u201ctrue,\u201d depending on whether it exists. In the example, this will be true because it does not exist. LambdaFunctionsBucket: \u201caws-well-architected-labs-ohio\u201d -Case sensitive! RDSLambdaKey: \u201cReliability/RDSLambda.zip\u201d -Case sensitive! RDSRRLambdaKey: \"Reliability/RDSReadReplicaLambda.zip\" -Case sensitive! VPCLambdaKey: \u201cReliability/VPCLambda.zip\u201d -Case sensitive! WaitForStackLambdaKey: \u201cReliability/WaitForStack.zip\u201d -Case sensitive! WebAppLambdaKey: \u201cReliability/WebAppLambda.zip\u201d -Case sensitive! Click the \u201cNext\u201d button. On the \u201cOptions\u201d page, click the \u201cNext\u201d button at the bottom of the page. On the \u201cReview\u201d page, scroll to the bottom and select the option \u201cI acknowledge that AWS CloudFormation might create IAM resources.\u201d Click the \u201cCreate\u201d button. This will take you to the summary with the stack creation in progress. This will take approximately a minute to deploy. You now need to navigate to the Step Functions console. At the top of the window, click the downward facing icon to the right of the word \u201cServices.\u201d This will bring up the list of services. Type \u201cStep Functions\u201d in the search box and press the enter key. On the Step Functions dashboard, you will see \u201cState Machines\u201d and you will have a new one named \u201cDeploymentMachine- .\u201d Click on that state machine. This will bring up an execution console. Click on the \u201cStart execution\u201d button. For input to the execution name, use \u201cBuildResiliency.\u201d One Region Deployment: For Input, use the following: Note: If you want to test failure of S3, then you should use an image in S3 that you control, and it should have public read access only. { log_level : DEBUG , region_name : us-east-2 , secondary_region_name : us-west-2 , cfn_region : us-east-2 , cfn_bucket : aws-well-architected-labs-ohio , folder : Reliability/ , workshop : 300-ResiliencyofEC2RDSandS3 , boot_bucket : aws-well-architected-labs-ohio , boot_prefix : Reliability/ , boot_object : bootstrap300Resiliency.sh , websiteimage : https://s3.us-east-2.amazonaws.com/arc327-well-architected-for-reliability/Cirque_of_the_Towers.jpg } 1. Then click the \u201cStart Execution\u201d button. 1. Two Region Deployment: 1. For Input, use the following: Note: If you want to test failure of S3, then you should use an image in S3 that you control, and it should have public read access only. { region1 : { log_level : DEBUG , region_name : us-east-2 , secondary_region_name : us-west-2 , cfn_region : us-east-2 , cfn_bucket : aws-well-architected-labs-ohio , folder : Reliability/ , workshop : 300-ResiliencyofEC2RDSandS3 , boot_bucket : aws-well-architected-labs-ohio , boot_prefix : Reliability/ , boot_object : bootstrap300Resiliency.sh , websiteimage : https://s3.us-east-2.amazonaws.com/arc327-well-architected-for-reliability/Cirque_of_the_Towers.jpg }, region2 : { log_level : DEBUG , region_name : us-west-2 , secondary_region_name : us-east-2 , cfn_region : us-east-2 , cfn_bucket : aws-well-architected-labs-ohio , folder : Reliability/ , workshop : 300-ResiliencyofEC2RDSandS3 , boot_bucket : aws-well-architected-labs-ohio , boot_prefix : Reliability/ , boot_object : bootstrap300Resiliency.sh , websiteimage : https://s3.us-east-2.amazonaws.com/arc327-well-architected-for-reliability/Cirque_of_the_Towers.jpg } } 1. Then click the \u201cStart Execution\u201d button. 14. This will take approximately 20-25 minutes for one region to deploy and approximately 45-50 minutes for two regions to deploy. You will have enough to start executing the lab exercises for the two region in 25-30 minutes. You can watch the state machine as it executes by clicking the icon to expand the visual workflow to the full screen. 1. One Region: 1. Two Regions: 15. You can also watch the CloudFormation stacks as they are created. If you are in a workshop, the instructor will have some background information to share while this is created. You can resume testing when the web tier has been deployed in the Ohio region. This will look something like this on the visual workflow. 16. You can start testing: 1. When the WaitForWebApp step for a single region deployment is completed, return to the CloudFormation console and select the \u201cWebServersforResiliencyTesting\u201d stack and then the Outputs tab at bottom. 1. When the WaitForWebApp1 step for a two region deployment is completed, return to the CloudFormation console and select the \u201cWebServersforResiliencyTesting\u201d stack and then the Outputs tab at bottom. 17. Click the value and it will bring up the website: 2. Discussion and Example Failure Scenarios There is a choice of environments to execute the failure simulations in. Linux command line (bash), Python, Java, and C#. The instructions for each environment are in separate sections. 2.1 Setting Up the bash Environment All the command line scripts use a utility called jq. You can download it from the site and leave it in your local directory, as long as that is in your execution path: https://stedolan.github.io/jq/ 1. You can find out what your execution path is with the following command. $ echo $PATH /usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/aws/bin:/home/ec2-user/.local/bin:/home/ec2-user/bin If you have sudo rights, then copy the executable to /usr/local/bin/jq and make it executable. $ sudo cp jq-linux64 /usr/local/bin/jq $ sudo chmod 755 /usr/local/bin/jq If you do not have sudo rights, then copy it into your home directory under a /bin directory. In Amazon linux, this is typically /home/ec2-user/bin. $ cp jq-linux64 ~/bin/jq $ chmod 755 ~/bin/jq Install the AWS Command Line Interface (CLI) if you do not have it installed (it is installed by default on Amazon Linux). https://aws.amazon.com/cli/ Run the aws configure command to configure your command line options. This will prompt you for the AWS Access Key ID, AWS Secret Access Key, and default region name. Enter the key information if you do not already have them installed, and set the default region to \u201cus-east-2\u201d and leave the default output format as \u201cNone.\u201d. $ aws configure AWS Access Key ID [*************xxxx]: Your AWS Access Key ID AWS Secret Access Key [**************xxxx]: Your AWS Secret Access Key Default region name: [us-east-2]: us-east-2 Default output format [None]: return Download the zip file of the resiliency bash scripts at the following URL: https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/bashresiliency.zip Unzip the folder in a location convenient for you to execute the scripts. They are also available in the Code/FailureSimulations/bash/ directory. 2.2 Setting up a Programming Language Based Environment You will need the same files that the AWS command line uses for credentials. You can either install the command line and use the \u2018aws configure\u2019 command as outlined in the bash set up, or you can manually create the configuration files. To create the files manually, create a .aws folder/directory in your home directory. 1. Bash and powershell use the same command. mkdir ~/.aws Change directory to that directory to create the configuration file. Bash cd ~/.aws Powershell cd ~\\.aws Use a text editor (vim, emacs, notepad, wordpad) to create a text file (no extension named \u201ccredentials.\u201d In this file you should have the following text. [default] aws_access_key_id = Your access key aws_secret_access_key = Your secret key region = us-east-2 2.3 Setting Up the Python Environment The scripts are written in python with boto3. On Amazon Linux, this is already installed. Use your local operating system instructions to install boto3: https://github.com/boto/boto3 Download the zip file of the resiliency scripts at the following URL. https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/pythonresiliency.zip Unzip the folder in a location convenient for you to execute the scripts. 2.4 Setting Up the Java Environment The command line utility in Java requires Java 8 SE. In Amazon Linux, you need to install Java 8 and remove Java 7. $ sudo yum install java-1.8.0-openjdk $ sudo yum remove java-1.7.0-openjdk Download the zipfile of the executables at the following URL https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/javaresiliency.zip . Unzip the folder in a location convenient for you to execute the command line programs. 2.5 Setting Up the C# Environment Download the zipfile of the executables at the following URL. https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/csharpresiliency.zip Unzip the folder in a location convenient for you to execute the command line programs. 2.6 Setting up the Powershell Environment If you do not have the AWS Tools for Powershell, download and install them following the instructions here. https://aws.amazon.com/powershell/ Follow the \u201cGetting Started\u201d instructions for configuring credentials. https://docs.aws.amazon.com/powershell/latest/userguide/pstools-getting-started.html Download the zipfile of the scripts at the following URL. https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/powershellresiliency.zip Unzip the folder in a location convenient for you to execute the scripts. 3. Failure modes 3.1 EC2 Failure Mode The first failure mode will be to fail a web server. To prepare for this, you should have two consoles open: VPC and EC2. From the AWS Console, click the downward facing icon to the right of the word \u201cServices.\u201d This will bring up the list of services. Type \u201cEC2\u201d in the search box and press the enter key. You also need the VPC Console. From the AWS Console, click the downward facing icon to the right of the word \u201cServices.\u201d This will bring up the list of services. Type \u201cVPC\u201d in the search box, then right click on the \u201cVPC Isolate Cloud Resources\u201d text and open the link in a new tab or window. You can then click the upward facing icon to the right of the word \u201cServices\u201d to make the menu of services disappear. On the EC2 Console, click \u201cInstances\u201d on the left side to bring up the list of instances. Use this as the command line argument to the scripts/programs below. Instance Failure in bash Execute the failure mode script for failing an instance: $./fail_instance.sh vpc-id Instance Failure in Python: Execute the failure mode script for failing an instance: $ python fail_instance.py vpc-id Instance Failure in Java: Execute the failure mode program for failing an instance: $ java -jar app-resiliency-1.0.jar EC2 vpc-id Instance Failure in C#: Execute the failure mode program for failing an instance: $ .\\AppResiliency EC2 vpc-id Instance Failure in Powershell: Execute the failure mode script for failing an instance: $ .\\fail_instance.ps1 vpc-id Watch the behavior of the Load Balancer Target Group and its Targets in the EC2 Console. See it get marked unhealthy and replaced by the Auto Scaling Group. 3.2 RDS Failure Mode From the AWS EC2 Console (you will still need the VPC ID from the VPC Console), click the downward facing icon to the right of the word \u201cServices.\u201d This will bring up the list of services. Type \u201cRDS\u201d in the search box and press the enter key. Use this as the command line argument to the scripts/programs below. RDS Instance Failure in bash: Execute the failure mode script for failing over an RDS instance: $./failover_rds.sh vpc-id RDS Instance Failure in Python: Execute the failure mode script for failing an instance: $ python fail_rds.py vpc-id RDS Instance Failure in Java: Execute the failure mode program for failing an instance: $ java -jar app-resiliency-1.0.jar RDS vpc-id RDS Instance Failure in C#: Execute the failure mode program for failing an instance: $ .\\AppResiliency RDS vpc-id RDS Instance Failure in Powershell: Execute the failure mode script for failing an instance: $ .\\failover_rds.ps1 vpc-id Watch the behavior of the website. What happens? Watch the EC2 Load Balancer Target Group and its Targets\u2019 health. Do the instances recover? 3.3 AZ Failure Availability zone failure. AZ Failure in bash: Execute the following script for failing over an Availability Zone: $./fail_az.sh az vpc-id AZ Failure in Python: Execute the failure mode script for failing an instance: $ python fail_az.py vpc-id az AZ Failure in Java: Execute the failure mode script for failing an instance: $ java -jar app-resiliency-1.0.jar AZ vpc-id az AZ Failure in C#: Execute the failure mode script for failing an instance: $ .\\AppResiliency AZ vpc-id az AZ Failure in Powershell: Execute the failure mode script for failing an instance: $ .\\fail_az.ps1 az vpc-id What is the expected effect? How long does it take to take effect? Look at the Target Group Targets to see them go unhealthy, also watch the EC2 instances to see the one in the target AZ shutdown and be restarted in one of the other AZs. What would you do if the ASG was only in one AZ? You could call the AutoScaling SuspendProcesses and then get the list of instances in the group and call EC2 StopInstances or TerminateInstances How would you undo all these changes? 3.4 Failure of S3 Failure of S3 means that the image will not be available. Failure in bash: The bash commands available do not allow for modification of the access permissions, so you'll have to do the work in the console. Navigate to the S3 console: [https://s3.console.aws.amazon.com/s3/home](https://s3.console.aws.amazon.com/s3/home] Select the bucket where the image is located. In my example, this is the bucket \"arc327-well-architected-for-reliability\" Select the object, then select the \"Permissions\" tab: Select the \"Public Access\" radio button, and deselect the \"Read object\" box: What is the expected effect? How long does it take to take effect? How would you diagnose if this is a larger problem than permissions? 3.5 Looking for more to do? You can use drift detection in the CloudFormation console to see what had chanegd, or work on code to heal their failure modes. 1. Remove the network ACLs they added 2. Reconfigure the AutoScaling Groups to use the AZ 4. Tear down this lab In order to take down the lab environment, you will need to remove the association of the Network ACL that was added to fail the AZ, and delete the Network ACL in the console. You can then delete the CloudFormation stacks (in both AWS regions) that were created. Here are the instructions for how to do that. 1. Navigate to the VPC Console. From the AWS Console, click the downward facing icon to the right of the word \u201cServices.\u201d This will bring up the list of services. Type \u201cVPC\u201d in the search box, then press enter. On the VPC Console, click on Network ACLs and select the Network ACL that is associated with 4 subnets in the ResiliencyVPC. Click the \u201cSubnet Associations\u201d tab and then Click the \u201cEdit button. You need to associate the 2 subnets that were changed to the blocking Network ACL back to this default Network ACL. Select the 2 subnets available, then click the \u201cSave\u201d button. Now select the Network ACL that is associated with 0 Subnets and click the \u201cDelete\u201d button to delete this Network ACL. If you have deployed into 2 regions, you have to delete the Database Migration Service and RDS Read Replicas before you can delete the RDS instances. You can delete the DMS (in Oregon) and read replica at the same time, but the console only allows you to select one of the stacks at a time. Select the \u201cDMSforResiliencyTesting\u201d stack, then click the \u201cActions\u201d button, and click \u201cDeleteStack:\u201d. You can then select the \u201c MySQLReadReplicaResiliencyTesting\u201d stack, click the \u201cActions\u201d button, and click \u201cDelete Stack\u201d to simultaneously delete the RDS Read Replica instance. If you have deployed in 2 regions, Navigate to the other region (Ohio, if you started in Oregon, and Oregon, if you started in Ohio), and perform the same deletion steps above for the RDS read replica and DMS, if in Oregon. Wait for the read replicas to be deleted. You can delete the web servers and RDS at the same time, but the console only allows you to select one of the stacks at a time. Select the \u201cWebServersforResiliencyTesting\u201d stack, then click the \u201cActions\u201d button, and click \u201cDelete Stack:\u201d. You can then select the \u201c MySQLforResiliencyTesting\u201d stack, click the \u201cActions\u201d button, and click \u201cDelete Stack\u201d to simultaneously delete the RDS instance. You can then select the \u201cDeployResiliencyWorkshop\u201d stack, click the \u201cActions\u201d button, and click \u201cDelete Stack\u201d to simultaneously delete the deployment machine and Lambda functions. Once all these stacks have deleted, you can then select the \u201cResiliencyVPC\u201d stack, click the \u201cActions\u201d button, and click \u201cDelete Stack\u201d to delete the VPC and remove the last piece of infrastructure. When it is delete complete, you are done. References useful resources: License Documentation License Licensed under the Creative Commons Share Alike 4.0 license. Code License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 300: Performing Resiliency Testing for EC2, RDS, and S3"},{"location":"Reliability/300 - Testing for Resiliency of EC2, RDS, and S3/Lab Guide/#level-300-performing-resiliency-testing-for-ec2-rds-and-s3","text":"","title":"Level 300: Performing Resiliency Testing for EC2, RDS, and S3"},{"location":"Reliability/300 - Testing for Resiliency of EC2, RDS, and S3/Lab Guide/#authors","text":"Rodney Lester, Reliability Lead, Well-Architected, AWS","title":"Authors"},{"location":"Reliability/300 - Testing for Resiliency of EC2, RDS, and S3/Lab Guide/#table-of-contents","text":"Deploying the infrastructure Discussion and Example Failure Scenarios Failure modes Tear Down","title":"Table of Contents"},{"location":"Reliability/300 - Testing for Resiliency of EC2, RDS, and S3/Lab Guide/#1-deploying-the-infrastructure","text":"AWS requires \u201cService-Linked\u201d Roles for AWS Auto Scaling, Elastic Load Balancing, and Amazon RDS to create the services and metrics they manage. In the past, these Roles may have been created automatically for you, or we may need to create them. Here is how to find which roles you need to create.","title":"1. Deploying the infrastructure "},{"location":"Reliability/300 - Testing for Resiliency of EC2, RDS, and S3/Lab Guide/#11-checking-for-existing-service-linked-roles","text":"Sign in to the AWS Management Console as an IAM user with MFA enabled or in a federated Role, and open the IAM console at https://console.aws.amazon.com/iam/ . In the navigation pane, click Roles . In the filter box, type \u201cService\u201d to find the service linked roles that exist in your account and look for \u201cAutoScaling,\u201d \u201cELB\u201d or \u201cElasticLoadBalancing,\u201d and \u201cRDS.\u201d In this screenshot, the service linked role for AutoScaling exists, but the roles for ELB and RDS do not. Note which roles will need to be created as you will use this information when performing the next step. In the AWS Services Search Box, type \u201cCloudFormation\u201d and click enter. You will need to download the CloudFormation template that will deploy the lambda functions and step functions state machine. Change the region to Ohio and navigate to the CloudFormation console. On the CloudFormation console, click \u201cCreate Stack:\u201d. There are two versions that can be deployed. You can deploy in one AWS region, which will allow you start testing sooner, or you can deploy into 2 AWS regions, which will enable you to test some additional aspects of S3, as well as as simulation a regional failure of your application For a single region deployment, select the option to \u201cSpecify an Amazon S3 template\" and enter https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/lambda_functions_for_deploy.json For a two region deployment, select the option to \u201cSpecify an Amazon S3 template\" and enter https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/lambda_functions_for_deploy_two_regions.json Click the \u201cNext\u201d button. On this page you will enter the following information: For the single region deployment: Stack name: \u201cDeployResiliencyWorkshop\u201d -No spaces! EnableAutoScalingServiceRole: \u201cfalse\u201d or \u201ctrue,\u201d depending on whether it exists. In the example, this will be false since it already exists. EnableELBServiceRole: \u201cfalse\u201d or \u201ctrue,\u201d depending on whether it exists. In the example, this will be true because it does not exist. EnableRDSServiceRole: \u201cfalse\u201d or \u201ctrue,\u201d depending on whether it exists. In the example, this will be true because it does not exist. LambdaFunctionsBucket: \u201caws-well-architected-labs-ohio\u201d -Case sensitive! RDSLambdaKey: \u201cReliability/RDSLambda.zip\u201d -Case sensitive! VPCLambdaKey: \u201cReliability/VPCLambda.zip\u201d -Case sensitive! WaitForStackLambdaKey: \u201cReliability/WaitForStack.zip\u201d -Case sensitive! WebAppLambdaKey: \u201cReliability/WebAppLambda.zip\u201d -Case sensitive! For the two region deployment: Stack name: \u201cDeployResiliencyWorkshop\u201d -No spaces! DMSLambdaKey: \u201cReliability/DMSLambda.zip\u201d -Case sensitive! EnableAutoScalingServiceRole: \u201cfalse\u201d or \u201ctrue,\u201d depending on whether it exists. In the example, this will be false since it already exists. EnableELBServiceRole: \u201cfalse\u201d or \u201ctrue,\u201d depending on whether it exists. In the example, this will be true because it does not exist. EnableRDSServiceRole: \u201cfalse\u201d or \u201ctrue,\u201d depending on whether it exists. In the example, this will be true because it does not exist. LambdaFunctionsBucket: \u201caws-well-architected-labs-ohio\u201d -Case sensitive! RDSLambdaKey: \u201cReliability/RDSLambda.zip\u201d -Case sensitive! RDSRRLambdaKey: \"Reliability/RDSReadReplicaLambda.zip\" -Case sensitive! VPCLambdaKey: \u201cReliability/VPCLambda.zip\u201d -Case sensitive! WaitForStackLambdaKey: \u201cReliability/WaitForStack.zip\u201d -Case sensitive! WebAppLambdaKey: \u201cReliability/WebAppLambda.zip\u201d -Case sensitive! Click the \u201cNext\u201d button. On the \u201cOptions\u201d page, click the \u201cNext\u201d button at the bottom of the page. On the \u201cReview\u201d page, scroll to the bottom and select the option \u201cI acknowledge that AWS CloudFormation might create IAM resources.\u201d Click the \u201cCreate\u201d button. This will take you to the summary with the stack creation in progress. This will take approximately a minute to deploy. You now need to navigate to the Step Functions console. At the top of the window, click the downward facing icon to the right of the word \u201cServices.\u201d This will bring up the list of services. Type \u201cStep Functions\u201d in the search box and press the enter key. On the Step Functions dashboard, you will see \u201cState Machines\u201d and you will have a new one named \u201cDeploymentMachine- .\u201d Click on that state machine. This will bring up an execution console. Click on the \u201cStart execution\u201d button. For input to the execution name, use \u201cBuildResiliency.\u201d One Region Deployment: For Input, use the following: Note: If you want to test failure of S3, then you should use an image in S3 that you control, and it should have public read access only. { log_level : DEBUG , region_name : us-east-2 , secondary_region_name : us-west-2 , cfn_region : us-east-2 , cfn_bucket : aws-well-architected-labs-ohio , folder : Reliability/ , workshop : 300-ResiliencyofEC2RDSandS3 , boot_bucket : aws-well-architected-labs-ohio , boot_prefix : Reliability/ , boot_object : bootstrap300Resiliency.sh , websiteimage : https://s3.us-east-2.amazonaws.com/arc327-well-architected-for-reliability/Cirque_of_the_Towers.jpg } 1. Then click the \u201cStart Execution\u201d button. 1. Two Region Deployment: 1. For Input, use the following: Note: If you want to test failure of S3, then you should use an image in S3 that you control, and it should have public read access only. { region1 : { log_level : DEBUG , region_name : us-east-2 , secondary_region_name : us-west-2 , cfn_region : us-east-2 , cfn_bucket : aws-well-architected-labs-ohio , folder : Reliability/ , workshop : 300-ResiliencyofEC2RDSandS3 , boot_bucket : aws-well-architected-labs-ohio , boot_prefix : Reliability/ , boot_object : bootstrap300Resiliency.sh , websiteimage : https://s3.us-east-2.amazonaws.com/arc327-well-architected-for-reliability/Cirque_of_the_Towers.jpg }, region2 : { log_level : DEBUG , region_name : us-west-2 , secondary_region_name : us-east-2 , cfn_region : us-east-2 , cfn_bucket : aws-well-architected-labs-ohio , folder : Reliability/ , workshop : 300-ResiliencyofEC2RDSandS3 , boot_bucket : aws-well-architected-labs-ohio , boot_prefix : Reliability/ , boot_object : bootstrap300Resiliency.sh , websiteimage : https://s3.us-east-2.amazonaws.com/arc327-well-architected-for-reliability/Cirque_of_the_Towers.jpg } } 1. Then click the \u201cStart Execution\u201d button. 14. This will take approximately 20-25 minutes for one region to deploy and approximately 45-50 minutes for two regions to deploy. You will have enough to start executing the lab exercises for the two region in 25-30 minutes. You can watch the state machine as it executes by clicking the icon to expand the visual workflow to the full screen. 1. One Region: 1. Two Regions: 15. You can also watch the CloudFormation stacks as they are created. If you are in a workshop, the instructor will have some background information to share while this is created. You can resume testing when the web tier has been deployed in the Ohio region. This will look something like this on the visual workflow. 16. You can start testing: 1. When the WaitForWebApp step for a single region deployment is completed, return to the CloudFormation console and select the \u201cWebServersforResiliencyTesting\u201d stack and then the Outputs tab at bottom. 1. When the WaitForWebApp1 step for a two region deployment is completed, return to the CloudFormation console and select the \u201cWebServersforResiliencyTesting\u201d stack and then the Outputs tab at bottom. 17. Click the value and it will bring up the website:","title":"1.1 Checking for Existing Service-Linked Roles"},{"location":"Reliability/300 - Testing for Resiliency of EC2, RDS, and S3/Lab Guide/#2-discussion-and-example-failure-scenarios","text":"There is a choice of environments to execute the failure simulations in. Linux command line (bash), Python, Java, and C#. The instructions for each environment are in separate sections.","title":"2. Discussion and Example Failure Scenarios "},{"location":"Reliability/300 - Testing for Resiliency of EC2, RDS, and S3/Lab Guide/#21-setting-up-the-bash-environment","text":"All the command line scripts use a utility called jq. You can download it from the site and leave it in your local directory, as long as that is in your execution path: https://stedolan.github.io/jq/ 1. You can find out what your execution path is with the following command. $ echo $PATH /usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/aws/bin:/home/ec2-user/.local/bin:/home/ec2-user/bin If you have sudo rights, then copy the executable to /usr/local/bin/jq and make it executable. $ sudo cp jq-linux64 /usr/local/bin/jq $ sudo chmod 755 /usr/local/bin/jq If you do not have sudo rights, then copy it into your home directory under a /bin directory. In Amazon linux, this is typically /home/ec2-user/bin. $ cp jq-linux64 ~/bin/jq $ chmod 755 ~/bin/jq Install the AWS Command Line Interface (CLI) if you do not have it installed (it is installed by default on Amazon Linux). https://aws.amazon.com/cli/ Run the aws configure command to configure your command line options. This will prompt you for the AWS Access Key ID, AWS Secret Access Key, and default region name. Enter the key information if you do not already have them installed, and set the default region to \u201cus-east-2\u201d and leave the default output format as \u201cNone.\u201d. $ aws configure AWS Access Key ID [*************xxxx]: Your AWS Access Key ID AWS Secret Access Key [**************xxxx]: Your AWS Secret Access Key Default region name: [us-east-2]: us-east-2 Default output format [None]: return Download the zip file of the resiliency bash scripts at the following URL: https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/bashresiliency.zip Unzip the folder in a location convenient for you to execute the scripts. They are also available in the Code/FailureSimulations/bash/ directory.","title":"2.1 Setting Up the bash Environment"},{"location":"Reliability/300 - Testing for Resiliency of EC2, RDS, and S3/Lab Guide/#22-setting-up-a-programming-language-based-environment","text":"You will need the same files that the AWS command line uses for credentials. You can either install the command line and use the \u2018aws configure\u2019 command as outlined in the bash set up, or you can manually create the configuration files. To create the files manually, create a .aws folder/directory in your home directory. 1. Bash and powershell use the same command. mkdir ~/.aws Change directory to that directory to create the configuration file. Bash cd ~/.aws Powershell cd ~\\.aws Use a text editor (vim, emacs, notepad, wordpad) to create a text file (no extension named \u201ccredentials.\u201d In this file you should have the following text. [default] aws_access_key_id = Your access key aws_secret_access_key = Your secret key region = us-east-2","title":"2.2 Setting up a Programming Language Based Environment"},{"location":"Reliability/300 - Testing for Resiliency of EC2, RDS, and S3/Lab Guide/#23-setting-up-the-python-environment","text":"The scripts are written in python with boto3. On Amazon Linux, this is already installed. Use your local operating system instructions to install boto3: https://github.com/boto/boto3 Download the zip file of the resiliency scripts at the following URL. https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/pythonresiliency.zip Unzip the folder in a location convenient for you to execute the scripts.","title":"2.3 Setting Up the Python Environment"},{"location":"Reliability/300 - Testing for Resiliency of EC2, RDS, and S3/Lab Guide/#24-setting-up-the-java-environment","text":"The command line utility in Java requires Java 8 SE. In Amazon Linux, you need to install Java 8 and remove Java 7. $ sudo yum install java-1.8.0-openjdk $ sudo yum remove java-1.7.0-openjdk Download the zipfile of the executables at the following URL https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/javaresiliency.zip . Unzip the folder in a location convenient for you to execute the command line programs.","title":"2.4 Setting Up the Java Environment"},{"location":"Reliability/300 - Testing for Resiliency of EC2, RDS, and S3/Lab Guide/#25-setting-up-the-c-environment","text":"Download the zipfile of the executables at the following URL. https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/csharpresiliency.zip Unzip the folder in a location convenient for you to execute the command line programs.","title":"2.5 Setting Up the C# Environment"},{"location":"Reliability/300 - Testing for Resiliency of EC2, RDS, and S3/Lab Guide/#26-setting-up-the-powershell-environment","text":"If you do not have the AWS Tools for Powershell, download and install them following the instructions here. https://aws.amazon.com/powershell/ Follow the \u201cGetting Started\u201d instructions for configuring credentials. https://docs.aws.amazon.com/powershell/latest/userguide/pstools-getting-started.html Download the zipfile of the scripts at the following URL. https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/powershellresiliency.zip Unzip the folder in a location convenient for you to execute the scripts.","title":"2.6 Setting up the Powershell Environment"},{"location":"Reliability/300 - Testing for Resiliency of EC2, RDS, and S3/Lab Guide/#3-failure-modes","text":"","title":"3. Failure modes "},{"location":"Reliability/300 - Testing for Resiliency of EC2, RDS, and S3/Lab Guide/#31-ec2-failure-mode","text":"The first failure mode will be to fail a web server. To prepare for this, you should have two consoles open: VPC and EC2. From the AWS Console, click the downward facing icon to the right of the word \u201cServices.\u201d This will bring up the list of services. Type \u201cEC2\u201d in the search box and press the enter key. You also need the VPC Console. From the AWS Console, click the downward facing icon to the right of the word \u201cServices.\u201d This will bring up the list of services. Type \u201cVPC\u201d in the search box, then right click on the \u201cVPC Isolate Cloud Resources\u201d text and open the link in a new tab or window. You can then click the upward facing icon to the right of the word \u201cServices\u201d to make the menu of services disappear. On the EC2 Console, click \u201cInstances\u201d on the left side to bring up the list of instances. Use this as the command line argument to the scripts/programs below. Instance Failure in bash Execute the failure mode script for failing an instance: $./fail_instance.sh vpc-id Instance Failure in Python: Execute the failure mode script for failing an instance: $ python fail_instance.py vpc-id Instance Failure in Java: Execute the failure mode program for failing an instance: $ java -jar app-resiliency-1.0.jar EC2 vpc-id Instance Failure in C#: Execute the failure mode program for failing an instance: $ .\\AppResiliency EC2 vpc-id Instance Failure in Powershell: Execute the failure mode script for failing an instance: $ .\\fail_instance.ps1 vpc-id Watch the behavior of the Load Balancer Target Group and its Targets in the EC2 Console. See it get marked unhealthy and replaced by the Auto Scaling Group.","title":"3.1 EC2 Failure Mode"},{"location":"Reliability/300 - Testing for Resiliency of EC2, RDS, and S3/Lab Guide/#32-rds-failure-mode","text":"From the AWS EC2 Console (you will still need the VPC ID from the VPC Console), click the downward facing icon to the right of the word \u201cServices.\u201d This will bring up the list of services. Type \u201cRDS\u201d in the search box and press the enter key. Use this as the command line argument to the scripts/programs below. RDS Instance Failure in bash: Execute the failure mode script for failing over an RDS instance: $./failover_rds.sh vpc-id RDS Instance Failure in Python: Execute the failure mode script for failing an instance: $ python fail_rds.py vpc-id RDS Instance Failure in Java: Execute the failure mode program for failing an instance: $ java -jar app-resiliency-1.0.jar RDS vpc-id RDS Instance Failure in C#: Execute the failure mode program for failing an instance: $ .\\AppResiliency RDS vpc-id RDS Instance Failure in Powershell: Execute the failure mode script for failing an instance: $ .\\failover_rds.ps1 vpc-id Watch the behavior of the website. What happens? Watch the EC2 Load Balancer Target Group and its Targets\u2019 health. Do the instances recover?","title":"3.2 RDS Failure Mode"},{"location":"Reliability/300 - Testing for Resiliency of EC2, RDS, and S3/Lab Guide/#33-az-failure","text":"Availability zone failure. AZ Failure in bash: Execute the following script for failing over an Availability Zone: $./fail_az.sh az vpc-id AZ Failure in Python: Execute the failure mode script for failing an instance: $ python fail_az.py vpc-id az AZ Failure in Java: Execute the failure mode script for failing an instance: $ java -jar app-resiliency-1.0.jar AZ vpc-id az AZ Failure in C#: Execute the failure mode script for failing an instance: $ .\\AppResiliency AZ vpc-id az AZ Failure in Powershell: Execute the failure mode script for failing an instance: $ .\\fail_az.ps1 az vpc-id What is the expected effect? How long does it take to take effect? Look at the Target Group Targets to see them go unhealthy, also watch the EC2 instances to see the one in the target AZ shutdown and be restarted in one of the other AZs. What would you do if the ASG was only in one AZ? You could call the AutoScaling SuspendProcesses and then get the list of instances in the group and call EC2 StopInstances or TerminateInstances How would you undo all these changes?","title":"3.3 AZ Failure"},{"location":"Reliability/300 - Testing for Resiliency of EC2, RDS, and S3/Lab Guide/#34-failure-of-s3","text":"Failure of S3 means that the image will not be available. Failure in bash: The bash commands available do not allow for modification of the access permissions, so you'll have to do the work in the console. Navigate to the S3 console: [https://s3.console.aws.amazon.com/s3/home](https://s3.console.aws.amazon.com/s3/home] Select the bucket where the image is located. In my example, this is the bucket \"arc327-well-architected-for-reliability\" Select the object, then select the \"Permissions\" tab: Select the \"Public Access\" radio button, and deselect the \"Read object\" box: What is the expected effect? How long does it take to take effect? How would you diagnose if this is a larger problem than permissions?","title":"3.4 Failure of S3"},{"location":"Reliability/300 - Testing for Resiliency of EC2, RDS, and S3/Lab Guide/#35-looking-for-more-to-do","text":"You can use drift detection in the CloudFormation console to see what had chanegd, or work on code to heal their failure modes. 1. Remove the network ACLs they added 2. Reconfigure the AutoScaling Groups to use the AZ","title":"3.5 Looking for more to do?"},{"location":"Reliability/300 - Testing for Resiliency of EC2, RDS, and S3/Lab Guide/#4-tear-down-this-lab","text":"In order to take down the lab environment, you will need to remove the association of the Network ACL that was added to fail the AZ, and delete the Network ACL in the console. You can then delete the CloudFormation stacks (in both AWS regions) that were created. Here are the instructions for how to do that. 1. Navigate to the VPC Console. From the AWS Console, click the downward facing icon to the right of the word \u201cServices.\u201d This will bring up the list of services. Type \u201cVPC\u201d in the search box, then press enter. On the VPC Console, click on Network ACLs and select the Network ACL that is associated with 4 subnets in the ResiliencyVPC. Click the \u201cSubnet Associations\u201d tab and then Click the \u201cEdit button. You need to associate the 2 subnets that were changed to the blocking Network ACL back to this default Network ACL. Select the 2 subnets available, then click the \u201cSave\u201d button. Now select the Network ACL that is associated with 0 Subnets and click the \u201cDelete\u201d button to delete this Network ACL. If you have deployed into 2 regions, you have to delete the Database Migration Service and RDS Read Replicas before you can delete the RDS instances. You can delete the DMS (in Oregon) and read replica at the same time, but the console only allows you to select one of the stacks at a time. Select the \u201cDMSforResiliencyTesting\u201d stack, then click the \u201cActions\u201d button, and click \u201cDeleteStack:\u201d. You can then select the \u201c MySQLReadReplicaResiliencyTesting\u201d stack, click the \u201cActions\u201d button, and click \u201cDelete Stack\u201d to simultaneously delete the RDS Read Replica instance. If you have deployed in 2 regions, Navigate to the other region (Ohio, if you started in Oregon, and Oregon, if you started in Ohio), and perform the same deletion steps above for the RDS read replica and DMS, if in Oregon. Wait for the read replicas to be deleted. You can delete the web servers and RDS at the same time, but the console only allows you to select one of the stacks at a time. Select the \u201cWebServersforResiliencyTesting\u201d stack, then click the \u201cActions\u201d button, and click \u201cDelete Stack:\u201d. You can then select the \u201c MySQLforResiliencyTesting\u201d stack, click the \u201cActions\u201d button, and click \u201cDelete Stack\u201d to simultaneously delete the RDS instance. You can then select the \u201cDeployResiliencyWorkshop\u201d stack, click the \u201cActions\u201d button, and click \u201cDelete Stack\u201d to simultaneously delete the deployment machine and Lambda functions. Once all these stacks have deleted, you can then select the \u201cResiliencyVPC\u201d stack, click the \u201cActions\u201d button, and click \u201cDelete Stack\u201d to delete the VPC and remove the last piece of infrastructure. When it is delete complete, you are done.","title":"4. Tear down this lab "},{"location":"Reliability/300 - Testing for Resiliency of EC2, RDS, and S3/Lab Guide/#references-useful-resources","text":"","title":"References &amp; useful resources:"},{"location":"Reliability/300 - Testing for Resiliency of EC2, RDS, and S3/Lab Guide/#license","text":"","title":"License"},{"location":"Reliability/300 - Testing for Resiliency of EC2, RDS, and S3/Lab Guide/#documentation-license","text":"Licensed under the Creative Commons Share Alike 4.0 license.","title":"Documentation License"},{"location":"Reliability/300 - Testing for Resiliency of EC2, RDS, and S3/Lab Guide/#code-license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Code License"},{"location":"Security/","text":"AWS Well-Architected Security Labs Introduction This repository contains documentation and code in the format of hands-on labs to help you learn, measure, and build using architectural best practices. The labs are categorized into levels, where 100 is introductory, 200/300 is intermediate and 400 is advanced. For more information about security on AWS visit AWS Security and read the AWS Well-Architected Security whitepaper . Labs: Level 100: AWS Account Root User Level 100: Basic Identity Access Management User, Group, Role Level 100: CloudFront with S3 Bucket Origin Level 200: Automated Deployment of Detective Controls Level 200: Automated Deployment of EC2 Web Application Level 200: Automated Deployment of IAM Groups and Roles Level 200: Automated Deployment of VPC Level 200: Basic EC2 with WAF Protection Level 200: CloudFront with WAF Protection Level 300: IAM Permission Boundaries Delegating Role Creation Level 300: IAM IAM Tag Based Access Control for EC2 License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"AWS Well-Architected Security Labs"},{"location":"Security/#aws-well-architected-security-labs","text":"","title":"AWS Well-Architected Security Labs"},{"location":"Security/#introduction","text":"This repository contains documentation and code in the format of hands-on labs to help you learn, measure, and build using architectural best practices. The labs are categorized into levels, where 100 is introductory, 200/300 is intermediate and 400 is advanced. For more information about security on AWS visit AWS Security and read the AWS Well-Architected Security whitepaper .","title":"Introduction"},{"location":"Security/#labs","text":"Level 100: AWS Account Root User Level 100: Basic Identity Access Management User, Group, Role Level 100: CloudFront with S3 Bucket Origin Level 200: Automated Deployment of Detective Controls Level 200: Automated Deployment of EC2 Web Application Level 200: Automated Deployment of IAM Groups and Roles Level 200: Automated Deployment of VPC Level 200: Basic EC2 with WAF Protection Level 200: CloudFront with WAF Protection Level 300: IAM Permission Boundaries Delegating Role Creation Level 300: IAM IAM Tag Based Access Control for EC2","title":"Labs:"},{"location":"Security/#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/100 - AWS Account & Root User/","text":"Level 100: AWS Account Root User Introduction This hands-on lab will guide you through the introductory steps to configure a new AWS account and secure the root user. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals: Protecting AWS credentials Fine-grained authorization Prerequisites: An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Overview Lab Guide.md the guide for this lab Checklist.md best practice checklist related to this lab /Code Code including CloudFormation templates related to this lab /Images referenced by this lab License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 100: AWS Account & Root User"},{"location":"Security/100 - AWS Account & Root User/#level-100-aws-account-root-user","text":"","title":"Level 100: AWS Account &amp; Root User"},{"location":"Security/100 - AWS Account & Root User/#introduction","text":"This hands-on lab will guide you through the introductory steps to configure a new AWS account and secure the root user. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/100 - AWS Account & Root User/#goals","text":"Protecting AWS credentials Fine-grained authorization","title":"Goals:"},{"location":"Security/100 - AWS Account & Root User/#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites:"},{"location":"Security/100 - AWS Account & Root User/#overview","text":"Lab Guide.md the guide for this lab Checklist.md best practice checklist related to this lab /Code Code including CloudFormation templates related to this lab /Images referenced by this lab","title":"Overview"},{"location":"Security/100 - AWS Account & Root User/#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/100 - AWS Account & Root User/Checklist/","text":"Level 100: AWS Account Root User: Best Practice Checklist [ ] Root user has no access keys [ ] Root user is protected by a strong password with MFA stored securely [ ] Enable security questions in account [ ] Account credential report is regularly reviewed preferably automated [ ] AWS account email address to a secured, limited distribution list [ ] AWS account phone number for account to trusted contact [ ] Organizations with control policies for multiple accounts with stack sets","title":"Level 100: AWS Account & Root User: Best Practice Checklist"},{"location":"Security/100 - AWS Account & Root User/Checklist/#level-100-aws-account-root-user-best-practice-checklist","text":"[ ] Root user has no access keys [ ] Root user is protected by a strong password with MFA stored securely [ ] Enable security questions in account [ ] Account credential report is regularly reviewed preferably automated [ ] AWS account email address to a secured, limited distribution list [ ] AWS account phone number for account to trusted contact [ ] Organizations with control policies for multiple accounts with stack sets","title":"Level 100: AWS Account &amp; Root User: Best Practice Checklist"},{"location":"Security/100 - AWS Account & Root User/Lab Guide/","text":"Level 100: AWS Account Root User: Lab Guide 1. Account Settings Root User Security When you first create an Amazon Web Services (AWS) account, you begin with a single sign-in identity that has complete access to all AWS services and resources in the account. This identity is called the AWS account root user and is accessed by signing in with the email address and password that you used to create the account. It is strongly recommend that you do not use the root user for your everyday tasks, even the administrative ones. Instead, adhere to the best practice of using the root user only to create your first IAM user, groups and roles. Then securely lock away the root user credentials and use them to perform only a few account and service management tasks. To view the tasks that require you to sign in as the root user, see AWS Tasks That Require Root User . 1.1 Generate and Review the AWS Account Credential Report Its good to get an idea of what you have configured already in your AWS account especially if you have had it for a while. You should audit your security configuration in the following situations: On a periodic basis. You should perform the steps described here at regular intervals as a best practice for security. If there are changes in your organization, such as people leaving. If you have stopped using one or more individual AWS services. This is important for removing permissions that users in your account no longer need. If you've added or removed software in your accounts, such as applications on Amazon EC2 instances, AWS OpsWorks stacks, AWS CloudFormation templates, etc. If you ever suspect that an unauthorized person might have accessed your account. As you review your account's security configuration, follow these guidelines: Be thorough . Look at all aspects of your security configuration, including those you might not use regularly. Don't assume . If you are unfamiliar with some aspect of your security configuration (for example, the reasoning behind a particular policy or the existence of a role), investigate the business need until you are satisfied. Keep things simple . To make auditing (and management) easier, use IAM groups, consistent naming schemes, and straightforward policies. More information can be found at https://docs.aws.amazon.com/general/latest/gr/aws-security-audit-guide.html You can use the AWS Management Console to download a credential report as a comma-separated values (CSV) file. Please note that credential report can take 4 hours to reflect changes. To download a credential report using the AWS Management Console: Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/ . In the navigation pane, click Credential report. Click Download Report. Further information about the report can be found at https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_getting-report.html 1.2 Enable a Virtual MFA Device for Your AWS Account Root User You can use IAM in the AWS Management Console to configure and enable a virtual MFA device for your root user. To manage MFA devices for the AWS account, you must be signed in to AWS using your root user credentials. You cannot manage MFA devices for the root user using other credentials. If your MFA device is lost, stolen, or not working, you can still sign in using alternative factors of authentication. To do this, you must verify your identity using the email and phone that are registered with your account. This means that if you can't sign in with your MFA device, you can sign in by verifying your identity using the email and phone that are registered with your account. Before you enable MFA for your root user, review your account settings and contact information to make sure that you have access to the email and phone number. To learn about signing in using alternative factors of authentication, see What If an MFA Device Is Lost or Stops Working ?. To disable this feature, contact AWS Support . Use your AWS account email address and password to sign in as the AWS account root user to the IAM console at https://console.aws.amazon.com/iam/ Do one of the following: Option 1 : Click Dashboard, and under Security Status, expand Activate MFA on your root user. Option 2 : On the right side of the navigation bar, click your account name, and click Security Credentials. If necessary, click Continue to Security Credentials. Then expand the Multi-Factor Authentication (MFA) section on the page. Click Manage MFA or Activate MFA, depending on which option you chose in the preceding step. In the wizard, click A virtual MFA device and then click Next Step. Confirm that a virtual MFA app is installed on the device, and then click Next Step. IAM generates and displays configuration information for the virtual MFA device, including a QR code graphic. The graphic is a representation of the secret configuration key that is available for manual entry on devices that do not support QR codes. With the Manage MFA Device wizard still open, open the virtual MFA app on the device. If the virtual MFA software supports multiple accounts (multiple virtual MFA devices), then click the option to create a new account (a new virtual device). The easiest way to configure the app is to use the app to scan the QR code. If you cannot scan the code, you can type the configuration information manually. To use the QR code to configure the virtual MFA device, follow the app instructions for scanning the code. For example, you might need to tap the camera icon or tap a command like Scan account barcode, and then use the device's camera to scan the QR code. If you cannot scan the code, type the configuration information manually by typing the Secret Configuration Key value into the app. For example, to do this in the AWS Virtual MFA app, click Manually add account, and then type the secret configuration key and click Create. Important Make a secure backup of the QR code or secret configuration key, or make sure that you enable multiple virtual MFA devices for your account. A virtual MFA device might become unavailable, for example, if you lose the smartphone where the virtual MFA device is hosted). If that happens, you will not be able to sign in to your account and you will have to contact customer service to remove MFA protection for the account. Note The QR code and secret configuration key generated by IAM are tied to your AWS account and cannot be used with a different account. They can, however, be reused to configure a new MFA device for your account in case you lose access to the original MFA device. The device starts generating six-digit numbers. In the Manage MFA Device wizard, in the Authentication Code 1 box, type the six-digit number that's currently displayed by the MFA device. Wait up to 30 seconds for the device to generate a new number, and then type the new six-digit number into the Authentication Code 2 box. Important Submit your request immediately after generating the codes. If you generate the codes and then wait too long to submit the request, the MFA device successfully associates with the user but the MFA device is out of sync. This happens because time-based one-time passwords (TOTP) expire after a short period of time. If this happens, you can resync the device. Click Next Step, and then click Finish. The device is ready for use with AWS. For information about using MFA with the AWS Management Console, see Using MFA Devices With Your IAM Sign-in Page . 1.3 Configure Account Security Challenge Questions Configure account security challenge questions because they are used to verify that you own an AWS account. Use your AWS account email address and password to sign in as the AWS account root user and open the AWS account settings page at https://console.aws.amazon.com/billing/home?#/account/ . Navigate to security challenge questions configuration section. Select three challenge questions and enter answers for each. Securely store the questions and answers as you would passwords or other credentials. Click update. 1.4 Configure Account Alternate Contacts Alternate contacts enable AWS to contact another person about issues with the account, even if you are unavailable. Use your AWS account email address and password to sign in as the AWS account root user and open the AWS account settings page at https://console.aws.amazon.com/billing/home?#/account/ . Navigate to alternate contacts configuration section. Enter contact details for billing, operations and security. Click update. 1.5 Remove Your AWS Account Root User Access Keys You use an access key (an access key ID and secret access key) to make programmatic requests to AWS. However, do not use your AWS account root user access key. The access key for your AWS account gives full access to all your resources for all AWS services, including your billing information. You cannot restrict the permissions associated with your AWS account access key. Check in the credential report; if you don't already have an access key for your AWS account, don't create one unless you absolutely need to. Instead, use your account email address and password to sign in to the AWS Management Console and create an IAM user for yourself that has administrative privileges. This will be explained in a later section. If you do have an access key for your AWS account, delete it unless you have a specific requirement. To delete or rotate your AWS account access keys, go to the Security Credentials page in the AWS Management Console and sign in with your account's email address and password. You can manage your access keys in the Access keys section. Never share your AWS account password or access keys with anyone. 1.6 Periodically Change the AWS Account Root User Password You must be signed in as the AWS account root user in order to change the password. To learn how to reset a forgotten root user password, see Resetting Your Lost or Forgotten Passwords or Access Keys . To change the password for the root user: Use your AWS account email address and password to sign in to the AWS Management Console as the root user. Note If you previously signed in to the console with IAM user credentials, your browser might remember this preference and open your account-specific sign-in page. You cannot use the IAM user sign-in page to sign in with your AWS account root user credentials. If you see the IAM user sign-in page, click Sign-in using root account credentials near the bottom of the page to return to the main sign-in page. From there, you can type your AWS account email address and password. In the upper right corner of the console, click your account name or number and then click My Account. On the right side of the page, next to the Account Settings section, click Edit. On the Password line choose Click here to change your password. Choose a strong password. Although you can set an account password policy for IAM users, that policy does not apply to your AWS account root user. AWS requires that your password meet these conditions: have a minimum of 8 characters and a maximum of 128 characters include a minimum of three of the following mix of character types: uppercase, lowercase, numbers, and ! @ # $ % ^ * () [] {} | _ + - = symbols not be identical to your AWS account name or email address Note AWS is rolling out improvements to the sign-in process. One of those improvements is to enforce a more secure password policy for your account. If your account has been upgraded, you are required to meet the password policy above. If your account has not yet been upgraded, then AWS does not enforce this policy, but highly recommends that you follow its guidelines for a more secure password. To protect your password, it's important to follow these best practices: Change your password periodically and keep your password private, since anyone who knows your password can access your account. Use a different password on AWS than you use on other sites. Avoid passwords that are easy to guess. These include passwords such as secret, password, amazon, or 123456. They also include things like a dictionary word, your name, email address, or other personal information that can easily be obtained. 1.7 Configure a Strong Password Policy for Your Users You can set a password policy on your AWS account to specify complexity requirements and mandatory rotation periods for your IAM users' passwords. The IAM password policy does not apply to the AWS root account password. To create or change a password policy: Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/. In the navigation pane, click Account Settings. In the Password Policy section, select the options you want to apply to your password policy. Click Apply Password Policy. 2. Tear down this lab Please note that the changes you made to the account and root user have no charges associated with them. References useful resources: AWS Tasks That Require Root User https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_getting-report.html AWS Identity and Access Management User Guide IAM Best Practices and Use Cases Resetting Your Lost or Forgotten Passwords or Access Keys Using MFA Devices With Your IAM Sign-in Page What If an MFA Device Is Lost or Stops Working License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 100: AWS Account & Root User: Lab Guide"},{"location":"Security/100 - AWS Account & Root User/Lab Guide/#level-100-aws-account-root-user-lab-guide","text":"","title":"Level 100: AWS Account &amp; Root User: Lab Guide"},{"location":"Security/100 - AWS Account & Root User/Lab Guide/#1-account-settings-root-user-security","text":"When you first create an Amazon Web Services (AWS) account, you begin with a single sign-in identity that has complete access to all AWS services and resources in the account. This identity is called the AWS account root user and is accessed by signing in with the email address and password that you used to create the account. It is strongly recommend that you do not use the root user for your everyday tasks, even the administrative ones. Instead, adhere to the best practice of using the root user only to create your first IAM user, groups and roles. Then securely lock away the root user credentials and use them to perform only a few account and service management tasks. To view the tasks that require you to sign in as the root user, see AWS Tasks That Require Root User .","title":"1. Account Settings &amp; Root User Security"},{"location":"Security/100 - AWS Account & Root User/Lab Guide/#11-generate-and-review-the-aws-account-credential-report","text":"Its good to get an idea of what you have configured already in your AWS account especially if you have had it for a while. You should audit your security configuration in the following situations: On a periodic basis. You should perform the steps described here at regular intervals as a best practice for security. If there are changes in your organization, such as people leaving. If you have stopped using one or more individual AWS services. This is important for removing permissions that users in your account no longer need. If you've added or removed software in your accounts, such as applications on Amazon EC2 instances, AWS OpsWorks stacks, AWS CloudFormation templates, etc. If you ever suspect that an unauthorized person might have accessed your account. As you review your account's security configuration, follow these guidelines: Be thorough . Look at all aspects of your security configuration, including those you might not use regularly. Don't assume . If you are unfamiliar with some aspect of your security configuration (for example, the reasoning behind a particular policy or the existence of a role), investigate the business need until you are satisfied. Keep things simple . To make auditing (and management) easier, use IAM groups, consistent naming schemes, and straightforward policies. More information can be found at https://docs.aws.amazon.com/general/latest/gr/aws-security-audit-guide.html You can use the AWS Management Console to download a credential report as a comma-separated values (CSV) file. Please note that credential report can take 4 hours to reflect changes. To download a credential report using the AWS Management Console: Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/ . In the navigation pane, click Credential report. Click Download Report. Further information about the report can be found at https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_getting-report.html","title":"1.1 Generate and Review the AWS Account Credential Report"},{"location":"Security/100 - AWS Account & Root User/Lab Guide/#12-enable-a-virtual-mfa-device-for-your-aws-account-root-user","text":"You can use IAM in the AWS Management Console to configure and enable a virtual MFA device for your root user. To manage MFA devices for the AWS account, you must be signed in to AWS using your root user credentials. You cannot manage MFA devices for the root user using other credentials. If your MFA device is lost, stolen, or not working, you can still sign in using alternative factors of authentication. To do this, you must verify your identity using the email and phone that are registered with your account. This means that if you can't sign in with your MFA device, you can sign in by verifying your identity using the email and phone that are registered with your account. Before you enable MFA for your root user, review your account settings and contact information to make sure that you have access to the email and phone number. To learn about signing in using alternative factors of authentication, see What If an MFA Device Is Lost or Stops Working ?. To disable this feature, contact AWS Support . Use your AWS account email address and password to sign in as the AWS account root user to the IAM console at https://console.aws.amazon.com/iam/ Do one of the following: Option 1 : Click Dashboard, and under Security Status, expand Activate MFA on your root user. Option 2 : On the right side of the navigation bar, click your account name, and click Security Credentials. If necessary, click Continue to Security Credentials. Then expand the Multi-Factor Authentication (MFA) section on the page. Click Manage MFA or Activate MFA, depending on which option you chose in the preceding step. In the wizard, click A virtual MFA device and then click Next Step. Confirm that a virtual MFA app is installed on the device, and then click Next Step. IAM generates and displays configuration information for the virtual MFA device, including a QR code graphic. The graphic is a representation of the secret configuration key that is available for manual entry on devices that do not support QR codes. With the Manage MFA Device wizard still open, open the virtual MFA app on the device. If the virtual MFA software supports multiple accounts (multiple virtual MFA devices), then click the option to create a new account (a new virtual device). The easiest way to configure the app is to use the app to scan the QR code. If you cannot scan the code, you can type the configuration information manually. To use the QR code to configure the virtual MFA device, follow the app instructions for scanning the code. For example, you might need to tap the camera icon or tap a command like Scan account barcode, and then use the device's camera to scan the QR code. If you cannot scan the code, type the configuration information manually by typing the Secret Configuration Key value into the app. For example, to do this in the AWS Virtual MFA app, click Manually add account, and then type the secret configuration key and click Create. Important Make a secure backup of the QR code or secret configuration key, or make sure that you enable multiple virtual MFA devices for your account. A virtual MFA device might become unavailable, for example, if you lose the smartphone where the virtual MFA device is hosted). If that happens, you will not be able to sign in to your account and you will have to contact customer service to remove MFA protection for the account. Note The QR code and secret configuration key generated by IAM are tied to your AWS account and cannot be used with a different account. They can, however, be reused to configure a new MFA device for your account in case you lose access to the original MFA device. The device starts generating six-digit numbers. In the Manage MFA Device wizard, in the Authentication Code 1 box, type the six-digit number that's currently displayed by the MFA device. Wait up to 30 seconds for the device to generate a new number, and then type the new six-digit number into the Authentication Code 2 box. Important Submit your request immediately after generating the codes. If you generate the codes and then wait too long to submit the request, the MFA device successfully associates with the user but the MFA device is out of sync. This happens because time-based one-time passwords (TOTP) expire after a short period of time. If this happens, you can resync the device. Click Next Step, and then click Finish. The device is ready for use with AWS. For information about using MFA with the AWS Management Console, see Using MFA Devices With Your IAM Sign-in Page .","title":"1.2 Enable a Virtual MFA Device for Your AWS Account Root User"},{"location":"Security/100 - AWS Account & Root User/Lab Guide/#13-configure-account-security-challenge-questions","text":"Configure account security challenge questions because they are used to verify that you own an AWS account. Use your AWS account email address and password to sign in as the AWS account root user and open the AWS account settings page at https://console.aws.amazon.com/billing/home?#/account/ . Navigate to security challenge questions configuration section. Select three challenge questions and enter answers for each. Securely store the questions and answers as you would passwords or other credentials. Click update.","title":"1.3 Configure Account Security Challenge Questions"},{"location":"Security/100 - AWS Account & Root User/Lab Guide/#14-configure-account-alternate-contacts","text":"Alternate contacts enable AWS to contact another person about issues with the account, even if you are unavailable. Use your AWS account email address and password to sign in as the AWS account root user and open the AWS account settings page at https://console.aws.amazon.com/billing/home?#/account/ . Navigate to alternate contacts configuration section. Enter contact details for billing, operations and security. Click update.","title":"1.4 Configure Account Alternate Contacts"},{"location":"Security/100 - AWS Account & Root User/Lab Guide/#15-remove-your-aws-account-root-user-access-keys","text":"You use an access key (an access key ID and secret access key) to make programmatic requests to AWS. However, do not use your AWS account root user access key. The access key for your AWS account gives full access to all your resources for all AWS services, including your billing information. You cannot restrict the permissions associated with your AWS account access key. Check in the credential report; if you don't already have an access key for your AWS account, don't create one unless you absolutely need to. Instead, use your account email address and password to sign in to the AWS Management Console and create an IAM user for yourself that has administrative privileges. This will be explained in a later section. If you do have an access key for your AWS account, delete it unless you have a specific requirement. To delete or rotate your AWS account access keys, go to the Security Credentials page in the AWS Management Console and sign in with your account's email address and password. You can manage your access keys in the Access keys section. Never share your AWS account password or access keys with anyone.","title":"1.5 Remove Your AWS Account Root User Access Keys"},{"location":"Security/100 - AWS Account & Root User/Lab Guide/#16-periodically-change-the-aws-account-root-user-password","text":"You must be signed in as the AWS account root user in order to change the password. To learn how to reset a forgotten root user password, see Resetting Your Lost or Forgotten Passwords or Access Keys . To change the password for the root user: Use your AWS account email address and password to sign in to the AWS Management Console as the root user. Note If you previously signed in to the console with IAM user credentials, your browser might remember this preference and open your account-specific sign-in page. You cannot use the IAM user sign-in page to sign in with your AWS account root user credentials. If you see the IAM user sign-in page, click Sign-in using root account credentials near the bottom of the page to return to the main sign-in page. From there, you can type your AWS account email address and password. In the upper right corner of the console, click your account name or number and then click My Account. On the right side of the page, next to the Account Settings section, click Edit. On the Password line choose Click here to change your password. Choose a strong password. Although you can set an account password policy for IAM users, that policy does not apply to your AWS account root user. AWS requires that your password meet these conditions: have a minimum of 8 characters and a maximum of 128 characters include a minimum of three of the following mix of character types: uppercase, lowercase, numbers, and ! @ # $ % ^ * () [] {} | _ + - = symbols not be identical to your AWS account name or email address Note AWS is rolling out improvements to the sign-in process. One of those improvements is to enforce a more secure password policy for your account. If your account has been upgraded, you are required to meet the password policy above. If your account has not yet been upgraded, then AWS does not enforce this policy, but highly recommends that you follow its guidelines for a more secure password. To protect your password, it's important to follow these best practices: Change your password periodically and keep your password private, since anyone who knows your password can access your account. Use a different password on AWS than you use on other sites. Avoid passwords that are easy to guess. These include passwords such as secret, password, amazon, or 123456. They also include things like a dictionary word, your name, email address, or other personal information that can easily be obtained.","title":"1.6 Periodically Change the AWS Account Root User Password"},{"location":"Security/100 - AWS Account & Root User/Lab Guide/#17-configure-a-strong-password-policy-for-your-users","text":"You can set a password policy on your AWS account to specify complexity requirements and mandatory rotation periods for your IAM users' passwords. The IAM password policy does not apply to the AWS root account password. To create or change a password policy: Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/. In the navigation pane, click Account Settings. In the Password Policy section, select the options you want to apply to your password policy. Click Apply Password Policy.","title":"1.7 Configure a Strong Password Policy for Your Users"},{"location":"Security/100 - AWS Account & Root User/Lab Guide/#2-tear-down-this-lab","text":"Please note that the changes you made to the account and root user have no charges associated with them.","title":"2. Tear down this lab"},{"location":"Security/100 - AWS Account & Root User/Lab Guide/#references-useful-resources","text":"AWS Tasks That Require Root User https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_getting-report.html AWS Identity and Access Management User Guide IAM Best Practices and Use Cases Resetting Your Lost or Forgotten Passwords or Access Keys Using MFA Devices With Your IAM Sign-in Page What If an MFA Device Is Lost or Stops Working","title":"References &amp; useful resources:"},{"location":"Security/100 - AWS Account & Root User/Lab Guide/#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/100 - Basic Identity & Access Management User, Group, Role/","text":"Level 100: Basic Identity Access Management User, Group, Role Introduction This hands-on lab will guide you through the introductory steps to configure AWS Identity and Access Management (IAM). You will use the AWS Management Console to guide you through how to configure your first IAM user, group and role for administrative access. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals: Protecting AWS credentials Fine-grained authorization Prerequisites: An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Overview Lab Guide.md the guide for this lab Checklist.md best practice checklist related to this lab /Code Code including CloudFormation templates related to this lab /Images referenced by this lab License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 100: Basic Identity & Access Management User, Group, Role"},{"location":"Security/100 - Basic Identity & Access Management User, Group, Role/#level-100-basic-identity-access-management-user-group-role","text":"","title":"Level 100: Basic Identity &amp; Access Management User, Group, Role"},{"location":"Security/100 - Basic Identity & Access Management User, Group, Role/#introduction","text":"This hands-on lab will guide you through the introductory steps to configure AWS Identity and Access Management (IAM). You will use the AWS Management Console to guide you through how to configure your first IAM user, group and role for administrative access. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/100 - Basic Identity & Access Management User, Group, Role/#goals","text":"Protecting AWS credentials Fine-grained authorization","title":"Goals:"},{"location":"Security/100 - Basic Identity & Access Management User, Group, Role/#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites:"},{"location":"Security/100 - Basic Identity & Access Management User, Group, Role/#overview","text":"Lab Guide.md the guide for this lab Checklist.md best practice checklist related to this lab /Code Code including CloudFormation templates related to this lab /Images referenced by this lab","title":"Overview"},{"location":"Security/100 - Basic Identity & Access Management User, Group, Role/#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/100 - Basic Identity & Access Management User, Group, Role/Checklist/","text":"Level 100: Basic Identity Access Management User, Group, Role: Best Practice Checklist [ ] Account credential report is regularly reviewed preferably automated [ ] IAM users\u2019 credentials are rotated on a regular basis [ ] IAM user credential lifecycle is integrated with HR process upon suspension or termination of staff [ ] IAM trust policies in roles are reviewed regularly against business needs [ ] Permission are limited business needs (least privilege) and evaluated periodically [ ] All credentials unused within X days are de-activated, preferably automated [ ] Access keys are used only where absolutely necessary and rotated every X days [ ] No IAM credentials are embedded in source code/scripts or stored insecurely [ ] A unique IAM Role is used for each function/application [ ] Non-EC2-based applications use IAM federation and Roles to access AWS services [ ] Mobile applications use Cognito","title":"Level 100: Basic Identity & Access Management User, Group, Role: Best Practice Checklist"},{"location":"Security/100 - Basic Identity & Access Management User, Group, Role/Checklist/#level-100-basic-identity-access-management-user-group-role-best-practice-checklist","text":"[ ] Account credential report is regularly reviewed preferably automated [ ] IAM users\u2019 credentials are rotated on a regular basis [ ] IAM user credential lifecycle is integrated with HR process upon suspension or termination of staff [ ] IAM trust policies in roles are reviewed regularly against business needs [ ] Permission are limited business needs (least privilege) and evaluated periodically [ ] All credentials unused within X days are de-activated, preferably automated [ ] Access keys are used only where absolutely necessary and rotated every X days [ ] No IAM credentials are embedded in source code/scripts or stored insecurely [ ] A unique IAM Role is used for each function/application [ ] Non-EC2-based applications use IAM federation and Roles to access AWS services [ ] Mobile applications use Cognito","title":"Level 100: Basic Identity &amp; Access Management User, Group, Role: Best Practice Checklist"},{"location":"Security/100 - Basic Identity & Access Management User, Group, Role/Lab Guide/","text":"Level 100: Basic Identity Access Management User, Group, Role: Lab Guide 1. AWS Identity Access Management As a best practice, do not use the AWS account root user for any task where it's not required. Instead, create a new IAM user for each person that requires administrator access. Then make those users administrators (only if they absolutely need full access to everything) by placing the users into an \"Administrators\" group to which you attach the AdministratorAccess managed policy. 1.1 Create Administrator IAM User and Group To create an administrator user for yourself and add the user to an administrators group: Use your AWS account email address and password to sign in as the AWS account root user to the IAM console at https://console.aws.amazon.com/iam/ . In the navigation pane, click Users and then click Add user. For User name, type a user name for yourself, such as Bob. Each user should have their own user name, do not share credentials. The name can consist of letters, digits, and the following characters: plus (+) , equal (=) , comma (,) , period (.) , at (@) , underscore (_) , and hyphen (-) . The name is not case sensitive and can be a maximum of 64 characters in length. Select the check box next to AWS Management Console access, select Custom password, and then type your new password in the text box. This user will be able to do almost anything in your account, by not giving it programmatic access (access secret key) you reduce your risk, and we will configure lower-privileged users and roles later. If you're creating the user for someone other than yourself, you can optionally select Require password reset to force the user to create a new password when first signing in. Click Next: Permissions. On the Set permissions for user page, click Add user to group. Click Create group. In the Create group dialog box, type the name for the new group such as Administrators. The name can consist of letters, digits, and the following characters: plus (+), equal (=), comma (,), period (.), at (@), underscore (_), and hyphen (-). The name is not case sensitive and can be a maximum of 128 characters in length. In the policy list, select the check box next to AdministratorAccess. Then click Create group. Back in the list of groups, verify the check box is next to your new group. Click Refresh if necessary to see the group in the list. Click Next: Review to see the list of group memberships to be added to the new user. When you are ready to proceed, click Create user. You can use this same process to create more groups and users and to give your users access to your AWS account resources. To learn about using policies that restrict user permissions to specific AWS resources, see Access Management and Example Policies . To add users to the group after it's created, see Adding and Removing Users in an IAM Group . Configure MFA on your new administrator user by choosing Users from the navigation pane. In the User Name list, click the name of the intended MFA user. Click the Security credentials tab. Next to Assigned MFA device, click the edit icon. You can now use this administrator user instead of your root user for this AWS account. It is a best practice to use least a least privileged approach to granting permissions, not everyone needs full administrator access! 1.2 Create Administrator IAM Role To create an administrator role for yourself (and other administrators) to be used with the administrator user and group you just created: 1. Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/ . 2. In the navigation pane, click Roles and then click Create role. 3. Click Another AWS account, then enter your account ID and tick Require MFA, then click Next: Permissions 4. Tick AdministratorAccess from the list, and then click Next: Review. 5. Enter a role name, e.g. 'Administrators' then click Create role. 6. Check the role you have configured by clicking the role you have just created. Record both the Role ARN and the link to the console. You can also optionally change the session duration timeout. 6. The role is now created, with full administrative access and MFA enforced. 2. Assume Administrator Role from an IAM user We will assume the role using the IAM user that we previously created in the web console. As the IAM user has full access it is a best practice not to have access keys to assume the role on the CLI, instead we should use a restricted IAM user for this so we can enforce the requirement of MFA. 2.1 Use Administrator Role in Web Console A role specifies a set of permissions that you can use to access AWS resources that you need. In that sense, it is similar to a user in AWS Identity and Access Management (IAM). A benefit of roles is they allow you to enforce the use of an MFA token to help protect your credentials. When you sign in as a user, you get a specific set of permissions. However, you don't sign in to a role, but once signed in (as a user) you can switch to a role. This temporarily sets aside your original user permissions and instead gives you the permissions assigned to the role. The role can be in your own account or any other AWS account. By default, your AWS Management Console session lasts for one hour. Important The permissions of your IAM user and any roles that you switch to are not cumulative. Only one set of permissions is active at a time. When you switch to a role, you temporarily give up your user permissions and work with the permissions that are assigned to the role. When you exit the role, your user permissions are automatically restored. Sign in to the AWS Management Console as an IAM user https://console.aws.amazon.com . In the console, click your user name on the navigation bar in the upper right. It typically looks like this: username@account_ID_number_or_alias . Alternatively you can paste the link in your browser that you recorded earlier. Click Switch Role. If this is the first time choosing this option, a page appears with more information. After reading it, click Switch Role. If you clear your browser cookies, this page can appear again. On the Switch Role page, type the account ID number or the account alias and the name of the role that you created for the Administrator in the previous step, for example, arn:aws:iam::account_ID:role/Administrator . (Optional) Type text that you want to appear on the navigation bar in place of your user name when this role is active. A name is suggested, based on the account and role information, but you can change it to whatever has meaning for you. You can also select a color to highlight the display name. The name and color can help remind you when this role is active, which changes your permissions. For example, for a role that gives you access to the test environment, you might specify a Display Name of Test and select the green Color. For the role that gives you access to production, you might specify a Display Name of Production and select red as the Color. Click Switch Role. The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you. Tip The last several roles that you used appear on the menu. The next time you need to switch to one of those roles, you can simply click the role you want. You only need to type the account and role information manually if the role is not displayed on the Identity menu. 7. You are now using the role with the granted permissions! To stop using a role In the IAM console, click your role's Display Name on the right side of the navigation bar. Click Back to UserName. The role and its permissions are deactivated, and the permissions associated with your IAM user and groups are automatically restored. 3. Tear down this lab Please note that the changes you made to the users, groups, and roles have no charges associated with them. References useful resources: AWS Identity and Access Management User Guide IAM Best Practices and Use Cases License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 100: Basic Identity & Access Management User, Group, Role: Lab Guide"},{"location":"Security/100 - Basic Identity & Access Management User, Group, Role/Lab Guide/#level-100-basic-identity-access-management-user-group-role-lab-guide","text":"","title":"Level 100: Basic Identity &amp; Access Management User, Group, Role: Lab Guide"},{"location":"Security/100 - Basic Identity & Access Management User, Group, Role/Lab Guide/#1-aws-identity-access-management","text":"As a best practice, do not use the AWS account root user for any task where it's not required. Instead, create a new IAM user for each person that requires administrator access. Then make those users administrators (only if they absolutely need full access to everything) by placing the users into an \"Administrators\" group to which you attach the AdministratorAccess managed policy.","title":"1. AWS Identity &amp; Access Management"},{"location":"Security/100 - Basic Identity & Access Management User, Group, Role/Lab Guide/#11-create-administrator-iam-user-and-group","text":"To create an administrator user for yourself and add the user to an administrators group: Use your AWS account email address and password to sign in as the AWS account root user to the IAM console at https://console.aws.amazon.com/iam/ . In the navigation pane, click Users and then click Add user. For User name, type a user name for yourself, such as Bob. Each user should have their own user name, do not share credentials. The name can consist of letters, digits, and the following characters: plus (+) , equal (=) , comma (,) , period (.) , at (@) , underscore (_) , and hyphen (-) . The name is not case sensitive and can be a maximum of 64 characters in length. Select the check box next to AWS Management Console access, select Custom password, and then type your new password in the text box. This user will be able to do almost anything in your account, by not giving it programmatic access (access secret key) you reduce your risk, and we will configure lower-privileged users and roles later. If you're creating the user for someone other than yourself, you can optionally select Require password reset to force the user to create a new password when first signing in. Click Next: Permissions. On the Set permissions for user page, click Add user to group. Click Create group. In the Create group dialog box, type the name for the new group such as Administrators. The name can consist of letters, digits, and the following characters: plus (+), equal (=), comma (,), period (.), at (@), underscore (_), and hyphen (-). The name is not case sensitive and can be a maximum of 128 characters in length. In the policy list, select the check box next to AdministratorAccess. Then click Create group. Back in the list of groups, verify the check box is next to your new group. Click Refresh if necessary to see the group in the list. Click Next: Review to see the list of group memberships to be added to the new user. When you are ready to proceed, click Create user. You can use this same process to create more groups and users and to give your users access to your AWS account resources. To learn about using policies that restrict user permissions to specific AWS resources, see Access Management and Example Policies . To add users to the group after it's created, see Adding and Removing Users in an IAM Group . Configure MFA on your new administrator user by choosing Users from the navigation pane. In the User Name list, click the name of the intended MFA user. Click the Security credentials tab. Next to Assigned MFA device, click the edit icon. You can now use this administrator user instead of your root user for this AWS account. It is a best practice to use least a least privileged approach to granting permissions, not everyone needs full administrator access!","title":"1.1 Create Administrator IAM User and Group"},{"location":"Security/100 - Basic Identity & Access Management User, Group, Role/Lab Guide/#12-create-administrator-iam-role","text":"To create an administrator role for yourself (and other administrators) to be used with the administrator user and group you just created: 1. Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/ . 2. In the navigation pane, click Roles and then click Create role. 3. Click Another AWS account, then enter your account ID and tick Require MFA, then click Next: Permissions 4. Tick AdministratorAccess from the list, and then click Next: Review. 5. Enter a role name, e.g. 'Administrators' then click Create role. 6. Check the role you have configured by clicking the role you have just created. Record both the Role ARN and the link to the console. You can also optionally change the session duration timeout. 6. The role is now created, with full administrative access and MFA enforced.","title":"1.2 Create Administrator IAM Role"},{"location":"Security/100 - Basic Identity & Access Management User, Group, Role/Lab Guide/#2-assume-administrator-role-from-an-iam-user","text":"We will assume the role using the IAM user that we previously created in the web console. As the IAM user has full access it is a best practice not to have access keys to assume the role on the CLI, instead we should use a restricted IAM user for this so we can enforce the requirement of MFA.","title":"2. Assume Administrator Role from an IAM user"},{"location":"Security/100 - Basic Identity & Access Management User, Group, Role/Lab Guide/#21-use-administrator-role-in-web-console","text":"A role specifies a set of permissions that you can use to access AWS resources that you need. In that sense, it is similar to a user in AWS Identity and Access Management (IAM). A benefit of roles is they allow you to enforce the use of an MFA token to help protect your credentials. When you sign in as a user, you get a specific set of permissions. However, you don't sign in to a role, but once signed in (as a user) you can switch to a role. This temporarily sets aside your original user permissions and instead gives you the permissions assigned to the role. The role can be in your own account or any other AWS account. By default, your AWS Management Console session lasts for one hour. Important The permissions of your IAM user and any roles that you switch to are not cumulative. Only one set of permissions is active at a time. When you switch to a role, you temporarily give up your user permissions and work with the permissions that are assigned to the role. When you exit the role, your user permissions are automatically restored. Sign in to the AWS Management Console as an IAM user https://console.aws.amazon.com . In the console, click your user name on the navigation bar in the upper right. It typically looks like this: username@account_ID_number_or_alias . Alternatively you can paste the link in your browser that you recorded earlier. Click Switch Role. If this is the first time choosing this option, a page appears with more information. After reading it, click Switch Role. If you clear your browser cookies, this page can appear again. On the Switch Role page, type the account ID number or the account alias and the name of the role that you created for the Administrator in the previous step, for example, arn:aws:iam::account_ID:role/Administrator . (Optional) Type text that you want to appear on the navigation bar in place of your user name when this role is active. A name is suggested, based on the account and role information, but you can change it to whatever has meaning for you. You can also select a color to highlight the display name. The name and color can help remind you when this role is active, which changes your permissions. For example, for a role that gives you access to the test environment, you might specify a Display Name of Test and select the green Color. For the role that gives you access to production, you might specify a Display Name of Production and select red as the Color. Click Switch Role. The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you. Tip The last several roles that you used appear on the menu. The next time you need to switch to one of those roles, you can simply click the role you want. You only need to type the account and role information manually if the role is not displayed on the Identity menu. 7. You are now using the role with the granted permissions! To stop using a role In the IAM console, click your role's Display Name on the right side of the navigation bar. Click Back to UserName. The role and its permissions are deactivated, and the permissions associated with your IAM user and groups are automatically restored.","title":"2.1 Use Administrator Role in Web Console"},{"location":"Security/100 - Basic Identity & Access Management User, Group, Role/Lab Guide/#3-tear-down-this-lab","text":"Please note that the changes you made to the users, groups, and roles have no charges associated with them.","title":"3. Tear down this lab"},{"location":"Security/100 - Basic Identity & Access Management User, Group, Role/Lab Guide/#references-useful-resources","text":"AWS Identity and Access Management User Guide IAM Best Practices and Use Cases","title":"References &amp; useful resources:"},{"location":"Security/100 - Basic Identity & Access Management User, Group, Role/Lab Guide/#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/100 - CloudFront with S3 Bucket Origin/","text":"Level 100: CloudFront with S3 Bucket Origin Introduction This hands-on lab will guide you through the steps to host static web content in an Amazon S3 bucket , protected and accelerated by Amazon CloudFront . Skills learned will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals: Protecting S3 bucket from direct public access Improving access time with caching Prerequisites: An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab. Overview Lab Guide.md the guide for this lab /Images referenced by this lab Permissions required IAM User with AdministratorAccess AWS managed policy License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 100: CloudFront with S3 Bucket Origin"},{"location":"Security/100 - CloudFront with S3 Bucket Origin/#level-100-cloudfront-with-s3-bucket-origin","text":"","title":"Level 100: CloudFront with S3 Bucket Origin"},{"location":"Security/100 - CloudFront with S3 Bucket Origin/#introduction","text":"This hands-on lab will guide you through the steps to host static web content in an Amazon S3 bucket , protected and accelerated by Amazon CloudFront . Skills learned will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/100 - CloudFront with S3 Bucket Origin/#goals","text":"Protecting S3 bucket from direct public access Improving access time with caching","title":"Goals:"},{"location":"Security/100 - CloudFront with S3 Bucket Origin/#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab.","title":"Prerequisites:"},{"location":"Security/100 - CloudFront with S3 Bucket Origin/#overview","text":"Lab Guide.md the guide for this lab /Images referenced by this lab","title":"Overview"},{"location":"Security/100 - CloudFront with S3 Bucket Origin/#permissions-required","text":"IAM User with AdministratorAccess AWS managed policy","title":"Permissions required"},{"location":"Security/100 - CloudFront with S3 Bucket Origin/#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/100 - CloudFront with S3 Bucket Origin/Lab Guide/","text":"Level 100: CloudFront with S3 Bucket Origin: Lab Guide 1. Create S3 bucket Create an Amazon S3 bucket to host static content using the Amazon S3 console. For more information about Amazon S3, see Introduction to Amazon S3 . 1. Open the Amazon S3 console at https://console.aws.amazon.com/s3/ . 2. From the console dashboard, choose Create bucket . 3. Enter a name for your bucket, type a unique DNS-compliant name for your new bucket. Follow these naming guidelines: The name must be unique across all existing bucket names in Amazon S3. The name must not contain uppercase characters. The name must start with a lowercase letter or number. The name must be between 3 and 63 characters long. Choose an AWS Region where you want the bucket to reside. Choose a Region close to you to minimize latency and costs, or to address regulatory requirements. Note that for this example we will accept the default settings and this bucket is secure by default. Consider enabling additional security options such as logging and encryption, the S3 documentation has additional information such as Protecting Data in Amazon S3 . Click Create . 2. Upload example index.html file Create a simple index.html file, you can create by coping the following text into your favourite text editor. !DOCTYPE html html head title Example /title /head body h1 Example Heading /h1 p Example paragraph. /p /body /html Open the Amazon S3 console at https://console.aws.amazon.com/s3/ . In the console click the name of your bucket you just created. Click the Upload button. Click the Add files button, select your index.html file, then click the Upload button. Your index.html file should now appear in the list. 3. Configure Amazon CloudFront Using the AWS Management Console, we will create a CloudFront distribution, and configure it to serve the S3 bucket we previously created. 1. Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home . 2. From the console dashboard, click Create Distribution . 3. Click Get Started in the Web section. 4. Specify the following settings for the distribution: * In the Origin Domain Name field Select the S3 bucket you created previously. * In Restrict Bucket Access click the Yes radio then click Create a New Identity . * Click the Yes, Update Bucket Policy Button . * Scroll down to the Distribution Settings section, in the Default Root Object field enter index.html * Click Create Distribution. * To return to the main CloudFront page click Distributions from the left navigation menu. * For more information on the other configuration options, see Values That You Specify When You Create or Update a Web Distribution in the CloudFront documentation. 5. After CloudFront creates your distribution which may take approximately 10 minutes, the value of the Status column for your distribution will change from In Progress to Deployed . 6. When your distribution is deployed, confirm that you can access your content using your new CloudFront Domain Name which you can see in the console. Copy the Domain Name into a web browser to test. For more information, see Testing a Web Distribution in the CloudFront documentation. 7. You now have content in a private S3 bucket, that only CloudFront has secure access to. CloudFront then serves the requests, effectively becoming a secure, reliable static hosting service with additional features available such as custom certificates and alternate domain names . For more information on configuring CloudFront, see Viewing and Updating CloudFront Distributions in the CloudFront documentation. 4. Tear down this lab The following instructions will remove the CloudFront distribution and S3 bucket created in this lab. Delete the CloudFront distribution: 1. Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. 2. From the console dashboard, select the distribution you created earlier and click the Disable button. To confirm, click the Yes, Disable button. 3. After approximately 15 minutes when the status is Disabled , select the distribution and click the Delete . button, and then to confirm click the Yes, Delete button. Delete the S3 bucket: 1. Open the Amazon S3 console at https://console.aws.amazon.com/s3/ . 2. Check the box next to the bucket you created previously, then click Empty from the menu. 3. Confirm the bucket you are emptying. 4. Once the bucket is empty check the box next to the bucket, then click Delete from the menu. 5. Confirm the bucket you are deleting. References useful resources: Amazon S3 Developer Guide Amazon CloudFront Developer Guide License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 100: CloudFront with S3 Bucket Origin: Lab Guide"},{"location":"Security/100 - CloudFront with S3 Bucket Origin/Lab Guide/#level-100-cloudfront-with-s3-bucket-origin-lab-guide","text":"","title":"Level 100: CloudFront with S3 Bucket Origin: Lab Guide"},{"location":"Security/100 - CloudFront with S3 Bucket Origin/Lab Guide/#1-create-s3-bucket","text":"Create an Amazon S3 bucket to host static content using the Amazon S3 console. For more information about Amazon S3, see Introduction to Amazon S3 . 1. Open the Amazon S3 console at https://console.aws.amazon.com/s3/ . 2. From the console dashboard, choose Create bucket . 3. Enter a name for your bucket, type a unique DNS-compliant name for your new bucket. Follow these naming guidelines: The name must be unique across all existing bucket names in Amazon S3. The name must not contain uppercase characters. The name must start with a lowercase letter or number. The name must be between 3 and 63 characters long. Choose an AWS Region where you want the bucket to reside. Choose a Region close to you to minimize latency and costs, or to address regulatory requirements. Note that for this example we will accept the default settings and this bucket is secure by default. Consider enabling additional security options such as logging and encryption, the S3 documentation has additional information such as Protecting Data in Amazon S3 . Click Create .","title":"1. Create S3 bucket"},{"location":"Security/100 - CloudFront with S3 Bucket Origin/Lab Guide/#2-upload-example-indexhtml-file","text":"Create a simple index.html file, you can create by coping the following text into your favourite text editor. !DOCTYPE html html head title Example /title /head body h1 Example Heading /h1 p Example paragraph. /p /body /html Open the Amazon S3 console at https://console.aws.amazon.com/s3/ . In the console click the name of your bucket you just created. Click the Upload button. Click the Add files button, select your index.html file, then click the Upload button. Your index.html file should now appear in the list.","title":"2. Upload example index.html file"},{"location":"Security/100 - CloudFront with S3 Bucket Origin/Lab Guide/#3-configure-amazon-cloudfront","text":"Using the AWS Management Console, we will create a CloudFront distribution, and configure it to serve the S3 bucket we previously created. 1. Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home . 2. From the console dashboard, click Create Distribution . 3. Click Get Started in the Web section. 4. Specify the following settings for the distribution: * In the Origin Domain Name field Select the S3 bucket you created previously. * In Restrict Bucket Access click the Yes radio then click Create a New Identity . * Click the Yes, Update Bucket Policy Button . * Scroll down to the Distribution Settings section, in the Default Root Object field enter index.html * Click Create Distribution. * To return to the main CloudFront page click Distributions from the left navigation menu. * For more information on the other configuration options, see Values That You Specify When You Create or Update a Web Distribution in the CloudFront documentation. 5. After CloudFront creates your distribution which may take approximately 10 minutes, the value of the Status column for your distribution will change from In Progress to Deployed . 6. When your distribution is deployed, confirm that you can access your content using your new CloudFront Domain Name which you can see in the console. Copy the Domain Name into a web browser to test. For more information, see Testing a Web Distribution in the CloudFront documentation. 7. You now have content in a private S3 bucket, that only CloudFront has secure access to. CloudFront then serves the requests, effectively becoming a secure, reliable static hosting service with additional features available such as custom certificates and alternate domain names . For more information on configuring CloudFront, see Viewing and Updating CloudFront Distributions in the CloudFront documentation.","title":"3. Configure Amazon CloudFront"},{"location":"Security/100 - CloudFront with S3 Bucket Origin/Lab Guide/#4-tear-down-this-lab","text":"The following instructions will remove the CloudFront distribution and S3 bucket created in this lab. Delete the CloudFront distribution: 1. Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. 2. From the console dashboard, select the distribution you created earlier and click the Disable button. To confirm, click the Yes, Disable button. 3. After approximately 15 minutes when the status is Disabled , select the distribution and click the Delete . button, and then to confirm click the Yes, Delete button. Delete the S3 bucket: 1. Open the Amazon S3 console at https://console.aws.amazon.com/s3/ . 2. Check the box next to the bucket you created previously, then click Empty from the menu. 3. Confirm the bucket you are emptying. 4. Once the bucket is empty check the box next to the bucket, then click Delete from the menu. 5. Confirm the bucket you are deleting.","title":"4. Tear down this lab"},{"location":"Security/100 - CloudFront with S3 Bucket Origin/Lab Guide/#references-useful-resources","text":"Amazon S3 Developer Guide Amazon CloudFront Developer Guide","title":"References &amp; useful resources:"},{"location":"Security/100 - CloudFront with S3 Bucket Origin/Lab Guide/#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200 - Automated Deployment of Detective Controls/","text":"Level 200: Automated Deployment of Detective Controls Introduction This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure detective controls including AWS CloudTrail and Amazon GuardDuty. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of AWS CloudTrail. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals: Implement detective controls Automate security best practices Prerequisites: An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Overview Lab Guide.md the guide for this lab /Code Code including CloudFormation templates related to this lab /Images referenced by this lab License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 200: Automated Deployment of Detective Controls"},{"location":"Security/200 - Automated Deployment of Detective Controls/#level-200-automated-deployment-of-detective-controls","text":"","title":"Level 200: Automated Deployment of Detective Controls"},{"location":"Security/200 - Automated Deployment of Detective Controls/#introduction","text":"This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure detective controls including AWS CloudTrail and Amazon GuardDuty. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of AWS CloudTrail. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/200 - Automated Deployment of Detective Controls/#goals","text":"Implement detective controls Automate security best practices","title":"Goals:"},{"location":"Security/200 - Automated Deployment of Detective Controls/#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites:"},{"location":"Security/200 - Automated Deployment of Detective Controls/#overview","text":"Lab Guide.md the guide for this lab /Code Code including CloudFormation templates related to this lab /Images referenced by this lab","title":"Overview"},{"location":"Security/200 - Automated Deployment of Detective Controls/#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200 - Automated Deployment of Detective Controls/Lab Guide/","text":"Level 200: Automated Deployment of Detective Controls: Lab Guide Authors Ben Potter, Security Lead, Well-Architected Table of Contents CloudTrail GuardDuty Config Tear Down 1. AWS CloudFormation to Configure Customized AWS CloudTrail AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. CloudTrail provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools, and other AWS services. Using AWS CloudFormation , we are going to create a new Amazon S3 bucket, and configure CloudTrail to send events to the bucket and to Amazon CloudWatch Logs for further analysis. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Click Create stack . Enter the following Amazon S3 URL : https://s3-us-west-2.amazonaws.com/aws-well-architected-labs/Security/Code/baseline-cloudtrail.yaml and click Next . Enter the following details: Stack name: The name of this stack. For this lab, use cloudtrail . CloudTrailBucketName: The name of the new S3 bucket to create for CloudTrail to send logs to. IMPORTANT Specify a bucket name that is unique. S3AccessLogsBucketName: The name of an existing S3 bucket for storing S3 access logs (optional). CloudWatchLogsRetentionTime: Number of days to retain logs in CloudWatch Logs. EncryptLogs: (optional) Use AWS KMS to encrypt logs stored in Amazon S3. A new KMS key will be created. BucketPolicyExplicitDeny: (optional) Explicitly deny destructive actions to the bucket. AWS root user will be required to modify this bucket if configured. ExpirationDays: Number of days to retain logs in the S3 bucket before they are automatically deleted. At the bottom of the page click Next . In this lab, we won't add any tags or other options. Click Next. Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you're satisfied with the configuration, check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack . After a few minutes the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You have now set up AWS CloudTrail to log to your bucket and retain events, giving you the ability to search history and later enable pro-active monitoring of your AWS account! 2. AWS CloudFormation to Configure Amazon GuardDuty Amazon GuardDuty is a threat detection service that continuously monitors for malicious or unauthorized behavior to help you protect your AWS accounts and workloads. It monitors for activity such as unusual API calls or potentially unauthorized deployments that indicate a possible account compromise. GuardDuty also detects potentially compromised instances or reconnaissance by attackers. Using AWS CloudFormation , we are going to configure GuardDuty, and configure alerting to your email address. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Click Create stack . Enter the following Amazon S3 URL : https://s3-us-west-2.amazonaws.com/aws-well-architected-labs/Security/Code/baseline-guardduty.yaml and click Next . Enter the following details: Stack name: The name of this stack. For this lab, use guardduty . CWEventId: The event Id for CloudWatch, leave as default. EmailAddress: The email address you own that will receive the alerts, you must have access to this address for testing. SNSTopicName: The name of the SNS topic to create, leave as default. At the bottom of the page click Next . In this lab, we won't add any tags or other options. Click Next. Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you're satisfied with the configuration, click Create stack . After a few minutes the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You should receive an email to confirm the SNS email subscription, you must confirm this. You have now set up Amazon GuardDuty to detect threats and email alerts. 3. AWS CloudFormation to Configure AWS Config AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations. Using AWS Config , we are going to create a new Amazon S3 bucket, and configure Config to save snapshots to the bucket. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Click Create stack . Enter the following Amazon S3 URL : https://s3-us-west-2.amazonaws.com/aws-well-architected-labs/Security/Code/baseline-config.yaml and click Next . 4. Enter the following details: Stack name: The name of this stack. For this lab, use config . ConfigBucketName: The name of the new S3 bucket to create for Config to save config snapshots to. IMPORTANT Specify a bucket name that is unique. S3AccessLogsBucketName: The name of an existing S3 bucket for storing S3 access logs (optional). InfrequentAccessTransitionTime: Number of days to keep S3 objects in standard tier before moving to infrequently accessed. BucketPolicyExplicitDeny: (optional) Explicitly deny destructive actions to the bucket. AWS root user will be required to modify this bucket if configured. At the bottom of the page click Next . In this lab, we won't add any tags or other options. Click Next. Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you're satisfied with the configuration, check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack . After a few minutes the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You have now set up AWS Config to log to your bucket and retain events, giving you the ability to search history and later enable pro-active monitoring of your AWS account! 4. Tear down this lab The following instructions will remove the resources that have a cost for running them. Delete the CloudTrail stack: 1. Sign in to the AWS Management Console, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. 2. Select the cloudtrail stack. 3. Click the Actions button then click Delete Stack. 4. Confirm the stack and then click the Yes, Delete button. Empty and delete the CloudTrail bucket: 1. Sign in to the AWS Management Console, and open the S3 console at https://console.aws.amazon.com/s3/. 2. Select the bucket name you previously created without clicking the name. 3. Click Empty bucket and enter the bucket name in the confirmation box. 4. Click Confirm and the bucket will be emptied when the bottom task bar has 0 operations in progress. 5. With the bucket now empty, click Delete bucket. 6. Enter the bucket name in the confirmation box and click Confirm. 7. The bucket will then be removed from the console. Delete the GuardDuty stack: 1. Sign in to the AWS Management Console, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. 2. Select the guardduty stack. 3. Click the Actions button then click Delete Stack. 4. Confirm the stack and then click the Yes, Delete button. Delete the Config stack: 1. Sign in to the AWS Management Console, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. 2. Select the config stack. 3. Click the Actions button then click Delete Stack. 4. Confirm the stack and then click the Yes, Delete button. References useful resources: AWS CloudTrail User Guide AWS CloudFormation User Guide Amazon GuardDuty User Guide AWS Config User Guide License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 200: Automated Deployment of Detective Controls: Lab Guide"},{"location":"Security/200 - Automated Deployment of Detective Controls/Lab Guide/#level-200-automated-deployment-of-detective-controls-lab-guide","text":"","title":"Level 200: Automated Deployment of Detective Controls: Lab Guide"},{"location":"Security/200 - Automated Deployment of Detective Controls/Lab Guide/#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/200 - Automated Deployment of Detective Controls/Lab Guide/#table-of-contents","text":"CloudTrail GuardDuty Config Tear Down","title":"Table of Contents"},{"location":"Security/200 - Automated Deployment of Detective Controls/Lab Guide/#1-aws-cloudformation-to-configure-customized-aws-cloudtrail","text":"AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. CloudTrail provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools, and other AWS services. Using AWS CloudFormation , we are going to create a new Amazon S3 bucket, and configure CloudTrail to send events to the bucket and to Amazon CloudWatch Logs for further analysis. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Click Create stack . Enter the following Amazon S3 URL : https://s3-us-west-2.amazonaws.com/aws-well-architected-labs/Security/Code/baseline-cloudtrail.yaml and click Next . Enter the following details: Stack name: The name of this stack. For this lab, use cloudtrail . CloudTrailBucketName: The name of the new S3 bucket to create for CloudTrail to send logs to. IMPORTANT Specify a bucket name that is unique. S3AccessLogsBucketName: The name of an existing S3 bucket for storing S3 access logs (optional). CloudWatchLogsRetentionTime: Number of days to retain logs in CloudWatch Logs. EncryptLogs: (optional) Use AWS KMS to encrypt logs stored in Amazon S3. A new KMS key will be created. BucketPolicyExplicitDeny: (optional) Explicitly deny destructive actions to the bucket. AWS root user will be required to modify this bucket if configured. ExpirationDays: Number of days to retain logs in the S3 bucket before they are automatically deleted. At the bottom of the page click Next . In this lab, we won't add any tags or other options. Click Next. Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you're satisfied with the configuration, check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack . After a few minutes the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You have now set up AWS CloudTrail to log to your bucket and retain events, giving you the ability to search history and later enable pro-active monitoring of your AWS account!","title":"1. AWS CloudFormation to Configure Customized AWS CloudTrail "},{"location":"Security/200 - Automated Deployment of Detective Controls/Lab Guide/#2-aws-cloudformation-to-configure-amazon-guardduty","text":"Amazon GuardDuty is a threat detection service that continuously monitors for malicious or unauthorized behavior to help you protect your AWS accounts and workloads. It monitors for activity such as unusual API calls or potentially unauthorized deployments that indicate a possible account compromise. GuardDuty also detects potentially compromised instances or reconnaissance by attackers. Using AWS CloudFormation , we are going to configure GuardDuty, and configure alerting to your email address. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Click Create stack . Enter the following Amazon S3 URL : https://s3-us-west-2.amazonaws.com/aws-well-architected-labs/Security/Code/baseline-guardduty.yaml and click Next . Enter the following details: Stack name: The name of this stack. For this lab, use guardduty . CWEventId: The event Id for CloudWatch, leave as default. EmailAddress: The email address you own that will receive the alerts, you must have access to this address for testing. SNSTopicName: The name of the SNS topic to create, leave as default. At the bottom of the page click Next . In this lab, we won't add any tags or other options. Click Next. Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you're satisfied with the configuration, click Create stack . After a few minutes the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You should receive an email to confirm the SNS email subscription, you must confirm this. You have now set up Amazon GuardDuty to detect threats and email alerts.","title":"2. AWS CloudFormation to Configure Amazon GuardDuty "},{"location":"Security/200 - Automated Deployment of Detective Controls/Lab Guide/#3-aws-cloudformation-to-configure-aws-config","text":"AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations. Using AWS Config , we are going to create a new Amazon S3 bucket, and configure Config to save snapshots to the bucket. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Click Create stack . Enter the following Amazon S3 URL : https://s3-us-west-2.amazonaws.com/aws-well-architected-labs/Security/Code/baseline-config.yaml and click Next . 4. Enter the following details: Stack name: The name of this stack. For this lab, use config . ConfigBucketName: The name of the new S3 bucket to create for Config to save config snapshots to. IMPORTANT Specify a bucket name that is unique. S3AccessLogsBucketName: The name of an existing S3 bucket for storing S3 access logs (optional). InfrequentAccessTransitionTime: Number of days to keep S3 objects in standard tier before moving to infrequently accessed. BucketPolicyExplicitDeny: (optional) Explicitly deny destructive actions to the bucket. AWS root user will be required to modify this bucket if configured. At the bottom of the page click Next . In this lab, we won't add any tags or other options. Click Next. Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you're satisfied with the configuration, check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack . After a few minutes the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You have now set up AWS Config to log to your bucket and retain events, giving you the ability to search history and later enable pro-active monitoring of your AWS account!","title":"3. AWS CloudFormation to Configure AWS Config "},{"location":"Security/200 - Automated Deployment of Detective Controls/Lab Guide/#4-tear-down-this-lab","text":"The following instructions will remove the resources that have a cost for running them. Delete the CloudTrail stack: 1. Sign in to the AWS Management Console, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. 2. Select the cloudtrail stack. 3. Click the Actions button then click Delete Stack. 4. Confirm the stack and then click the Yes, Delete button. Empty and delete the CloudTrail bucket: 1. Sign in to the AWS Management Console, and open the S3 console at https://console.aws.amazon.com/s3/. 2. Select the bucket name you previously created without clicking the name. 3. Click Empty bucket and enter the bucket name in the confirmation box. 4. Click Confirm and the bucket will be emptied when the bottom task bar has 0 operations in progress. 5. With the bucket now empty, click Delete bucket. 6. Enter the bucket name in the confirmation box and click Confirm. 7. The bucket will then be removed from the console. Delete the GuardDuty stack: 1. Sign in to the AWS Management Console, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. 2. Select the guardduty stack. 3. Click the Actions button then click Delete Stack. 4. Confirm the stack and then click the Yes, Delete button. Delete the Config stack: 1. Sign in to the AWS Management Console, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. 2. Select the config stack. 3. Click the Actions button then click Delete Stack. 4. Confirm the stack and then click the Yes, Delete button.","title":"4. Tear down this lab "},{"location":"Security/200 - Automated Deployment of Detective Controls/Lab Guide/#references-useful-resources","text":"AWS CloudTrail User Guide AWS CloudFormation User Guide Amazon GuardDuty User Guide AWS Config User Guide","title":"References &amp; useful resources:"},{"location":"Security/200 - Automated Deployment of Detective Controls/Lab Guide/#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200 - Automated Deployment of EC2 Web Application/","text":"Level 200: Automated Deployment of EC2 Web Application Introduction This hands-on lab will guide you through the steps to configure a web application in Amazon EC2 with a defense in depth approach. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . The WordPress example CloudFormation template will deploy a basic WordPress content management system, incorporating a number of AWS security best practices. This example is not intended to be a comprehensive WordPress system, please consult Build a WordPress Website for more information. Goals: EC2 automated deployment Autoscaling and load balancing Prerequisites: An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account with full access to CloudFormation, EC2, VPC, IAM, Elastic Load Balancing. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Basic understanding of AWS CloudFormation , visit the Getting Started section of the user guide. Deployed the CloudFormation VPC stack in the lab Automated Deployment of VPC . EC2 SSH key for accessing instances. Overview Lab Guide.md the guide for this lab /Code Code including CloudFormation templates related to this lab /Images referenced by this lab Permissions required IAM User with AdministratorAccess AWS managed policy License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 200: Automated Deployment of EC2 Web Application"},{"location":"Security/200 - Automated Deployment of EC2 Web Application/#level-200-automated-deployment-of-ec2-web-application","text":"","title":"Level 200: Automated Deployment of EC2 Web Application"},{"location":"Security/200 - Automated Deployment of EC2 Web Application/#introduction","text":"This hands-on lab will guide you through the steps to configure a web application in Amazon EC2 with a defense in depth approach. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . The WordPress example CloudFormation template will deploy a basic WordPress content management system, incorporating a number of AWS security best practices. This example is not intended to be a comprehensive WordPress system, please consult Build a WordPress Website for more information.","title":"Introduction"},{"location":"Security/200 - Automated Deployment of EC2 Web Application/#goals","text":"EC2 automated deployment Autoscaling and load balancing","title":"Goals:"},{"location":"Security/200 - Automated Deployment of EC2 Web Application/#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account with full access to CloudFormation, EC2, VPC, IAM, Elastic Load Balancing. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Basic understanding of AWS CloudFormation , visit the Getting Started section of the user guide. Deployed the CloudFormation VPC stack in the lab Automated Deployment of VPC . EC2 SSH key for accessing instances.","title":"Prerequisites:"},{"location":"Security/200 - Automated Deployment of EC2 Web Application/#overview","text":"Lab Guide.md the guide for this lab /Code Code including CloudFormation templates related to this lab /Images referenced by this lab","title":"Overview"},{"location":"Security/200 - Automated Deployment of EC2 Web Application/#permissions-required","text":"IAM User with AdministratorAccess AWS managed policy","title":"Permissions required"},{"location":"Security/200 - Automated Deployment of EC2 Web Application/#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200 - Automated Deployment of EC2 Web Application/Lab Guide/","text":"Level 200: Automated Deployment of EC2 Web Application Authors Ben Potter, Security Lead, Well-Architected Table of Contents Create Web Stack Tear Down 1. Create Web Stack Please note a prerequisite to this lab is that you have deployed the CloudFormation VPC stack in the lab Automated Deployment of VPC with the default parameters and recommended stack name, and have an EC2 SSH key created for instances. This step will create the web application and all components using the example CloudFormation template, inside the VPC you have created previously. 1. Download the latest version of the CloudFormation template from /Code to your computer. 2. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . 3. Click Create Stack. 4. Click Upload a template file and then click Choose file . 5. Choose the CloudFormation template you downloaded in step 1, return to the CloudFormation console page and click Next . 5. Enter the following details: * Stack name: The name of this stack. For this lab, use WebApp1-WordPress and match the case. * Web1SSHKeyName: Select an existing EC2 SSH key you have. * ALBSGSource: Your current IP address in CIDR notation which will be allowed to connect to the application load balancer, this secures your web application from the public while you are configuring and testing. * DB1User: User name of your choice for the RDS database that must contain 1 to 16 alphanumeric characters. first character must be a letter, cannot be a word reserved by the database engine. * DB1Password: Password of your choice for the RDS database that can be any printable ASCII character except \"/\", \"\"\", or \"@\". and contain 8 to 41 characters. The remaining parameters may be left as defaults, you can find out more in the description for each. 6. At the bottom of the page click Next . 7. In this lab, we won't add any tags or other options. Click Next. Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . 8. Review the information for the stack. When you're satisfied with the configuration, check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack . 9. After a number of minutes the final stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You have now created the WordPress stack (well actually CloudFormation did it for you). 2. Tear down this lab The following instructions will remove the resources that you have created in this lab. Delete the WordPress CloudFormation stack: 1. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . 2. Click the radio button on the left of the WebApp1-WordPress stack. 3. Click the Actions button then click Delete stack . 4. Confirm the stack and then click Delete button. References useful resources: AWS CloudFormation User Guide Amazon EC2 User Guide for Linux Instances License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 200: Automated Deployment of EC2 Web Application"},{"location":"Security/200 - Automated Deployment of EC2 Web Application/Lab Guide/#level-200-automated-deployment-of-ec2-web-application","text":"","title":"Level 200: Automated Deployment of EC2 Web Application"},{"location":"Security/200 - Automated Deployment of EC2 Web Application/Lab Guide/#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/200 - Automated Deployment of EC2 Web Application/Lab Guide/#table-of-contents","text":"Create Web Stack Tear Down","title":"Table of Contents"},{"location":"Security/200 - Automated Deployment of EC2 Web Application/Lab Guide/#1-create-web-stack","text":"Please note a prerequisite to this lab is that you have deployed the CloudFormation VPC stack in the lab Automated Deployment of VPC with the default parameters and recommended stack name, and have an EC2 SSH key created for instances. This step will create the web application and all components using the example CloudFormation template, inside the VPC you have created previously. 1. Download the latest version of the CloudFormation template from /Code to your computer. 2. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . 3. Click Create Stack. 4. Click Upload a template file and then click Choose file . 5. Choose the CloudFormation template you downloaded in step 1, return to the CloudFormation console page and click Next . 5. Enter the following details: * Stack name: The name of this stack. For this lab, use WebApp1-WordPress and match the case. * Web1SSHKeyName: Select an existing EC2 SSH key you have. * ALBSGSource: Your current IP address in CIDR notation which will be allowed to connect to the application load balancer, this secures your web application from the public while you are configuring and testing. * DB1User: User name of your choice for the RDS database that must contain 1 to 16 alphanumeric characters. first character must be a letter, cannot be a word reserved by the database engine. * DB1Password: Password of your choice for the RDS database that can be any printable ASCII character except \"/\", \"\"\", or \"@\". and contain 8 to 41 characters. The remaining parameters may be left as defaults, you can find out more in the description for each. 6. At the bottom of the page click Next . 7. In this lab, we won't add any tags or other options. Click Next. Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . 8. Review the information for the stack. When you're satisfied with the configuration, check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack . 9. After a number of minutes the final stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You have now created the WordPress stack (well actually CloudFormation did it for you).","title":"1. Create Web Stack "},{"location":"Security/200 - Automated Deployment of EC2 Web Application/Lab Guide/#2-tear-down-this-lab","text":"The following instructions will remove the resources that you have created in this lab. Delete the WordPress CloudFormation stack: 1. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . 2. Click the radio button on the left of the WebApp1-WordPress stack. 3. Click the Actions button then click Delete stack . 4. Confirm the stack and then click Delete button.","title":"2. Tear down this lab"},{"location":"Security/200 - Automated Deployment of EC2 Web Application/Lab Guide/#references-useful-resources","text":"AWS CloudFormation User Guide Amazon EC2 User Guide for Linux Instances","title":"References &amp; useful resources:"},{"location":"Security/200 - Automated Deployment of EC2 Web Application/Lab Guide/#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200 - Automated Deployment of IAM Groups and Roles/","text":"Level 200: Automated Deployment of IAM Groups and Roles Introduction This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure AWS Identity and Access Management (IAM) Groups and roles for cross-account access. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of a new or existing AWS account with IAM best practices. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals: Fine-grained authorization Automate security best practices Prerequisites: An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Overview Lab Guide.md the guide for this lab /Code Code including CloudFormation templates related to this lab /Images referenced by this lab License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 200: Automated Deployment of IAM Groups and Roles"},{"location":"Security/200 - Automated Deployment of IAM Groups and Roles/#level-200-automated-deployment-of-iam-groups-and-roles","text":"","title":"Level 200: Automated Deployment of IAM Groups and Roles"},{"location":"Security/200 - Automated Deployment of IAM Groups and Roles/#introduction","text":"This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure AWS Identity and Access Management (IAM) Groups and roles for cross-account access. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of a new or existing AWS account with IAM best practices. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/200 - Automated Deployment of IAM Groups and Roles/#goals","text":"Fine-grained authorization Automate security best practices","title":"Goals:"},{"location":"Security/200 - Automated Deployment of IAM Groups and Roles/#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites:"},{"location":"Security/200 - Automated Deployment of IAM Groups and Roles/#overview","text":"Lab Guide.md the guide for this lab /Code Code including CloudFormation templates related to this lab /Images referenced by this lab","title":"Overview"},{"location":"Security/200 - Automated Deployment of IAM Groups and Roles/#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200 - Automated Deployment of IAM Groups and Roles/Lab Guide/","text":"Level 200: Automated Deployment of IAM Groups and Roles: Lab Guide 1. AWS CloudFormation to Create a Groups, Policies and Roles with MFA Enforced Using AWS CloudFormation we are going to deploy a set of groups, roles, and managed policies that will help with your security \"baseline\" of your AWS account. 1.1 Create AWS CloudFormation Stack Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Click Create New Stack. Select Specify an Amazon S3 template URL and enter the following URL for the template: https://s3-us-west-2.amazonaws.com/aws-well-architected-labs/Security/Code/baseline-iam.yaml and click Next. Enter the following details: Stack name: The name of this stack. For this lab, use baseline-iam . AllowRegion: A single region to restrict access, enter your preferred region. BaselineExportName: The CloudFormation export name prefix used with the resource name for the resources created, for example, Baseline-PrivilegedAdminRole. BaselineNamePrefix: The prefix for roles, groups, and policies created by this stack. IdentityManagementAccount: (optional) AccountId that contains centralized IAM users and is trusted to assume all roles, or blank for no cross-account trust. Note that the trusted account needs to be appropriately secured. OrganizationsRootAccount: (optional) AccountId that is trusted to assume Organizations role, or blank for no cross-account trust. Note that the trusted account needs to be appropriately secured. ToolingManagementAccount: AccountId that is trusted to assume the ReadOnly and StackSet roles, or blank for no cross-account trust. Note that the trusted account needs to be appropriately secured. Click Next. In this scenario, we won't add any tags or other options. Click Next. Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you're satisfied with the settings, click Next. Select I acknowledge that AWS CloudFormation might create IAM resources with custom names, and click Create. After a few minutes, the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE. You have now set up a number of managed polices, groups, and roles that you can test to improve your AWS security! 2. Assume Roles from an IAM user We will assume the roles previously created in the web console and command line interface (CLI) using an existing IAM user. 2.1 Use Restricted Administrator Role in Web Console A role specifies a set of permissions that you can use to access AWS resources that you need. In that sense, it is similar to a user in AWS Identity and Access Management (IAM). A benefit of roles is they allow you to enforce the use of an MFA token to help protect your credentials. When you sign in as a user, you get a specific set of permissions. However, you don't sign in to a role, but once signed in (as a user) you can switch to a role. This temporarily sets aside your original user permissions and instead gives you the permissions assigned to the role. The role can be in your own account or any other AWS account. By default, your AWS Management Console session lasts for one hour. Important The permissions of your IAM user and any roles that you switch to are not cumulative. Only one set of permissions is active at a time. When you switch to a role, you temporarily give up your user permissions and work with the permissions that are assigned to the role. When you exit the role, your user permissions are automatically restored. Sign in to the AWS Management Console as an IAM user https://console.aws.amazon.com . In the console, click your user name on the navigation bar in the upper right. It typically looks like this: username@account_ID_number_or_alias . Alternatively you can paste the link in your browser that you recorded earlier. Click Switch Role. If this is the first time choosing this option, a page appears with more information. After reading it, click Switch Role. If you clear your browser cookies, this page can appear again. On the Switch Role page, type the account ID number or the account alias and the name of the role that you created for the Administrator in the previous step, for example, arn:aws:iam::account_ID:role/Administrator . (Optional) Type text that you want to appear on the navigation bar in place of your user name when this role is active. A name is suggested, based on the account and role information, but you can change it to whatever has meaning for you. You can also select a color to highlight the display name. The name and color can help remind you when this role is active, which changes your permissions. For example, for a role that gives you access to the test environment, you might specify a Display Name of Test and select the green Color. For the role that gives you access to production, you might specify a Display Name of Production and select red as the Color. Click Switch Role. The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you. Tip The last several roles that you used appear on the menu. The next time you need to switch to one of those roles, you can simply choose the role you want. You only need to type the account and role information manually if the role is not displayed on the Identity menu. 7. You are now using the role with the granted permissions! To stop using a role In the IAM console, choose your role's Display Name on the right side of the navigation bar. Choose Back to UserName. The role and its permissions are deactivated, and the permissions associated with your IAM user and groups are automatically restored. 2.2 Use Restricted Administrator Role in Command Line Interface (CLI) Coming soon, for now check out: https://docs.aws.amazon.com/cli/latest/userguide/cli-roles.html 3. Tear down this lab The following instructions will remove the resources that have a cost for running them. Please note that the changes you made to the root login, users, groups, and policies have no charges associated with them. Delete the IAM stack: 1. Sign in to the AWS Management Console, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. 2. Select the baseline-iam stack. 3. Click the Actions button then click Delete Stack. 4. Confirm the stack and then click the Yes, Delete button. References useful resources: AWS Identity and Access Management User Guide IAM Best Practices and Use Cases AWS CloudFormation User Guide License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 200: Automated Deployment of IAM Groups and Roles: Lab Guide"},{"location":"Security/200 - Automated Deployment of IAM Groups and Roles/Lab Guide/#level-200-automated-deployment-of-iam-groups-and-roles-lab-guide","text":"","title":"Level 200: Automated Deployment of IAM Groups and Roles: Lab Guide"},{"location":"Security/200 - Automated Deployment of IAM Groups and Roles/Lab Guide/#1-aws-cloudformation-to-create-a-groups-policies-and-roles-with-mfa-enforced","text":"Using AWS CloudFormation we are going to deploy a set of groups, roles, and managed policies that will help with your security \"baseline\" of your AWS account.","title":"1. AWS CloudFormation to Create a Groups, Policies and Roles with MFA Enforced"},{"location":"Security/200 - Automated Deployment of IAM Groups and Roles/Lab Guide/#11-create-aws-cloudformation-stack","text":"Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Click Create New Stack. Select Specify an Amazon S3 template URL and enter the following URL for the template: https://s3-us-west-2.amazonaws.com/aws-well-architected-labs/Security/Code/baseline-iam.yaml and click Next. Enter the following details: Stack name: The name of this stack. For this lab, use baseline-iam . AllowRegion: A single region to restrict access, enter your preferred region. BaselineExportName: The CloudFormation export name prefix used with the resource name for the resources created, for example, Baseline-PrivilegedAdminRole. BaselineNamePrefix: The prefix for roles, groups, and policies created by this stack. IdentityManagementAccount: (optional) AccountId that contains centralized IAM users and is trusted to assume all roles, or blank for no cross-account trust. Note that the trusted account needs to be appropriately secured. OrganizationsRootAccount: (optional) AccountId that is trusted to assume Organizations role, or blank for no cross-account trust. Note that the trusted account needs to be appropriately secured. ToolingManagementAccount: AccountId that is trusted to assume the ReadOnly and StackSet roles, or blank for no cross-account trust. Note that the trusted account needs to be appropriately secured. Click Next. In this scenario, we won't add any tags or other options. Click Next. Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you're satisfied with the settings, click Next. Select I acknowledge that AWS CloudFormation might create IAM resources with custom names, and click Create. After a few minutes, the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE. You have now set up a number of managed polices, groups, and roles that you can test to improve your AWS security!","title":"1.1 Create AWS CloudFormation Stack"},{"location":"Security/200 - Automated Deployment of IAM Groups and Roles/Lab Guide/#2-assume-roles-from-an-iam-user","text":"We will assume the roles previously created in the web console and command line interface (CLI) using an existing IAM user.","title":"2. Assume Roles from an IAM user"},{"location":"Security/200 - Automated Deployment of IAM Groups and Roles/Lab Guide/#21-use-restricted-administrator-role-in-web-console","text":"A role specifies a set of permissions that you can use to access AWS resources that you need. In that sense, it is similar to a user in AWS Identity and Access Management (IAM). A benefit of roles is they allow you to enforce the use of an MFA token to help protect your credentials. When you sign in as a user, you get a specific set of permissions. However, you don't sign in to a role, but once signed in (as a user) you can switch to a role. This temporarily sets aside your original user permissions and instead gives you the permissions assigned to the role. The role can be in your own account or any other AWS account. By default, your AWS Management Console session lasts for one hour. Important The permissions of your IAM user and any roles that you switch to are not cumulative. Only one set of permissions is active at a time. When you switch to a role, you temporarily give up your user permissions and work with the permissions that are assigned to the role. When you exit the role, your user permissions are automatically restored. Sign in to the AWS Management Console as an IAM user https://console.aws.amazon.com . In the console, click your user name on the navigation bar in the upper right. It typically looks like this: username@account_ID_number_or_alias . Alternatively you can paste the link in your browser that you recorded earlier. Click Switch Role. If this is the first time choosing this option, a page appears with more information. After reading it, click Switch Role. If you clear your browser cookies, this page can appear again. On the Switch Role page, type the account ID number or the account alias and the name of the role that you created for the Administrator in the previous step, for example, arn:aws:iam::account_ID:role/Administrator . (Optional) Type text that you want to appear on the navigation bar in place of your user name when this role is active. A name is suggested, based on the account and role information, but you can change it to whatever has meaning for you. You can also select a color to highlight the display name. The name and color can help remind you when this role is active, which changes your permissions. For example, for a role that gives you access to the test environment, you might specify a Display Name of Test and select the green Color. For the role that gives you access to production, you might specify a Display Name of Production and select red as the Color. Click Switch Role. The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you. Tip The last several roles that you used appear on the menu. The next time you need to switch to one of those roles, you can simply choose the role you want. You only need to type the account and role information manually if the role is not displayed on the Identity menu. 7. You are now using the role with the granted permissions! To stop using a role In the IAM console, choose your role's Display Name on the right side of the navigation bar. Choose Back to UserName. The role and its permissions are deactivated, and the permissions associated with your IAM user and groups are automatically restored.","title":"2.1 Use Restricted Administrator Role in Web Console"},{"location":"Security/200 - Automated Deployment of IAM Groups and Roles/Lab Guide/#22-use-restricted-administrator-role-in-command-line-interface-cli","text":"Coming soon, for now check out: https://docs.aws.amazon.com/cli/latest/userguide/cli-roles.html","title":"2.2 Use Restricted Administrator Role in Command Line Interface (CLI)"},{"location":"Security/200 - Automated Deployment of IAM Groups and Roles/Lab Guide/#3-tear-down-this-lab","text":"The following instructions will remove the resources that have a cost for running them. Please note that the changes you made to the root login, users, groups, and policies have no charges associated with them. Delete the IAM stack: 1. Sign in to the AWS Management Console, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. 2. Select the baseline-iam stack. 3. Click the Actions button then click Delete Stack. 4. Confirm the stack and then click the Yes, Delete button.","title":"3. Tear down this lab"},{"location":"Security/200 - Automated Deployment of IAM Groups and Roles/Lab Guide/#references-useful-resources","text":"AWS Identity and Access Management User Guide IAM Best Practices and Use Cases AWS CloudFormation User Guide","title":"References &amp; useful resources:"},{"location":"Security/200 - Automated Deployment of IAM Groups and Roles/Lab Guide/#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200 - Automated Deployment of VPC/","text":"Level 200: Automated Deployment of VPC Introduction This hands-on lab will guide you through the steps to configure an Amazon VPC and outline some of the available security features. AWS CloudFormation will be used to automate the deployment and provide a repeatable way to re-use the template after this lab. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . The example CloudFormation template will deploy a complete VPC incorporating a number of AWS security best practices. Subnets are created in multiple availability zones for the following network tiers: * Application Load Balancer - named ALB1 * Application instances - named App1 * Shared services - named Shared1 * Databases - named DB1 VPC endpoints are created for private connectivity to AWS services. NAT Gateways created allow different subnets in the VPC to connect to the internet, without any direct ingress access being possible due to Route Table configurations. Network ACLs control access at each subnet layer, while VPC Flow Logs captures information about IP traffic and stores in Amazon CloudWatch Logs. Goals: VPC security features VPC layered subnet architecture Automated deployments Prerequisites: An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account with full access to CloudFormation, EC2, VPC, IAM. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Basic understanding of AWS CloudFormation , visit the Getting Started section of the user guide. Overview Lab Guide.md the guide for this lab /Code Code including CloudFormation templates related to this lab /Images referenced by this lab Permissions required IAM User with AdministratorAccess AWS managed policy License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 200: Automated Deployment of VPC"},{"location":"Security/200 - Automated Deployment of VPC/#level-200-automated-deployment-of-vpc","text":"","title":"Level 200: Automated Deployment of VPC"},{"location":"Security/200 - Automated Deployment of VPC/#introduction","text":"This hands-on lab will guide you through the steps to configure an Amazon VPC and outline some of the available security features. AWS CloudFormation will be used to automate the deployment and provide a repeatable way to re-use the template after this lab. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . The example CloudFormation template will deploy a complete VPC incorporating a number of AWS security best practices. Subnets are created in multiple availability zones for the following network tiers: * Application Load Balancer - named ALB1 * Application instances - named App1 * Shared services - named Shared1 * Databases - named DB1 VPC endpoints are created for private connectivity to AWS services. NAT Gateways created allow different subnets in the VPC to connect to the internet, without any direct ingress access being possible due to Route Table configurations. Network ACLs control access at each subnet layer, while VPC Flow Logs captures information about IP traffic and stores in Amazon CloudWatch Logs.","title":"Introduction"},{"location":"Security/200 - Automated Deployment of VPC/#goals","text":"VPC security features VPC layered subnet architecture Automated deployments","title":"Goals:"},{"location":"Security/200 - Automated Deployment of VPC/#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account with full access to CloudFormation, EC2, VPC, IAM. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Basic understanding of AWS CloudFormation , visit the Getting Started section of the user guide.","title":"Prerequisites:"},{"location":"Security/200 - Automated Deployment of VPC/#overview","text":"Lab Guide.md the guide for this lab /Code Code including CloudFormation templates related to this lab /Images referenced by this lab","title":"Overview"},{"location":"Security/200 - Automated Deployment of VPC/#permissions-required","text":"IAM User with AdministratorAccess AWS managed policy","title":"Permissions required"},{"location":"Security/200 - Automated Deployment of VPC/#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200 - Automated Deployment of VPC/Lab Guide/","text":"Level 200: Automated Deployment of VPC Authors Ben Potter, Security Lead, Well-Architected Table of Contents Create VPC Stack Tear Down 1. Create VPC Stack This step will create the VPC and all components using the example CloudFormation template. 1. Download the latest version of the CloudFormation template from /Code to your computer. 2. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . 3. Click Create Stack. 4. Click Upload a template file and then click Choose file . 5. Choose the CloudFormation template you downloaded in step 1, return to the CloudFormation console page and click Next . 5. Enter the following details: * Stack name: The name of this stack. For this lab, use WebApp1-VPC and match the case. The parameters may be left as defaults, you can find out more in the description for each. If you change the default name take note as you will need to use it for other labs including \"Automated Deployment of EC2 Web Application\". 6. At the bottom of the page click Next . 7. In this lab, we won't add any tags or other options. Click Next. Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . 8. Review the information for the stack. When you're satisfied with the configuration, check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack . 9. After a few minutes the final stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You have now created the VPC stack (well actually CloudFormation did it for you). 2. Tear down this lab The following instructions will remove the resources that you have created in this lab. Delete the VPC CloudFormation stack: 1. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . 2. Click the radio button on the left of the WebApp1-VPC stack. 3. Click the Actions button then click Delete stack . 4. Confirm the stack and then click Delete button. Delete the CloudWatch Logs: 1. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudwatch/ . 2. Click Logs in the left navigation. 3. Click the radio button on the left of the WebApp1-VPCFlowLog . 4. Click the Actions Button then click Delete Log Group . 5. Verify the log group name then click Yes, Delete . References useful resources: AWS CloudFormation User Guide Amazon VPC User Guide License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 200: Automated Deployment of VPC"},{"location":"Security/200 - Automated Deployment of VPC/Lab Guide/#level-200-automated-deployment-of-vpc","text":"","title":"Level 200: Automated Deployment of VPC"},{"location":"Security/200 - Automated Deployment of VPC/Lab Guide/#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/200 - Automated Deployment of VPC/Lab Guide/#table-of-contents","text":"Create VPC Stack Tear Down","title":"Table of Contents"},{"location":"Security/200 - Automated Deployment of VPC/Lab Guide/#1-create-vpc-stack","text":"This step will create the VPC and all components using the example CloudFormation template. 1. Download the latest version of the CloudFormation template from /Code to your computer. 2. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . 3. Click Create Stack. 4. Click Upload a template file and then click Choose file . 5. Choose the CloudFormation template you downloaded in step 1, return to the CloudFormation console page and click Next . 5. Enter the following details: * Stack name: The name of this stack. For this lab, use WebApp1-VPC and match the case. The parameters may be left as defaults, you can find out more in the description for each. If you change the default name take note as you will need to use it for other labs including \"Automated Deployment of EC2 Web Application\". 6. At the bottom of the page click Next . 7. In this lab, we won't add any tags or other options. Click Next. Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . 8. Review the information for the stack. When you're satisfied with the configuration, check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack . 9. After a few minutes the final stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You have now created the VPC stack (well actually CloudFormation did it for you).","title":"1. Create VPC Stack "},{"location":"Security/200 - Automated Deployment of VPC/Lab Guide/#2-tear-down-this-lab","text":"The following instructions will remove the resources that you have created in this lab. Delete the VPC CloudFormation stack: 1. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . 2. Click the radio button on the left of the WebApp1-VPC stack. 3. Click the Actions button then click Delete stack . 4. Confirm the stack and then click Delete button. Delete the CloudWatch Logs: 1. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudwatch/ . 2. Click Logs in the left navigation. 3. Click the radio button on the left of the WebApp1-VPCFlowLog . 4. Click the Actions Button then click Delete Log Group . 5. Verify the log group name then click Yes, Delete .","title":"2. Tear down this lab"},{"location":"Security/200 - Automated Deployment of VPC/Lab Guide/#references-useful-resources","text":"AWS CloudFormation User Guide Amazon VPC User Guide","title":"References &amp; useful resources:"},{"location":"Security/200 - Automated Deployment of VPC/Lab Guide/#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200 - Basic EC2 with WAF Protection/","text":"Level 200: EC2 Web Infrastructure Protection Introduction This hands-on lab will guide you through the introductory steps to protect an Amazon EC2 workload from network based attacks. You will use the AWS Management Console and AWS CloudFormation to guide you through how to secure an Amazon EC2 based web application with defense in depth methods. Skills learned will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals: Protecting network and host-level boundaries System security configuration and maintenance Enforcing service-level protection Prerequisites: An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab. Select region with support for AWS WAF for Application Load Balancers from list: AWS Regions and Endpoints . Overview Lab Guide.md the guide for this lab Checklist.md best practice checklist related to this lab /Code Code including CloudFormation templates related to this lab /Images referenced by this lab License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 200: EC2 Web Infrastructure Protection"},{"location":"Security/200 - Basic EC2 with WAF Protection/#level-200-ec2-web-infrastructure-protection","text":"","title":"Level 200: EC2 Web Infrastructure Protection"},{"location":"Security/200 - Basic EC2 with WAF Protection/#introduction","text":"This hands-on lab will guide you through the introductory steps to protect an Amazon EC2 workload from network based attacks. You will use the AWS Management Console and AWS CloudFormation to guide you through how to secure an Amazon EC2 based web application with defense in depth methods. Skills learned will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/200 - Basic EC2 with WAF Protection/#goals","text":"Protecting network and host-level boundaries System security configuration and maintenance Enforcing service-level protection","title":"Goals:"},{"location":"Security/200 - Basic EC2 with WAF Protection/#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab. Select region with support for AWS WAF for Application Load Balancers from list: AWS Regions and Endpoints .","title":"Prerequisites:"},{"location":"Security/200 - Basic EC2 with WAF Protection/#overview","text":"Lab Guide.md the guide for this lab Checklist.md best practice checklist related to this lab /Code Code including CloudFormation templates related to this lab /Images referenced by this lab","title":"Overview"},{"location":"Security/200 - Basic EC2 with WAF Protection/#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200 - Basic EC2 with WAF Protection/Checklist/","text":"Level 100: EC2 Web Infrastructure Protection: Best Practice Checklist Security and Network Manage access to AWS resources and APIs using identity federation, IAM users, and IAM roles. Establish credential management policies and procedures for creating, distributing, rotating, and revoking AWS access credentials. For more information, see IAM Best Practices in the IAM User Guide. Implement the least permissive rules for your security group. For more information, see Security Group Rules. Regularly patch, update, and secure the operating system and applications on your instance. For more information about updating Amazon Linux, see Managing Software on Your Linux Instance. For more information about updating your Windows instance, see Updating Your Windows Instance in the Amazon EC2 User Guide for Windows Instances. Launch your instances into a VPC instead of EC2-Classic. Note that if you created your AWS account after 2013-12-04, we automatically launch your instances into a VPC. For more information about the benefits, see Amazon EC2 and Amazon Virtual Private Cloud. Storage Understand the implications of the root device type for data persistence, backup, and recovery. For more information, see Storage for the Root Device. Use separate Amazon EBS volumes for the operating system versus your data. Ensure that the volume with your data persists after instance termination. For more information, see Preserving Amazon EBS Volumes on Instance Termination. Use the instance store available for your instance to store temporary data. Remember that the data stored in instance store is deleted when you stop or terminate your instance. If you use instance store for database storage, ensure that you have a cluster with a replication factor that ensures fault tolerance. Resource Management Use instance metadata and custom resource tags to track and identify your AWS resources. For more information, see Instance Metadata and User Data and Tagging Your Amazon EC2 Resources. View your current limits for Amazon EC2. Plan to request any limit increases in advance of the time that you'll need them. For more information, see Amazon EC2 Service Limits. Backup and Recovery Regularly back up your EBS volumes using Amazon EBS snapshots, and create an Amazon Machine Image (AMI) from your instance to save the configuration as a template for launching future instances. Deploy critical components of your application across multiple Availability Zones, and replicate your data appropriately. Design your applications to handle dynamic IP addressing when your instance restarts. For more information, see Amazon EC2 Instance IP Addressing. Monitor and respond to events. For more information, see Monitoring Amazon EC2. Ensure that you are prepared to handle failover. For a basic solution, you can manually attach a network interface or Elastic IP address to a replacement instance. For more information, see Elastic Network Interfaces. For an automated solution, you can use Amazon EC2 Auto Scaling. For more information, see the Amazon EC2 Auto Scaling User Guide. Regularly test the process of recovering your instances and Amazon EBS volumes if they fail.","title":"Level 100: EC2 Web Infrastructure Protection: Best Practice Checklist"},{"location":"Security/200 - Basic EC2 with WAF Protection/Checklist/#level-100-ec2-web-infrastructure-protection-best-practice-checklist","text":"","title":"Level 100: EC2 Web Infrastructure Protection: Best Practice Checklist"},{"location":"Security/200 - Basic EC2 with WAF Protection/Checklist/#security-and-network","text":"Manage access to AWS resources and APIs using identity federation, IAM users, and IAM roles. Establish credential management policies and procedures for creating, distributing, rotating, and revoking AWS access credentials. For more information, see IAM Best Practices in the IAM User Guide. Implement the least permissive rules for your security group. For more information, see Security Group Rules. Regularly patch, update, and secure the operating system and applications on your instance. For more information about updating Amazon Linux, see Managing Software on Your Linux Instance. For more information about updating your Windows instance, see Updating Your Windows Instance in the Amazon EC2 User Guide for Windows Instances. Launch your instances into a VPC instead of EC2-Classic. Note that if you created your AWS account after 2013-12-04, we automatically launch your instances into a VPC. For more information about the benefits, see Amazon EC2 and Amazon Virtual Private Cloud.","title":"Security and Network"},{"location":"Security/200 - Basic EC2 with WAF Protection/Checklist/#storage","text":"Understand the implications of the root device type for data persistence, backup, and recovery. For more information, see Storage for the Root Device. Use separate Amazon EBS volumes for the operating system versus your data. Ensure that the volume with your data persists after instance termination. For more information, see Preserving Amazon EBS Volumes on Instance Termination. Use the instance store available for your instance to store temporary data. Remember that the data stored in instance store is deleted when you stop or terminate your instance. If you use instance store for database storage, ensure that you have a cluster with a replication factor that ensures fault tolerance.","title":"Storage"},{"location":"Security/200 - Basic EC2 with WAF Protection/Checklist/#resource-management","text":"Use instance metadata and custom resource tags to track and identify your AWS resources. For more information, see Instance Metadata and User Data and Tagging Your Amazon EC2 Resources. View your current limits for Amazon EC2. Plan to request any limit increases in advance of the time that you'll need them. For more information, see Amazon EC2 Service Limits.","title":"Resource Management"},{"location":"Security/200 - Basic EC2 with WAF Protection/Checklist/#backup-and-recovery","text":"Regularly back up your EBS volumes using Amazon EBS snapshots, and create an Amazon Machine Image (AMI) from your instance to save the configuration as a template for launching future instances. Deploy critical components of your application across multiple Availability Zones, and replicate your data appropriately. Design your applications to handle dynamic IP addressing when your instance restarts. For more information, see Amazon EC2 Instance IP Addressing. Monitor and respond to events. For more information, see Monitoring Amazon EC2. Ensure that you are prepared to handle failover. For a basic solution, you can manually attach a network interface or Elastic IP address to a replacement instance. For more information, see Elastic Network Interfaces. For an automated solution, you can use Amazon EC2 Auto Scaling. For more information, see the Amazon EC2 Auto Scaling User Guide. Regularly test the process of recovering your instances and Amazon EBS volumes if they fail.","title":"Backup and Recovery"},{"location":"Security/200 - Basic EC2 with WAF Protection/Lab Guide/","text":"Level 200: EC2 Web Infrastructure Protection: Lab Guide 1. Launch Instance For launching your first instance, we are going to use the launch wizard in the Amazon EC2 console. 1.1 Launch Single Linux Instance You can launch a Linux instance using the AWS Management Console. This tutorial is intended to help you launch your first instance quickly, so it doesn't cover all possible options. For more information about the advanced options, see Launching an Instance . Launch an instance: 1. Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. 2. From the console dashboard, choose Launch Instance. 3. The choose an Amazon Machine Image (AMI) page displays a list of basic configurations, called Amazon Machine Images (AMIs), that serve as templates for your instance. Select the HVM edition of the Amazon Linux AMI (not Amazon Linux 2). 4. On the Choose an Instance Type page, you can select the hardware configuration of your instance. Select the t2.micro type, which is selected by default. Notice that this instance type is eligible for the free tier. Then select Next: Configure Instance Details. 5. On the Configure Instance Details page, make the following changes: 5.1 Select Create new IAM role. 5.2 In the new tab that opens, select Create role. 5.3 With AWS service pre-selected, select EC2 from the top of the list, then click Next: Permissions. 5.4 Enter s3 in the search and select AmazonS3ReadOnlyAccess from the list of policies, then click Next: Review. This policy will give this EC2 instance access to read and list any objects in Amazon S3 within your AWS account. 5.5 Enter a role name, such as ec2-s3-read-only-role , and then click Create role. 5.6 Back on the EC2 launch web browser tab, select the refresh button next to Create new IAM role, and click the role you just created. 5.7 Scroll down and expand the Advanced Details section. Enter the following in the User Data test box to automatically install Apache web server and apply basic configuration when the instance is launched: #!/bin/bash yum update -y yum install -y httpd24 service httpd start chkconfig httpd on groupadd www usermod -a -G www ec2-user chown -R root:www /var/www chmod 2775 /var/www find /var/www -type d -exec chmod 2775 {} + find /var/www -type f -exec chmod 0664 {} + 6. Accept defaults and Choose Next: Add tags. 7. Click Next: Configure Security Group. 7.1 On type SSH, select Source as My IP 7.2 Click Add Rule, select Type as HTTP and source as Anywhere Note that best practice is to have an Elastic Load Balancer inline or the EC2 instance not directly exposed. However, for simplicity in this lab, we are opening the access to anywhere. Later modules will secure access with Elastic Load Balancer. 7.2 Select Add Rule to add both SSH and HTTP, and on source, select My IP . 7.3 Click Review and Launch. 8. On the Review Instance Launch page, check the details, and then click Launch. 9. If you do not have an existing key pair for access instances, a prompt will appear. Click Create New, then type a name such as lab , click Download Key Pair, and then click Launch Instances. Important This is the only chance to save the private key file. You'll need to provide the name of your key pair when you launch an instance, and you'll provide the corresponding private key each time you connect to the instance. Click View Instances. When your instance is launched, its status will change to running, and it will need a few minutes to apply patches and install Apache web server. You can connect to the Apache test page by entering the public DNS, which you can find on the description tab or instances list. Take note of this public DNS value. 2. Create AWS WAF Rules 2.1 AWS CloudFormation to create AWS WAF ACL for Application Load Balancer Using AWS CloudFormation , we are going to deploy a basic example AWS WAF configuration for use with Application Load Balancer. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Click Create New Stack. Select Specify an Amazon S3 template URL and enter the following URL for the template: https://s3-us-west-2.amazonaws.com/aws-well-architected-labs/Security/Code/waf-regional.yaml and click Next. Enter the following details: Stack name: The name of this stack. For this lab, use lab-waf-regional . WAFName: Enter the base name to be used for resource and export names for this stack. For this lab, you can use WAFLabReg . WAFCloudWatchPrefix: Enter the name of the CloudWatch prefix to use for each rule using alphanumeric characters only. For this lab, you can use WAFLabReg . The remainder of the parameters can be left as defaults. Click Next. In this scenario, we won't add any tags or other options. Click Next. Review the information for the stack. When you're satisfied with the settings, click Create. After a few minutes, the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE. You have now set up a basic AWS WAF configuration ready for Application Load Balancer to use! 3. Create Application Load Balancer with WAF integration Using the AWS Management Console, we will create an Application Load Balancer, link it to the AWS WAF ACL we previously created and test. 3.1 Create Application Load Balancer Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. From the console dashboard, choose Load Balancers from the Load Balancing section. Click Create Load Balancer. Click Create under the Application Load Balancer section. Enter Name for Application Load Balancer such as lab-alb . Select all availability zones in your region then click Next. You will need to click Next again to accept your load balancer is using insecure listener. Click Create a new security group and enter name and description such as lab-alb and accept default of open to internet. Accept defaults and enter Name such as lab-alb and click Next. From the list of instances click the check box and then Add to registered button. Then click Next. Review the details and click Create. A successfull message should appear, click Close. Take not of the DNS name under the Description tab, you will need this for testing. 3.2 Configure Application Load Balancer with WAF Open the AWS WAF console at https://console.aws.amazon.com/waf/. In the navigation pane, choose Web ACLs. Choose the web ACL that you want to associate with the Application Load Balancer. On the Rules tab, under AWS resources using this web ACL, choose Add association. When prompted, use the Resource list to choose the Application Load Balancer that you want to associate this web ACL such as lab-alb and click Add. The Application Load Balancer should now appear under resources using. You can now test access by entering the DNS name of your load balancer in a web browser. 4. Tear down this lab The following instructions will remove the resources that have a cost for running them. Please note that Security Groups and SSH key will exist. You may remove these also or leave for future use. Terminate the instance: 1. Sign in to the AWS Management Console, and open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. 2. From the left console instance menu, select Instances. 3. Select the instance you created to terminate. 4. From the Actions button (or right click) select Instance State Terminate. 5. Verify this is the instance you want terminated, then click the Yes, Terminate button. Delete the Application Load Balancer: 1. Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. 2. From the console dashboard, choose Load Balancers from the Load Balancing section. 3. Choose the load balancer you created previously such as lab-alb and click Actions, then Delete. 4. Confirm by clicking Yes, Delete. 5. From the console dashboard, choose Target Groups from the Load Balancing section. 3. Choose the target group you created previously such as lab-alb and click Actions, then Delete. Delete the AWS WAF stack: 1. Open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. 2. Select the lab-waf-regional stack. 3. Click the Actions button, and then click Delete Stack. 4. Confirm the stack, and then click the Yes, Delete button. References useful resources: Amazon Elastic Compute Cloud User Guide for Linux Instances Tutorial: Configure Apache Web Server on Amazon Linux 2 to Use SSL/TLS AWS WAF, AWS Firewall Manager, and AWS Shield Advanced Developer Guide License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 200: EC2 Web Infrastructure Protection: Lab Guide"},{"location":"Security/200 - Basic EC2 with WAF Protection/Lab Guide/#level-200-ec2-web-infrastructure-protection-lab-guide","text":"","title":"Level 200: EC2 Web Infrastructure Protection: Lab Guide"},{"location":"Security/200 - Basic EC2 with WAF Protection/Lab Guide/#1-launch-instance","text":"For launching your first instance, we are going to use the launch wizard in the Amazon EC2 console.","title":"1. Launch Instance"},{"location":"Security/200 - Basic EC2 with WAF Protection/Lab Guide/#11-launch-single-linux-instance","text":"You can launch a Linux instance using the AWS Management Console. This tutorial is intended to help you launch your first instance quickly, so it doesn't cover all possible options. For more information about the advanced options, see Launching an Instance . Launch an instance: 1. Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. 2. From the console dashboard, choose Launch Instance. 3. The choose an Amazon Machine Image (AMI) page displays a list of basic configurations, called Amazon Machine Images (AMIs), that serve as templates for your instance. Select the HVM edition of the Amazon Linux AMI (not Amazon Linux 2). 4. On the Choose an Instance Type page, you can select the hardware configuration of your instance. Select the t2.micro type, which is selected by default. Notice that this instance type is eligible for the free tier. Then select Next: Configure Instance Details. 5. On the Configure Instance Details page, make the following changes: 5.1 Select Create new IAM role. 5.2 In the new tab that opens, select Create role. 5.3 With AWS service pre-selected, select EC2 from the top of the list, then click Next: Permissions. 5.4 Enter s3 in the search and select AmazonS3ReadOnlyAccess from the list of policies, then click Next: Review. This policy will give this EC2 instance access to read and list any objects in Amazon S3 within your AWS account. 5.5 Enter a role name, such as ec2-s3-read-only-role , and then click Create role. 5.6 Back on the EC2 launch web browser tab, select the refresh button next to Create new IAM role, and click the role you just created. 5.7 Scroll down and expand the Advanced Details section. Enter the following in the User Data test box to automatically install Apache web server and apply basic configuration when the instance is launched: #!/bin/bash yum update -y yum install -y httpd24 service httpd start chkconfig httpd on groupadd www usermod -a -G www ec2-user chown -R root:www /var/www chmod 2775 /var/www find /var/www -type d -exec chmod 2775 {} + find /var/www -type f -exec chmod 0664 {} + 6. Accept defaults and Choose Next: Add tags. 7. Click Next: Configure Security Group. 7.1 On type SSH, select Source as My IP 7.2 Click Add Rule, select Type as HTTP and source as Anywhere Note that best practice is to have an Elastic Load Balancer inline or the EC2 instance not directly exposed. However, for simplicity in this lab, we are opening the access to anywhere. Later modules will secure access with Elastic Load Balancer. 7.2 Select Add Rule to add both SSH and HTTP, and on source, select My IP . 7.3 Click Review and Launch. 8. On the Review Instance Launch page, check the details, and then click Launch. 9. If you do not have an existing key pair for access instances, a prompt will appear. Click Create New, then type a name such as lab , click Download Key Pair, and then click Launch Instances. Important This is the only chance to save the private key file. You'll need to provide the name of your key pair when you launch an instance, and you'll provide the corresponding private key each time you connect to the instance. Click View Instances. When your instance is launched, its status will change to running, and it will need a few minutes to apply patches and install Apache web server. You can connect to the Apache test page by entering the public DNS, which you can find on the description tab or instances list. Take note of this public DNS value.","title":"1.1 Launch Single Linux Instance"},{"location":"Security/200 - Basic EC2 with WAF Protection/Lab Guide/#2-create-aws-waf-rules","text":"","title":"2. Create AWS WAF Rules"},{"location":"Security/200 - Basic EC2 with WAF Protection/Lab Guide/#21-aws-cloudformation-to-create-aws-waf-acl-for-application-load-balancer","text":"Using AWS CloudFormation , we are going to deploy a basic example AWS WAF configuration for use with Application Load Balancer. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Click Create New Stack. Select Specify an Amazon S3 template URL and enter the following URL for the template: https://s3-us-west-2.amazonaws.com/aws-well-architected-labs/Security/Code/waf-regional.yaml and click Next. Enter the following details: Stack name: The name of this stack. For this lab, use lab-waf-regional . WAFName: Enter the base name to be used for resource and export names for this stack. For this lab, you can use WAFLabReg . WAFCloudWatchPrefix: Enter the name of the CloudWatch prefix to use for each rule using alphanumeric characters only. For this lab, you can use WAFLabReg . The remainder of the parameters can be left as defaults. Click Next. In this scenario, we won't add any tags or other options. Click Next. Review the information for the stack. When you're satisfied with the settings, click Create. After a few minutes, the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE. You have now set up a basic AWS WAF configuration ready for Application Load Balancer to use!","title":"2.1 AWS CloudFormation to create AWS WAF ACL for Application Load Balancer"},{"location":"Security/200 - Basic EC2 with WAF Protection/Lab Guide/#3-create-application-load-balancer-with-waf-integration","text":"Using the AWS Management Console, we will create an Application Load Balancer, link it to the AWS WAF ACL we previously created and test.","title":"3. Create Application Load Balancer with WAF integration"},{"location":"Security/200 - Basic EC2 with WAF Protection/Lab Guide/#31-create-application-load-balancer","text":"Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. From the console dashboard, choose Load Balancers from the Load Balancing section. Click Create Load Balancer. Click Create under the Application Load Balancer section. Enter Name for Application Load Balancer such as lab-alb . Select all availability zones in your region then click Next. You will need to click Next again to accept your load balancer is using insecure listener. Click Create a new security group and enter name and description such as lab-alb and accept default of open to internet. Accept defaults and enter Name such as lab-alb and click Next. From the list of instances click the check box and then Add to registered button. Then click Next. Review the details and click Create. A successfull message should appear, click Close. Take not of the DNS name under the Description tab, you will need this for testing.","title":"3.1 Create Application Load Balancer"},{"location":"Security/200 - Basic EC2 with WAF Protection/Lab Guide/#32-configure-application-load-balancer-with-waf","text":"Open the AWS WAF console at https://console.aws.amazon.com/waf/. In the navigation pane, choose Web ACLs. Choose the web ACL that you want to associate with the Application Load Balancer. On the Rules tab, under AWS resources using this web ACL, choose Add association. When prompted, use the Resource list to choose the Application Load Balancer that you want to associate this web ACL such as lab-alb and click Add. The Application Load Balancer should now appear under resources using. You can now test access by entering the DNS name of your load balancer in a web browser.","title":"3.2 Configure Application Load Balancer with WAF"},{"location":"Security/200 - Basic EC2 with WAF Protection/Lab Guide/#4-tear-down-this-lab","text":"The following instructions will remove the resources that have a cost for running them. Please note that Security Groups and SSH key will exist. You may remove these also or leave for future use. Terminate the instance: 1. Sign in to the AWS Management Console, and open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. 2. From the left console instance menu, select Instances. 3. Select the instance you created to terminate. 4. From the Actions button (or right click) select Instance State Terminate. 5. Verify this is the instance you want terminated, then click the Yes, Terminate button. Delete the Application Load Balancer: 1. Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. 2. From the console dashboard, choose Load Balancers from the Load Balancing section. 3. Choose the load balancer you created previously such as lab-alb and click Actions, then Delete. 4. Confirm by clicking Yes, Delete. 5. From the console dashboard, choose Target Groups from the Load Balancing section. 3. Choose the target group you created previously such as lab-alb and click Actions, then Delete. Delete the AWS WAF stack: 1. Open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. 2. Select the lab-waf-regional stack. 3. Click the Actions button, and then click Delete Stack. 4. Confirm the stack, and then click the Yes, Delete button.","title":"4. Tear down this lab"},{"location":"Security/200 - Basic EC2 with WAF Protection/Lab Guide/#references-useful-resources","text":"Amazon Elastic Compute Cloud User Guide for Linux Instances Tutorial: Configure Apache Web Server on Amazon Linux 2 to Use SSL/TLS AWS WAF, AWS Firewall Manager, and AWS Shield Advanced Developer Guide","title":"References &amp; useful resources:"},{"location":"Security/200 - Basic EC2 with WAF Protection/Lab Guide/#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200 - CloudFront with WAF Protection/","text":"Level 200: CloudFront with WAF Protection Introduction This hands-on lab will guide you through the steps to protect a workload from network based attacks using Amazon CloudFront and AWS Web Application Firewall (WAF). You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy CloudFront with WAF integration to apply defense in depth methods. Skills learned will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals: Protecting network and host-level boundaries System security configuration and maintenance Enforcing service-level protection Prerequisites: An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab. Overview Lab Guide.md the guide for this lab /Code Code including CloudFormation templates related to this lab /Images referenced by this lab License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 200: CloudFront with WAF Protection"},{"location":"Security/200 - CloudFront with WAF Protection/#level-200-cloudfront-with-waf-protection","text":"","title":"Level 200: CloudFront with WAF Protection"},{"location":"Security/200 - CloudFront with WAF Protection/#introduction","text":"This hands-on lab will guide you through the steps to protect a workload from network based attacks using Amazon CloudFront and AWS Web Application Firewall (WAF). You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy CloudFront with WAF integration to apply defense in depth methods. Skills learned will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/200 - CloudFront with WAF Protection/#goals","text":"Protecting network and host-level boundaries System security configuration and maintenance Enforcing service-level protection","title":"Goals:"},{"location":"Security/200 - CloudFront with WAF Protection/#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab.","title":"Prerequisites:"},{"location":"Security/200 - CloudFront with WAF Protection/#overview","text":"Lab Guide.md the guide for this lab /Code Code including CloudFormation templates related to this lab /Images referenced by this lab","title":"Overview"},{"location":"Security/200 - CloudFront with WAF Protection/#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200 - CloudFront with WAF Protection/Lab Guide/","text":"Level 100: CloudFront with WAF Protection: Lab Guide Authors Ben Potter, Security Lead, Well-Architected Table of Contents Launch Instance Configure WAF Configure CloudFront Tear Down 1. Launch Instance You can launch a Linux instance using the AWS Management Console. This tutorial is intended to help you launch your first instance quickly, so it doesn't cover all possible options. For more information about the advanced options, see Launching an Instance . Launch an instance: 1. Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. 2. From the console dashboard, choose Launch Instance. 3. The choose an Amazon Machine Image (AMI) page displays a list of basic configurations, called Amazon Machine Images (AMIs), that serve as templates for your instance. Select the HVM edition of the Amazon Linux AMI (not Amazon Linux Version 2). 4. On the Choose an Instance Type page, you can select the hardware configuration of your instance. Select the t2.micro type, which is selected by default. Notice that this instance type is eligible for the free tier. Then select Next: Configure Instance Details. 5. On the Configure Instance Details page, make the following changes: 5.1 Select Create new IAM role. 5.2 In the new tab that opens, select Create role. 5.3 With AWS service pre-selected, select EC2 from the top of the list, then click Next: Permissions. ![ec2-launch-wizard](Images/ec2-launch-wizard-create-role-start.png) 5.4 Enter s3 in the search and select AmazonS3ReadOnlyAccess from the list of policies, then click Next: Review. This policy will give this EC2 instance access to read and list any objects in Amazon S3 within your AWS account. 5.5 Enter a role name, such as ec2-s3-read-only-role , and then click Create role. 5.6 Back on the EC2 launch web browser tab, select the refresh button next to Create new IAM role, and click the role you just created. 5.7 Scroll down and expand the Advanced Details section. Enter the following in the User Data test box to automatically install Apache web server and apply basic configuration when the instance is launched: #!/bin/bash yum update -y yum install -y httpd24 service httpd start chkconfig httpd on groupadd www usermod -a -G www ec2-user chown -R root:www /var/www chmod 2775 /var/www find /var/www -type d -exec chmod 2775 {} + find /var/www -type f -exec chmod 0664 {} + 6. Accept defaults and Choose Next: Add tags. 7. Click Next: Configure Security Group. 7.1 On type SSH, select Source as My IP 7.2 Click Add Rule, select Type as HTTP and source as Anywhere Note that best practice is to have an Elastic Load Balancer inline or the EC2 instance not directly exposed. However, for simplicity in this lab, we are opening the access to anywhere. Later modules will secure access with Elastic Load Balancer. 7.2 Select Add Rule to add both SSH and HTTP, and on source, select My IP . 7.3 Click Review and Launch. 8. On the Review Instance Launch page, check the details, and then click Launch. 9. If you do not have an existing key pair for access instances, a prompt will appear. Click Create New, then type a name such as lab , click Download Key Pair, and then click Launch Instances. Important This is the only chance to save the private key file. You'll need to provide the name of your key pair when you launch an instance, and you'll provide the corresponding private key each time you connect to the instance. Click View Instances. When your instance is launched, its status will change to running, and it will need a few minutes to apply patches and install Apache web server. You can connect to the Apache test page by entering the public DNS, which you can find on the description tab or instances list. Take note of this public DNS value. 2. Configure AWS WAF Using AWS CloudFormation , we are going to deploy a basic example AWS WAF configuration for use with CloudFront. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Click Create stack . Enter the following Amazon S3 URL : https://s3-us-west-2.amazonaws.com/aws-well-architected-labs/Security/Code/waf-global.yaml and click Next . Enter the following details: Stack name: The name of this stack. For this lab, use waf . WAFName: Enter the base name to be used for resource and export names for this stack. For this lab, you can use Lab1 . WAFCloudWatchPrefix: Enter the name of the CloudWatch prefix to use for each rule using alphanumeric characters only. For this lab, you can use Lab1 . The remainder of the parameters can be left as defaults. At the bottom of the page click Next . In this lab, we won't add any tags or other options. Click Next. Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you're satisfied with the configuration, click Create stack . After a few minutes the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You have now set up a basic AWS WAF configuration ready for CloudFront to use! 3. Configure Amazon CloudFront Using the AWS Management Console, we will create a CloudFront distribution, and link it to the AWS WAF ACL we previously created. 1. Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. 2. From the console dashboard, choose Create Distribution. 3. Click Get Started in the Web section. 4. Specify the following settings for the distribution: * In Origin Domain Name enter the EC2 public DNS name you recorded from your instance launch. * In the distribution Settings section, click AWS WAF Web ACL, and select the one you created previously. * Click Create Distrubution. * For more information on the other configuration options, see Values That You Specify When You Create or Update a Web Distribution in the CloudFront documentation. 5. After CloudFront creates your distribution, the value of the Status column for your distribution will change from In Progress to Deployed. 6. When your distribution is deployed, confirm that you can access your content using your new CloudFront URL or CNAME. Copy the Domain Name into a web browser to test. For more information, see Testing a Web Distribution in the CloudFront documentation. 7. You have now configured Amazon CloudFront with basic settings and AWS WAF. For more information on configuring CloudFront, see Viewing and Updating CloudFront Distributions in the CloudFront documentation. 3. Tear down this lab The following instructions will remove the resources that have a cost for running them. Please note that Security Groups and SSH key will exist. You may remove these also or leave for future use. Delete the CloudFront distribution: 1. Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. 2. From the console dashboard, select the distribution you created earlier and click the Disable button. To confirm, click the Yes, Disable button. 3. After approximately 15 minutes when the status is Deployed, select the distribution and click the Delete button, and then to confirm click the Yes, Delete button. Delete the AWS WAF stack: 1. Sign in to the AWS Management Console, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. 2. Select the waf-cloudfront stack. 3. Click the Actions button, and then click Delete Stack. 4. Confirm the stack, and then click the Yes, Delete button. References useful resources: Amazon Elastic Compute Cloud User Guide for Linux Instances Amazon CloudFront Developer Guide Tutorial: Configure Apache Web Server on Amazon Linux 2 to Use SSL/TLS AWS WAF, AWS Firewall Manager, and AWS Shield Advanced Developer Guide License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 100: CloudFront with WAF Protection: Lab Guide"},{"location":"Security/200 - CloudFront with WAF Protection/Lab Guide/#level-100-cloudfront-with-waf-protection-lab-guide","text":"","title":"Level 100: CloudFront with WAF Protection: Lab Guide"},{"location":"Security/200 - CloudFront with WAF Protection/Lab Guide/#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/200 - CloudFront with WAF Protection/Lab Guide/#table-of-contents","text":"Launch Instance Configure WAF Configure CloudFront Tear Down","title":"Table of Contents"},{"location":"Security/200 - CloudFront with WAF Protection/Lab Guide/#1-launch-instance","text":"You can launch a Linux instance using the AWS Management Console. This tutorial is intended to help you launch your first instance quickly, so it doesn't cover all possible options. For more information about the advanced options, see Launching an Instance . Launch an instance: 1. Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. 2. From the console dashboard, choose Launch Instance. 3. The choose an Amazon Machine Image (AMI) page displays a list of basic configurations, called Amazon Machine Images (AMIs), that serve as templates for your instance. Select the HVM edition of the Amazon Linux AMI (not Amazon Linux Version 2). 4. On the Choose an Instance Type page, you can select the hardware configuration of your instance. Select the t2.micro type, which is selected by default. Notice that this instance type is eligible for the free tier. Then select Next: Configure Instance Details. 5. On the Configure Instance Details page, make the following changes: 5.1 Select Create new IAM role. 5.2 In the new tab that opens, select Create role. 5.3 With AWS service pre-selected, select EC2 from the top of the list, then click Next: Permissions. ![ec2-launch-wizard](Images/ec2-launch-wizard-create-role-start.png) 5.4 Enter s3 in the search and select AmazonS3ReadOnlyAccess from the list of policies, then click Next: Review. This policy will give this EC2 instance access to read and list any objects in Amazon S3 within your AWS account. 5.5 Enter a role name, such as ec2-s3-read-only-role , and then click Create role. 5.6 Back on the EC2 launch web browser tab, select the refresh button next to Create new IAM role, and click the role you just created. 5.7 Scroll down and expand the Advanced Details section. Enter the following in the User Data test box to automatically install Apache web server and apply basic configuration when the instance is launched: #!/bin/bash yum update -y yum install -y httpd24 service httpd start chkconfig httpd on groupadd www usermod -a -G www ec2-user chown -R root:www /var/www chmod 2775 /var/www find /var/www -type d -exec chmod 2775 {} + find /var/www -type f -exec chmod 0664 {} + 6. Accept defaults and Choose Next: Add tags. 7. Click Next: Configure Security Group. 7.1 On type SSH, select Source as My IP 7.2 Click Add Rule, select Type as HTTP and source as Anywhere Note that best practice is to have an Elastic Load Balancer inline or the EC2 instance not directly exposed. However, for simplicity in this lab, we are opening the access to anywhere. Later modules will secure access with Elastic Load Balancer. 7.2 Select Add Rule to add both SSH and HTTP, and on source, select My IP . 7.3 Click Review and Launch. 8. On the Review Instance Launch page, check the details, and then click Launch. 9. If you do not have an existing key pair for access instances, a prompt will appear. Click Create New, then type a name such as lab , click Download Key Pair, and then click Launch Instances. Important This is the only chance to save the private key file. You'll need to provide the name of your key pair when you launch an instance, and you'll provide the corresponding private key each time you connect to the instance. Click View Instances. When your instance is launched, its status will change to running, and it will need a few minutes to apply patches and install Apache web server. You can connect to the Apache test page by entering the public DNS, which you can find on the description tab or instances list. Take note of this public DNS value.","title":"1. Launch Instance "},{"location":"Security/200 - CloudFront with WAF Protection/Lab Guide/#2-configure-aws-waf","text":"Using AWS CloudFormation , we are going to deploy a basic example AWS WAF configuration for use with CloudFront. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Click Create stack . Enter the following Amazon S3 URL : https://s3-us-west-2.amazonaws.com/aws-well-architected-labs/Security/Code/waf-global.yaml and click Next . Enter the following details: Stack name: The name of this stack. For this lab, use waf . WAFName: Enter the base name to be used for resource and export names for this stack. For this lab, you can use Lab1 . WAFCloudWatchPrefix: Enter the name of the CloudWatch prefix to use for each rule using alphanumeric characters only. For this lab, you can use Lab1 . The remainder of the parameters can be left as defaults. At the bottom of the page click Next . In this lab, we won't add any tags or other options. Click Next. Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you're satisfied with the configuration, click Create stack . After a few minutes the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You have now set up a basic AWS WAF configuration ready for CloudFront to use!","title":"2. Configure AWS WAF "},{"location":"Security/200 - CloudFront with WAF Protection/Lab Guide/#3-configure-amazon-cloudfront","text":"Using the AWS Management Console, we will create a CloudFront distribution, and link it to the AWS WAF ACL we previously created. 1. Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. 2. From the console dashboard, choose Create Distribution. 3. Click Get Started in the Web section. 4. Specify the following settings for the distribution: * In Origin Domain Name enter the EC2 public DNS name you recorded from your instance launch. * In the distribution Settings section, click AWS WAF Web ACL, and select the one you created previously. * Click Create Distrubution. * For more information on the other configuration options, see Values That You Specify When You Create or Update a Web Distribution in the CloudFront documentation. 5. After CloudFront creates your distribution, the value of the Status column for your distribution will change from In Progress to Deployed. 6. When your distribution is deployed, confirm that you can access your content using your new CloudFront URL or CNAME. Copy the Domain Name into a web browser to test. For more information, see Testing a Web Distribution in the CloudFront documentation. 7. You have now configured Amazon CloudFront with basic settings and AWS WAF. For more information on configuring CloudFront, see Viewing and Updating CloudFront Distributions in the CloudFront documentation.","title":"3. Configure Amazon CloudFront "},{"location":"Security/200 - CloudFront with WAF Protection/Lab Guide/#3-tear-down-this-lab","text":"The following instructions will remove the resources that have a cost for running them. Please note that Security Groups and SSH key will exist. You may remove these also or leave for future use. Delete the CloudFront distribution: 1. Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. 2. From the console dashboard, select the distribution you created earlier and click the Disable button. To confirm, click the Yes, Disable button. 3. After approximately 15 minutes when the status is Deployed, select the distribution and click the Delete button, and then to confirm click the Yes, Delete button. Delete the AWS WAF stack: 1. Sign in to the AWS Management Console, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. 2. Select the waf-cloudfront stack. 3. Click the Actions button, and then click Delete Stack. 4. Confirm the stack, and then click the Yes, Delete button.","title":"3. Tear down this lab "},{"location":"Security/200 - CloudFront with WAF Protection/Lab Guide/#references-useful-resources","text":"Amazon Elastic Compute Cloud User Guide for Linux Instances Amazon CloudFront Developer Guide Tutorial: Configure Apache Web Server on Amazon Linux 2 to Use SSL/TLS AWS WAF, AWS Firewall Manager, and AWS Shield Advanced Developer Guide","title":"References &amp; useful resources:"},{"location":"Security/200 - CloudFront with WAF Protection/Lab Guide/#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/300 - IAM Permission Boundaries Delegating Role Creation/","text":"Level 300: IAM Permission Boundaries Delegating Role Creation Introduction This hands-on lab will guide you through the steps to configure an example AWS Identity and Access Management (IAM) permission boundary. AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature in which you use a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. When you set a permissions boundary for an entity, the entity can perform only the actions that are allowed by the policy. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as a developer, to create user roles that are restricted to specific services and regions - only if they are created with the permission boundary. This allows you to delegate access to create IAM roles and policies, without them exceeding the permissions in the permission boundary. We will also use a naming standard with a prefix, making it easier to control and organize policies and roles that your developers create. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals: IAM permission boundaries IAM policy conditions Prerequisites: An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user with MFA enabled that can assume roles in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Overview Lab Guide.md the guide for this lab /Images referenced by this lab Permissions required IAM User with AdministratorAccess AWS managed policy License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 300: IAM Permission Boundaries Delegating Role Creation"},{"location":"Security/300 - IAM Permission Boundaries Delegating Role Creation/#level-300-iam-permission-boundaries-delegating-role-creation","text":"","title":"Level 300: IAM Permission Boundaries Delegating Role Creation"},{"location":"Security/300 - IAM Permission Boundaries Delegating Role Creation/#introduction","text":"This hands-on lab will guide you through the steps to configure an example AWS Identity and Access Management (IAM) permission boundary. AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature in which you use a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. When you set a permissions boundary for an entity, the entity can perform only the actions that are allowed by the policy. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as a developer, to create user roles that are restricted to specific services and regions - only if they are created with the permission boundary. This allows you to delegate access to create IAM roles and policies, without them exceeding the permissions in the permission boundary. We will also use a naming standard with a prefix, making it easier to control and organize policies and roles that your developers create. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/300 - IAM Permission Boundaries Delegating Role Creation/#goals","text":"IAM permission boundaries IAM policy conditions","title":"Goals:"},{"location":"Security/300 - IAM Permission Boundaries Delegating Role Creation/#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user with MFA enabled that can assume roles in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites:"},{"location":"Security/300 - IAM Permission Boundaries Delegating Role Creation/#overview","text":"Lab Guide.md the guide for this lab /Images referenced by this lab","title":"Overview"},{"location":"Security/300 - IAM Permission Boundaries Delegating Role Creation/#permissions-required","text":"IAM User with AdministratorAccess AWS managed policy","title":"Permissions required"},{"location":"Security/300 - IAM Permission Boundaries Delegating Role Creation/#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/300 - IAM Permission Boundaries Delegating Role Creation/Lab Guide/","text":"Level 300: IAM Permission Boundaries Delegating Role Creation Authors Ben Potter, Security Lead, Well-Architected Table of Contents Create IAM Policies Create and Test Developer Role Create and Test Region Restricted User Role Tear Down 1. Create IAM policies 1.1 Create policy for permission boundary This policy will be used for the permission boundary when the developer role creates their own user role with their delegated permissions. In this lab we are only going to allow regions us-east-1 (North Virginia) and us-west-1 (North California), optionally you can change these to your favourite regions and add / remove as many as you need. The only service actions we are going to allow are ec2 and lambda, note that these services require additional supporting actions if you were to re-use this policy after this lab, depending on your requirements. 1. Sign in to the AWS Management Console as an IAM user with MFA enabled that can assume roles in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . 2. In the navigation pane, click Policies and then click Create policy . 3. On the Create policy page click the JSON tab. 4. Replace the example start of the policy that is already in the editor with the policy below. { Version : 2012-10-17 , Statement : [ { Sid : EC2RestrictRegion , Effect : Allow , Action : ec2:* , Resource : * , Condition : { StringEquals : { aws:RequestedRegion : [ us-east-1 , us-west-1 ] } } }, { Sid : LambdaRestrictRegion , Effect : Allow , Action : lambda:* , Resource : * , Condition : { StringEquals : { aws:RequestedRegion : [ us-east-1 , us-west-1 ] } } } ] } Click Review policy . Enter the name of restrict-region-boundary and any description to help you identify the policy, verify the summary and then click Create policy . 1.2 Create developer IAM restricted policy This policy will be attached to the developer role, and will allow the developer to create policies and roles with a name prefix of app1 , and only if the permission boundary restrict-region-boundary is attached. You will need to change the account id placeholders of 123456789012 to your account number in 5 places. You can find your account id by navigating to https://console.aws.amazon.com/billing/home?#/account in the console. Naming prefixes are useful when you have different teams or in this case different applications running in the same AWS account. They can be used to keep your resources looking tidy, and also in IAM policy as the resource as we are doing here. 1. Create a managed policy using the JSON policy below and name of createrole-restrict-region-boundary . { Version : 2012-10-17 , Statement : [ { Sid : CreatePolicy , Effect : Allow , Action : [ iam:CreatePolicy , iam:CreatePolicyVersion , iam:DeletePolicyVersion ], Resource : arn:aws:iam::123456789012:policy/app1* }, { Sid : CreateRole , Effect : Allow , Action : [ iam:CreateRole ], Resource : arn:aws:iam::123456789012:role/app1* , Condition : { StringEquals : { iam:PermissionsBoundary : arn:aws:iam::123456789012:policy/restrict-region-boundary } } }, { Sid : AttachDetachRolePolicy , Effect : Allow , Action : [ iam:DetachRolePolicy , iam:AttachRolePolicy ], Resource : arn:aws:iam::123456789012:role/app1* , Condition : { ArnEquals : { iam:PolicyARN : [ arn:aws:iam::123456789012:policy/* , arn:aws:iam::aws:policy/* ] } } } ] } 1.3 Create developer IAM console access role This policy allows list and read type IAM service actions so you can see what you have created using the console. Note that it is not a requirement if you simply wanted to create the role and policy, or if you were using the Command Line Interface (CLI) or CloudFormation. 1. Create a managed policy using the JSON policy below and name of iam-restricted-list-read . { Version : 2012-10-17 , Statement : [ { Sid : Get , Effect : Allow , Action : [ iam:ListPolicies , iam:GetRole , iam:GetPolicyVersion , iam:ListRoleTags , iam:GetPolicy , iam:ListPolicyVersions , iam:ListAttachedRolePolicies , iam:ListRoles , iam:ListRolePolicies , iam:GetRolePolicy ], Resource : * } ] } 2. Create and Test Developer Role 2.1 Create Developer Role Create a role for developers that will have permission to create roles and policies, with the permission boundary and naming prefix enforced: 1. Sign in to the AWS Management Console as an IAM user with MFA enabled that can assume roles in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . 2. In the navigation pane, click Roles and then click Create role . 3. Click Another AWS account, then enter your account ID and tick Require MFA, then click Next: Permissions . We enforce MFA here as it is a best practice. 4. In the search field start typing createrole then check the box next to the createrole-restrict-region-boundary policy. 5. Erase your previous search and start typing iam-res then check the box next to the iam-restricted-list-read policy and then click Next: Tags . 6. For this lab we will not use IAM tags, click Next: Review . 7. Enter the name of developer-restricted-iam for the Role name and click Create role . 8. Check the role you have created by clicking on developer-restricted-iam in the list. Record both the Role ARN and the link to the console. 9. The role is now created, ready to test! 2.2. Test Developer Role Now you will use an existing IAM user with MFA enabled to assume the new developer-restricted-iam role. 1. Sign in to the AWS Management Console as an IAM user with MFA enabled. https://console.aws.amazon.com . 2. In the console, click your user name on the navigation bar in the upper right. It typically looks like this: username@account_ID_number_or_alias then click Switch Role . Alternatively you can paste the link in your browser that you recorded earlier. 3. On the Switch Role page, type the account ID number or the account alias and the name of the role developer-restricted-iam that you created in the previous step. (Optional) Type text that you want to appear on the navigation bar in place of your user name when this role is active. A name is suggested, based on the account and role information, but you can change it to whatever has meaning for you. You can also select a color to highlight the display name. 4. Click Switch Role . If this is the first time choosing this option, a page appears with more information. After reading it, click Switch Role. If you clear your browser cookies, this page can appear again. 5. The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you replacing the permission that you had as the IAM user. **Tip** The last several roles that you used appear on the menu. The next time you need to switch to one of those roles, you can simply click the role you want. You only need to type the account and role information manually if the role is not displayed on the Identity menu. You are now using the developer role with the granted permissions, stay logged in using the role for the next section. 3. Create and Test User Role 3.1 Create User Role While you are still assuming the developer-restricted-iam role you created in the previous step, create a new user role with the boundary policy attached and name it with the prefix. We will use AWS managed policies for this user role, however the createrole-restrict-region-boundary policy will allow us to create and attach our own policies, only if they have a prefix of app1 . 1. Verify that you are Using the developer role previously created by checking the top bar it should look like and open the IAM console at https://console.aws.amazon.com/iam/ . You will notice a number of permission denied messages as this developer role is restricted. Least privilege is a best practice! 2. In the navigation pane, click Roles and then click Create role . 3. Click Another AWS account , then enter your account ID that you have been using for this lab and tick Require MFA , then click Next: Permissions . 4. In the search field start typing ec2full then check the box next to the AmazonEC2FullAccess policy. 5. Erase your previous search and start typing lambda then check the box next to the AWSLambdaFullAccess policy. 6. Expand the bottom section Set permissions boundary and click Use a permissions boundary to control the maximum role permissions . In the search field start typing boundary then click the radio button for restrict-region-boundary and then click Next: Tags . 7. For this lab we will not use IAM tags, click Next: Review . 8. Enter the Role name of app1-user-region-restricted-services for the role and click Create role . 9. The role should create successfully if you followed all the steps. Record both the Role ARN and the link to the console. If you receive an error message a common mistake is not changing the account number in the policies in the previous steps. 3.2 Test User Role Now you will use an existing IAM user to assume the new app1-user-region-restricted-services role, as if you were a user who only needs to administer EC2 and Lambda in your allowed regions. 1. In the console, click your role's Display Name on the right side of the navigation bar. Click Back to your previous username . You are now back to using your original IAM user. 2. In the console, click your user name on the navigation bar in the upper right. Alternatively you can paste the link in your browser that you recorded earlier for the app1-user-region-restricted-services role. 3. On the Switch Role page, type the account ID number or the account alias and the name of the role app1-user-region-restricted-services that you created in the previous step. 5. Select a different color to before, otherwise it will overwrite that profile in your browser. 6. Click Switch Role . The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you. 7. You are now using the user role with the only actions allowed of EC2 and Lambda in us-east-1 (North Virginia) and us-west-1 (North California) regions! 8. Navigate to the EC2 Management Console in the us-east-1 region https://us-east-1.console.aws.amazon.com/ec2/v2/home?region=us-east-1 . The EC2 Dashboard should display a summary list of resources with the only error being Error retrieving resource count from Elastic Load Balancing as that requires additional permissions. 9. Navigate to the EC2 Management Console in a region that is not allowed, such as ap-southeast-2 (Sydney) https://ap-southeast-2.console.aws.amazon.com/ec2/v2/home?region=ap-southeast-2 . The EC2 Dashboard should display a number of unauthorized error messages. 10. Congratulations! You have now learnt about IAM permission boundaries and have one working! 4. Tear down this lab Please note that the changes you made to the users, groups, and roles have no charges associated with them. 1. Using the original IAM user, for each of the roles you created select them in the IAM console at https://console.aws.amazon.com/iam/ and click Delete role . The roles created are: app1-user-region-restricted-services developer-restricted-iam 2. For each of the policies you created, one at a time select the radio button then Policy actions drop down menu then Delete . The policies created are: restrict-region-boundary createrole-restrict-region-boundary iam-restricted-list-read References useful resources: Permissions Boundaries for IAM Entities AWS Identity and Access Management User Guide IAM Best Practices and Use Cases Become an IAM Policy Master in 60 Minutes or Less Actions, Resources, and Condition Keys for Identity And Access Management License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 300: IAM Permission Boundaries Delegating Role Creation"},{"location":"Security/300 - IAM Permission Boundaries Delegating Role Creation/Lab Guide/#level-300-iam-permission-boundaries-delegating-role-creation","text":"","title":"Level 300: IAM Permission Boundaries Delegating Role Creation"},{"location":"Security/300 - IAM Permission Boundaries Delegating Role Creation/Lab Guide/#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/300 - IAM Permission Boundaries Delegating Role Creation/Lab Guide/#table-of-contents","text":"Create IAM Policies Create and Test Developer Role Create and Test Region Restricted User Role Tear Down","title":"Table of Contents"},{"location":"Security/300 - IAM Permission Boundaries Delegating Role Creation/Lab Guide/#1-create-iam-policies","text":"","title":"1. Create IAM policies "},{"location":"Security/300 - IAM Permission Boundaries Delegating Role Creation/Lab Guide/#11-create-policy-for-permission-boundary","text":"This policy will be used for the permission boundary when the developer role creates their own user role with their delegated permissions. In this lab we are only going to allow regions us-east-1 (North Virginia) and us-west-1 (North California), optionally you can change these to your favourite regions and add / remove as many as you need. The only service actions we are going to allow are ec2 and lambda, note that these services require additional supporting actions if you were to re-use this policy after this lab, depending on your requirements. 1. Sign in to the AWS Management Console as an IAM user with MFA enabled that can assume roles in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . 2. In the navigation pane, click Policies and then click Create policy . 3. On the Create policy page click the JSON tab. 4. Replace the example start of the policy that is already in the editor with the policy below. { Version : 2012-10-17 , Statement : [ { Sid : EC2RestrictRegion , Effect : Allow , Action : ec2:* , Resource : * , Condition : { StringEquals : { aws:RequestedRegion : [ us-east-1 , us-west-1 ] } } }, { Sid : LambdaRestrictRegion , Effect : Allow , Action : lambda:* , Resource : * , Condition : { StringEquals : { aws:RequestedRegion : [ us-east-1 , us-west-1 ] } } } ] } Click Review policy . Enter the name of restrict-region-boundary and any description to help you identify the policy, verify the summary and then click Create policy .","title":"1.1 Create policy for permission boundary"},{"location":"Security/300 - IAM Permission Boundaries Delegating Role Creation/Lab Guide/#12-create-developer-iam-restricted-policy","text":"This policy will be attached to the developer role, and will allow the developer to create policies and roles with a name prefix of app1 , and only if the permission boundary restrict-region-boundary is attached. You will need to change the account id placeholders of 123456789012 to your account number in 5 places. You can find your account id by navigating to https://console.aws.amazon.com/billing/home?#/account in the console. Naming prefixes are useful when you have different teams or in this case different applications running in the same AWS account. They can be used to keep your resources looking tidy, and also in IAM policy as the resource as we are doing here. 1. Create a managed policy using the JSON policy below and name of createrole-restrict-region-boundary . { Version : 2012-10-17 , Statement : [ { Sid : CreatePolicy , Effect : Allow , Action : [ iam:CreatePolicy , iam:CreatePolicyVersion , iam:DeletePolicyVersion ], Resource : arn:aws:iam::123456789012:policy/app1* }, { Sid : CreateRole , Effect : Allow , Action : [ iam:CreateRole ], Resource : arn:aws:iam::123456789012:role/app1* , Condition : { StringEquals : { iam:PermissionsBoundary : arn:aws:iam::123456789012:policy/restrict-region-boundary } } }, { Sid : AttachDetachRolePolicy , Effect : Allow , Action : [ iam:DetachRolePolicy , iam:AttachRolePolicy ], Resource : arn:aws:iam::123456789012:role/app1* , Condition : { ArnEquals : { iam:PolicyARN : [ arn:aws:iam::123456789012:policy/* , arn:aws:iam::aws:policy/* ] } } } ] }","title":"1.2 Create developer IAM restricted policy"},{"location":"Security/300 - IAM Permission Boundaries Delegating Role Creation/Lab Guide/#13-create-developer-iam-console-access-role","text":"This policy allows list and read type IAM service actions so you can see what you have created using the console. Note that it is not a requirement if you simply wanted to create the role and policy, or if you were using the Command Line Interface (CLI) or CloudFormation. 1. Create a managed policy using the JSON policy below and name of iam-restricted-list-read . { Version : 2012-10-17 , Statement : [ { Sid : Get , Effect : Allow , Action : [ iam:ListPolicies , iam:GetRole , iam:GetPolicyVersion , iam:ListRoleTags , iam:GetPolicy , iam:ListPolicyVersions , iam:ListAttachedRolePolicies , iam:ListRoles , iam:ListRolePolicies , iam:GetRolePolicy ], Resource : * } ] }","title":"1.3 Create developer IAM console access role"},{"location":"Security/300 - IAM Permission Boundaries Delegating Role Creation/Lab Guide/#2-create-and-test-developer-role","text":"","title":"2. Create and Test Developer Role "},{"location":"Security/300 - IAM Permission Boundaries Delegating Role Creation/Lab Guide/#21-create-developer-role","text":"Create a role for developers that will have permission to create roles and policies, with the permission boundary and naming prefix enforced: 1. Sign in to the AWS Management Console as an IAM user with MFA enabled that can assume roles in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . 2. In the navigation pane, click Roles and then click Create role . 3. Click Another AWS account, then enter your account ID and tick Require MFA, then click Next: Permissions . We enforce MFA here as it is a best practice. 4. In the search field start typing createrole then check the box next to the createrole-restrict-region-boundary policy. 5. Erase your previous search and start typing iam-res then check the box next to the iam-restricted-list-read policy and then click Next: Tags . 6. For this lab we will not use IAM tags, click Next: Review . 7. Enter the name of developer-restricted-iam for the Role name and click Create role . 8. Check the role you have created by clicking on developer-restricted-iam in the list. Record both the Role ARN and the link to the console. 9. The role is now created, ready to test!","title":"2.1 Create Developer Role"},{"location":"Security/300 - IAM Permission Boundaries Delegating Role Creation/Lab Guide/#22-test-developer-role","text":"Now you will use an existing IAM user with MFA enabled to assume the new developer-restricted-iam role. 1. Sign in to the AWS Management Console as an IAM user with MFA enabled. https://console.aws.amazon.com . 2. In the console, click your user name on the navigation bar in the upper right. It typically looks like this: username@account_ID_number_or_alias then click Switch Role . Alternatively you can paste the link in your browser that you recorded earlier. 3. On the Switch Role page, type the account ID number or the account alias and the name of the role developer-restricted-iam that you created in the previous step. (Optional) Type text that you want to appear on the navigation bar in place of your user name when this role is active. A name is suggested, based on the account and role information, but you can change it to whatever has meaning for you. You can also select a color to highlight the display name. 4. Click Switch Role . If this is the first time choosing this option, a page appears with more information. After reading it, click Switch Role. If you clear your browser cookies, this page can appear again. 5. The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you replacing the permission that you had as the IAM user. **Tip** The last several roles that you used appear on the menu. The next time you need to switch to one of those roles, you can simply click the role you want. You only need to type the account and role information manually if the role is not displayed on the Identity menu. You are now using the developer role with the granted permissions, stay logged in using the role for the next section.","title":"2.2. Test Developer Role"},{"location":"Security/300 - IAM Permission Boundaries Delegating Role Creation/Lab Guide/#3-create-and-test-user-role","text":"","title":"3. Create and Test User Role "},{"location":"Security/300 - IAM Permission Boundaries Delegating Role Creation/Lab Guide/#31-create-user-role","text":"While you are still assuming the developer-restricted-iam role you created in the previous step, create a new user role with the boundary policy attached and name it with the prefix. We will use AWS managed policies for this user role, however the createrole-restrict-region-boundary policy will allow us to create and attach our own policies, only if they have a prefix of app1 . 1. Verify that you are Using the developer role previously created by checking the top bar it should look like and open the IAM console at https://console.aws.amazon.com/iam/ . You will notice a number of permission denied messages as this developer role is restricted. Least privilege is a best practice! 2. In the navigation pane, click Roles and then click Create role . 3. Click Another AWS account , then enter your account ID that you have been using for this lab and tick Require MFA , then click Next: Permissions . 4. In the search field start typing ec2full then check the box next to the AmazonEC2FullAccess policy. 5. Erase your previous search and start typing lambda then check the box next to the AWSLambdaFullAccess policy. 6. Expand the bottom section Set permissions boundary and click Use a permissions boundary to control the maximum role permissions . In the search field start typing boundary then click the radio button for restrict-region-boundary and then click Next: Tags . 7. For this lab we will not use IAM tags, click Next: Review . 8. Enter the Role name of app1-user-region-restricted-services for the role and click Create role . 9. The role should create successfully if you followed all the steps. Record both the Role ARN and the link to the console. If you receive an error message a common mistake is not changing the account number in the policies in the previous steps.","title":"3.1 Create User Role"},{"location":"Security/300 - IAM Permission Boundaries Delegating Role Creation/Lab Guide/#32-test-user-role","text":"Now you will use an existing IAM user to assume the new app1-user-region-restricted-services role, as if you were a user who only needs to administer EC2 and Lambda in your allowed regions. 1. In the console, click your role's Display Name on the right side of the navigation bar. Click Back to your previous username . You are now back to using your original IAM user. 2. In the console, click your user name on the navigation bar in the upper right. Alternatively you can paste the link in your browser that you recorded earlier for the app1-user-region-restricted-services role. 3. On the Switch Role page, type the account ID number or the account alias and the name of the role app1-user-region-restricted-services that you created in the previous step. 5. Select a different color to before, otherwise it will overwrite that profile in your browser. 6. Click Switch Role . The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you. 7. You are now using the user role with the only actions allowed of EC2 and Lambda in us-east-1 (North Virginia) and us-west-1 (North California) regions! 8. Navigate to the EC2 Management Console in the us-east-1 region https://us-east-1.console.aws.amazon.com/ec2/v2/home?region=us-east-1 . The EC2 Dashboard should display a summary list of resources with the only error being Error retrieving resource count from Elastic Load Balancing as that requires additional permissions. 9. Navigate to the EC2 Management Console in a region that is not allowed, such as ap-southeast-2 (Sydney) https://ap-southeast-2.console.aws.amazon.com/ec2/v2/home?region=ap-southeast-2 . The EC2 Dashboard should display a number of unauthorized error messages. 10. Congratulations! You have now learnt about IAM permission boundaries and have one working!","title":"3.2 Test User Role"},{"location":"Security/300 - IAM Permission Boundaries Delegating Role Creation/Lab Guide/#4-tear-down-this-lab","text":"Please note that the changes you made to the users, groups, and roles have no charges associated with them. 1. Using the original IAM user, for each of the roles you created select them in the IAM console at https://console.aws.amazon.com/iam/ and click Delete role . The roles created are: app1-user-region-restricted-services developer-restricted-iam 2. For each of the policies you created, one at a time select the radio button then Policy actions drop down menu then Delete . The policies created are: restrict-region-boundary createrole-restrict-region-boundary iam-restricted-list-read","title":"4. Tear down this lab "},{"location":"Security/300 - IAM Permission Boundaries Delegating Role Creation/Lab Guide/#references-useful-resources","text":"Permissions Boundaries for IAM Entities AWS Identity and Access Management User Guide IAM Best Practices and Use Cases Become an IAM Policy Master in 60 Minutes or Less Actions, Resources, and Condition Keys for Identity And Access Management","title":"References &amp; useful resources:"},{"location":"Security/300 - IAM Permission Boundaries Delegating Role Creation/Lab Guide/#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/300 - IAM Tag Based Access Control for EC2/","text":"Level 300: IAM Tag Based Access Control for EC2 Introduction This hands-on lab will guide you through the steps to configure example AWS Identity and Access Management (IAM) policies, and a role to use EC2 resource tags for access control. Using tags is powerful as it helps you scale your permission management, however you need to be careful about the management of the tags which you will learn in this lab. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as an EC2 administrator. This allows the EC2 administrator to create tags when creating resources only if they match the requirements, and control which existing resources and values they can tag. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals: IAM least privilege IAM policy conditions Prerequisites: An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user with MFA enabled that can assume roles in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Overview Lab Guide.md the guide for this lab /Images referenced by this lab Permissions required IAM User with AdministratorAccess AWS managed policy License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 300: IAM Tag Based Access Control for EC2"},{"location":"Security/300 - IAM Tag Based Access Control for EC2/#level-300-iam-tag-based-access-control-for-ec2","text":"","title":"Level 300: IAM Tag Based Access Control for EC2"},{"location":"Security/300 - IAM Tag Based Access Control for EC2/#introduction","text":"This hands-on lab will guide you through the steps to configure example AWS Identity and Access Management (IAM) policies, and a role to use EC2 resource tags for access control. Using tags is powerful as it helps you scale your permission management, however you need to be careful about the management of the tags which you will learn in this lab. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as an EC2 administrator. This allows the EC2 administrator to create tags when creating resources only if they match the requirements, and control which existing resources and values they can tag. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/300 - IAM Tag Based Access Control for EC2/#goals","text":"IAM least privilege IAM policy conditions","title":"Goals:"},{"location":"Security/300 - IAM Tag Based Access Control for EC2/#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user with MFA enabled that can assume roles in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites:"},{"location":"Security/300 - IAM Tag Based Access Control for EC2/#overview","text":"Lab Guide.md the guide for this lab /Images referenced by this lab","title":"Overview"},{"location":"Security/300 - IAM Tag Based Access Control for EC2/#permissions-required","text":"IAM User with AdministratorAccess AWS managed policy","title":"Permissions required"},{"location":"Security/300 - IAM Tag Based Access Control for EC2/#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/300 - IAM Tag Based Access Control for EC2/Lab Guide/","text":"Level 300: IAM Tag Based Access Control for EC2 Authors Ben Potter, Security Lead, Well-Architected Table of Contents Create IAM Policies Create IAM Role Test Tear Down In this lab we use the RequestTag condition key to require specific tag value during create actions in the EC2 service. This allows users to create tags when creating resources only if they meet specific requirements. To control which existing resources users can modify tags on we use a combination of RequestTag and ResourceTag conditions. To control resources users can manage based on tag values we use ResourceTag based on a tag that exists on a resource. You can think of RequestTag condition key is for new resources when you are creating, and ResourceTag is the tag that already exists on the resource. 1. Create IAM managed policies The policies are split into five different functions for demonstration purposes, you may like to modify and combine them to use after this lab to your exact requirements. In addition to enforcing tags, a region restriction only allow regions us-east-1 (North Virginia) and us-west-1 (North California). 1.1 Create policy named ec2-list-read This policy allows read only permissions with a region condition. The only service actions we are going to allow are EC2, note that you typically require additional supporting actions such as Elastic Load Balancing if you were to re-use this policy after this lab, depending on your requirements. 1. Sign in to the AWS Maagement Console as an IAM user with MFA enabled that can assume roles in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . 2. In the navigation pane, click Policies and then click Create policy . 3. On the Create policy page click the JSON tab. 4. Replace the example start of the policy that is already in the editor with the policy below. { Version : 2012-10-17 , Statement : [ { Sid : ec2listread , Effect : Allow , Action : [ ec2:DescribeInstances , ec2:DescribeAggregateIdFormat , ec2:DescribeVolumesModifications , ec2:GetHostReservationPurchasePreview , ec2:DescribeSnapshots , ec2:DescribePlacementGroups , ec2:GetConsoleScreenshot , ec2:DescribeHostReservationOfferings , ec2:DescribeInternetGateways , ec2:GetLaunchTemplateData , ec2:DescribeVolumeStatus , ec2:DescribeScheduledInstanceAvailability , ec2:DescribeSpotDatafeedSubscription , ec2:DescribeVolumes , ec2:DescribeFpgaImageAttribute , ec2:DescribeExportTasks , ec2:DescribeAccountAttributes , ec2:DescribeNetworkInterfacePermissions , ec2:DescribeReservedInstances , ec2:DescribeKeyPairs , ec2:DescribeNetworkAcls , ec2:DescribeRouteTables , ec2:DescribeReservedInstancesListings , ec2:DescribeEgressOnlyInternetGateways , ec2:DescribeSpotFleetRequestHistory , ec2:DescribeLaunchTemplates , ec2:DescribeVpcClassicLinkDnsSupport , ec2:DescribeVpnConnections , ec2:DescribeSnapshotAttribute , ec2:DescribeVpcPeeringConnections , ec2:DescribeReservedInstancesOfferings , ec2:DescribeIdFormat , ec2:DescribeFleetInstances , ec2:DescribeVpcEndpointServiceConfigurations , ec2:DescribePrefixLists , ec2:GetReservedInstancesExchangeQuote , ec2:DescribeVolumeAttribute , ec2:DescribeInstanceCreditSpecifications , ec2:DescribeVpcClassicLink , ec2:DescribeImportSnapshotTasks , ec2:DescribeVpcEndpointServicePermissions , ec2:GetPasswordData , ec2:DescribeScheduledInstances , ec2:DescribeImageAttribute , ec2:DescribeFleets , ec2:DescribeVpcEndpoints , ec2:DescribeReservedInstancesModifications , ec2:DescribeElasticGpus , ec2:DescribeSubnets , ec2:DescribeVpnGateways , ec2:DescribeMovingAddresses , ec2:DescribeFleetHistory , ec2:DescribePrincipalIdFormat , ec2:DescribeAddresses , ec2:DescribeInstanceAttribute , ec2:DescribeRegions , ec2:DescribeFlowLogs , ec2:DescribeDhcpOptions , ec2:DescribeVpcEndpointServices , ec2:DescribeSpotInstanceRequests , ec2:DescribeVpcAttribute , ec2:GetConsoleOutput , ec2:DescribeSpotPriceHistory , ec2:DescribeNetworkInterfaces , ec2:DescribeAvailabilityZones , ec2:DescribeNetworkInterfaceAttribute , ec2:DescribeVpcEndpointConnections , ec2:DescribeInstanceStatus , ec2:DescribeHostReservations , ec2:DescribeIamInstanceProfileAssociations , ec2:DescribeTags , ec2:DescribeLaunchTemplateVersions , ec2:DescribeBundleTasks , ec2:DescribeIdentityIdFormat , ec2:DescribeImportImageTasks , ec2:DescribeClassicLinkInstances , ec2:DescribeNatGateways , ec2:DescribeCustomerGateways , ec2:DescribeVpcEndpointConnectionNotifications , ec2:DescribeSecurityGroups , ec2:DescribeSpotFleetRequests , ec2:DescribeHosts , ec2:DescribeImages , ec2:DescribeFpgaImages , ec2:DescribeSpotFleetInstances , ec2:DescribeSecurityGroupReferences , ec2:DescribeVpcs , ec2:DescribeConversionTasks , ec2:DescribeStaleSecurityGroups ], Resource : * , Condition : { StringEquals : { aws:RequestedRegion : [ us-east-1 , us-west-1 ] } } } ] } Click Review policy . Enter the name of ec2-list-read and any description to help you identify the policy, verify the summary and then click Create policy . 1.2 Create policy named ec2-create-tags This policy allows the creation of tags for EC2, with a condition of the action being RunInstances , which is launching an instance. 1. Create a managed policy using the JSON policy below and name of ec2-create-tags . { Version : 2012-10-17 , Statement : [ { Sid : ec2createtags , Effect : Allow , Action : ec2:CreateTags , Resource : * , Condition : { StringEquals : { ec2:CreateAction : RunInstances } } } ] } 1.3 Create policy named ec2-create-tags-existing This policy allows creation (and overwriting) of EC2 tags only if the resources are already tagged Team / Alpha . 1. Create a managed policy using the JSON policy below and name of ec2-create-tags-existing . { Version : 2012-10-17 , Statement : [ { Sid : ec2createtagsexisting , Effect : Allow , Action : ec2:CreateTags , Resource : * , Condition : { StringEquals : { ec2:ResourceTag/Team : Alpha }, ForAllValues:StringEquals : { aws:TagKeys : [ Team , Name ] }, StringEqualsIfExists : { aws:RequestTag/Team : Alpha } } } ] } 1.4 Create policy named ec2-run-instances This first section of this policy allows instances to be launched, only if the conditions of region and specific tag keys are matched. The second section allows other resources to be created at instance launch time with region condition. 1. Create a managed policy using the JSON policy below and name of ec2-run-instances . { Version : 2012-10-17 , Statement : [ { Sid : ec2runinstances , Effect : Allow , Action : ec2:RunInstances , Resource : arn:aws:ec2:*:*:instance/* , Condition : { StringEquals : { aws:RequestedRegion : [ us-east-1 , us-west-1 ], aws:RequestTag/Team : Alpha }, ForAllValues:StringEquals : { aws:TagKeys : [ Name , Team ] } } }, { Sid : ec2runinstancesother , Effect : Allow , Action : ec2:RunInstances , Resource : [ arn:aws:ec2:*:*:subnet/* , arn:aws:ec2:*:*:key-pair/* , arn:aws:ec2:*::snapshot/* , arn:aws:ec2:*:*:launch-template/* , arn:aws:ec2:*:*:volume/* , arn:aws:ec2:*:*:security-group/* , arn:aws:ec2:*:*:placement-group/* , arn:aws:ec2:*:*:network-interface/* , arn:aws:ec2:*::image/* ], Condition : { StringEquals : { aws:RequestedRegion : [ us-east-1 , us-west-1 ] } } } ] } 1.5 Create policy named ec2-manage-instances This policy allows reboot, terminate, start and stop of instances, with a condition of the key Team is Alpha and region. 1. Create a managed policy using the JSON policy below and name of ec2-manage-instances . { Version : 2012-10-17 , Statement : [ { Sid : ec2manageinstances , Effect : Allow , Action : [ ec2:RebootInstances , ec2:TerminateInstances , ec2:StartInstances , ec2:StopInstances ], Resource : * , Condition : { StringEquals : { ec2:ResourceTag/Team : Alpha , aws:RequestedRegion : [ us-east-1 , us-west-1 ] } } } ] } 2. Create Role Create a role for EC2 administrators, and attach the managed policies previously created. 1. Sign in to the AWS Management Console as an IAM user with MFA enabled that can assume roles in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . 2. In the navigation pane, click Roles and then click Create role . 3. Click Another AWS account, then enter the account ID of the account you are using now and tick Require MFA, then click Next: Permissions . We enforce MFA here as it is a best practice. 4. In the search field start typing ec2- then check the box next to the policies you just created: ec2-create-tags , ec2-create-tags-existing , ec2-list-read , ec2-manage-instances , ec2-run-instances . and then click Next: Tags . 5. For this lab we will not use IAM tags, click Next: Review . 6. Enter the name of ec2-admin-team-alpha for the Role name and click Create role . 8. Check the role you have created by clicking on ec2-admin-team-alpha in the list. Record both the Role ARN and the link to the console. 9. The role is now created, ready to test! 3. Test Role 3.1 Assume ec2-admin-team-alpha Role Now you will use an existing IAM user with MFA enabled to assume the new ec2-admin-team-alpha role. 1. Sign in to the AWS Management Console as an IAM user with MFA enabled. https://console.aws.amazon.com . 2. In the console, click your user name on the navigation bar in the upper right. It typically looks like this: username@account_ID_number_or_alias then click Switch Role . Alternatively you can paste the link in your browser that you recorded earlier. 3. On the Switch Role page, type the account ID number or the account alias and the name of the role ec2-admin-team-alpha that you created in the previous step. (Optional) Type text that you want to appear on the navigation bar in place of your user name when this role is active. A name is suggested, based on the account and role information, but you can change it to whatever has meaning for you. You can also select a color to highlight the display name. 4. Click Switch Role . If this is the first time choosing this option, a page appears with more information. After reading it, click Switch Role. If you clear your browser cookies, this page can appear again. 5. The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you replacing the permission that you had as the IAM user. **Tip** The last several roles that you used appear on the menu. The next time you need to switch to one of those roles, you can simply click the role you want. You only need to type the account and role information manually if the role is not displayed on the Identity menu. 3.2 Launch Instance With Without Tags Navigate to the EC2 Management Console in the us-east-2 (Ohio) region https://us-east-2.console.aws.amazon.com/ec2/v2/home?region=us-east-2 . The EC2 Dashboard should display a list of errors including You are not authorized . This is the first test passed, as us-east-2 region is not allowed. Navigate to the EC2 Management Console in the us-east-1 (North Virginia) region https://us-east-1.console.aws.amazon.com/ec2/v2/home?region=us-east-1 . The EC2 Dashboard should display a summary list of resources with the only error being Error retrieving resource count from Elastic Load Balancing as that requires additional permissions. Click Launch Instance button to start the wizard. Click Select next to the first Amazon Linux 2 Amazon Machine Image to launch. Accept the default instance size by clicking Next: Configure Instance Details . Accept default details by clicking Next: Add Storage . Accept default storage options by clicking Next: Add Tags . Lets add an incorrect tag now that will fail to launch. Click Add Tag enter Key of Name and Value of Example . Repeat to add Key of Team and Value of Beta . Note: Keys and values are case sensitive! Click Next: Configure Security Group . Click Select an existing security group , click the check box next to security group with name default , then click Review and Launch . Click Launch then click the option to Proceed without a key pair . Tick the I acknowledge box then click Launch Instances . The launch should fail, if it succeeded verify the role you are using and the managed roles you have attached as per previous steps. Click Back to Review Screen then click Edit tags to modify the tags. Change the Team key to a value of Alpha which matches the IAM policy previously created then click Review and Launch . On the review launch page once again click Launch then click the option to Proceed without a key pair . Tick the I acknowledge box then click Launch Instances . You should see a message that the instance is now launching. Click View Instances and do not terminate it just yet. 3.3 Modify Tags On Instances Continuing from 3.2 in the EC2 Management Console instances view, click the check box next to the instance named Example then the Tags tab. Click Add/Edit Tags , try changing the Team key to a value of Test then click Save . An error message should appear. Change the Team key back to Alpha, and edit the Name key to a value of Test and click Save . The request should succeed. 3.4 Manage Instances Continuing from 3.3 in the EC2 Management Console instances view, click the check box next to the instance named Test . Click Actions button then expand out Instance State then Terminate . Check the instance is the one you wish to terminate by it's name and click Yes, Terminate . The instance should now terminate. Congratulations! You have now learnt about IAM permission boundaries and have one working! 4. Tear down this lab Please note that the changes you made to the policies and roles have no charges associated with them. 1. Using the original IAM user, select the ec2-admin-team-alpha role in the IAM console at https://console.aws.amazon.com/iam/ and click Delete role . 2. For each of the policies you created, one at a time select the radio button then Policy actions drop down menu then Delete . The policies created are: ec2-create-tags ec2-create-tags-existing ec2-list-read ec2-manage-instances ec2-run-instances References useful resources: AWS Identity and Access Management User Guide IAM Best Practices and Use Cases Become an IAM Policy Master in 60 Minutes or Less Actions, Resources, and Condition Keys for Identity And Access Management License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 300: IAM Tag Based Access Control for EC2"},{"location":"Security/300 - IAM Tag Based Access Control for EC2/Lab Guide/#level-300-iam-tag-based-access-control-for-ec2","text":"","title":"Level 300: IAM Tag Based Access Control for EC2"},{"location":"Security/300 - IAM Tag Based Access Control for EC2/Lab Guide/#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/300 - IAM Tag Based Access Control for EC2/Lab Guide/#table-of-contents","text":"Create IAM Policies Create IAM Role Test Tear Down In this lab we use the RequestTag condition key to require specific tag value during create actions in the EC2 service. This allows users to create tags when creating resources only if they meet specific requirements. To control which existing resources users can modify tags on we use a combination of RequestTag and ResourceTag conditions. To control resources users can manage based on tag values we use ResourceTag based on a tag that exists on a resource. You can think of RequestTag condition key is for new resources when you are creating, and ResourceTag is the tag that already exists on the resource.","title":"Table of Contents"},{"location":"Security/300 - IAM Tag Based Access Control for EC2/Lab Guide/#1-create-iam-managed-policies","text":"The policies are split into five different functions for demonstration purposes, you may like to modify and combine them to use after this lab to your exact requirements. In addition to enforcing tags, a region restriction only allow regions us-east-1 (North Virginia) and us-west-1 (North California).","title":"1. Create IAM managed policies "},{"location":"Security/300 - IAM Tag Based Access Control for EC2/Lab Guide/#11-create-policy-named-ec2-list-read","text":"This policy allows read only permissions with a region condition. The only service actions we are going to allow are EC2, note that you typically require additional supporting actions such as Elastic Load Balancing if you were to re-use this policy after this lab, depending on your requirements. 1. Sign in to the AWS Maagement Console as an IAM user with MFA enabled that can assume roles in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . 2. In the navigation pane, click Policies and then click Create policy . 3. On the Create policy page click the JSON tab. 4. Replace the example start of the policy that is already in the editor with the policy below. { Version : 2012-10-17 , Statement : [ { Sid : ec2listread , Effect : Allow , Action : [ ec2:DescribeInstances , ec2:DescribeAggregateIdFormat , ec2:DescribeVolumesModifications , ec2:GetHostReservationPurchasePreview , ec2:DescribeSnapshots , ec2:DescribePlacementGroups , ec2:GetConsoleScreenshot , ec2:DescribeHostReservationOfferings , ec2:DescribeInternetGateways , ec2:GetLaunchTemplateData , ec2:DescribeVolumeStatus , ec2:DescribeScheduledInstanceAvailability , ec2:DescribeSpotDatafeedSubscription , ec2:DescribeVolumes , ec2:DescribeFpgaImageAttribute , ec2:DescribeExportTasks , ec2:DescribeAccountAttributes , ec2:DescribeNetworkInterfacePermissions , ec2:DescribeReservedInstances , ec2:DescribeKeyPairs , ec2:DescribeNetworkAcls , ec2:DescribeRouteTables , ec2:DescribeReservedInstancesListings , ec2:DescribeEgressOnlyInternetGateways , ec2:DescribeSpotFleetRequestHistory , ec2:DescribeLaunchTemplates , ec2:DescribeVpcClassicLinkDnsSupport , ec2:DescribeVpnConnections , ec2:DescribeSnapshotAttribute , ec2:DescribeVpcPeeringConnections , ec2:DescribeReservedInstancesOfferings , ec2:DescribeIdFormat , ec2:DescribeFleetInstances , ec2:DescribeVpcEndpointServiceConfigurations , ec2:DescribePrefixLists , ec2:GetReservedInstancesExchangeQuote , ec2:DescribeVolumeAttribute , ec2:DescribeInstanceCreditSpecifications , ec2:DescribeVpcClassicLink , ec2:DescribeImportSnapshotTasks , ec2:DescribeVpcEndpointServicePermissions , ec2:GetPasswordData , ec2:DescribeScheduledInstances , ec2:DescribeImageAttribute , ec2:DescribeFleets , ec2:DescribeVpcEndpoints , ec2:DescribeReservedInstancesModifications , ec2:DescribeElasticGpus , ec2:DescribeSubnets , ec2:DescribeVpnGateways , ec2:DescribeMovingAddresses , ec2:DescribeFleetHistory , ec2:DescribePrincipalIdFormat , ec2:DescribeAddresses , ec2:DescribeInstanceAttribute , ec2:DescribeRegions , ec2:DescribeFlowLogs , ec2:DescribeDhcpOptions , ec2:DescribeVpcEndpointServices , ec2:DescribeSpotInstanceRequests , ec2:DescribeVpcAttribute , ec2:GetConsoleOutput , ec2:DescribeSpotPriceHistory , ec2:DescribeNetworkInterfaces , ec2:DescribeAvailabilityZones , ec2:DescribeNetworkInterfaceAttribute , ec2:DescribeVpcEndpointConnections , ec2:DescribeInstanceStatus , ec2:DescribeHostReservations , ec2:DescribeIamInstanceProfileAssociations , ec2:DescribeTags , ec2:DescribeLaunchTemplateVersions , ec2:DescribeBundleTasks , ec2:DescribeIdentityIdFormat , ec2:DescribeImportImageTasks , ec2:DescribeClassicLinkInstances , ec2:DescribeNatGateways , ec2:DescribeCustomerGateways , ec2:DescribeVpcEndpointConnectionNotifications , ec2:DescribeSecurityGroups , ec2:DescribeSpotFleetRequests , ec2:DescribeHosts , ec2:DescribeImages , ec2:DescribeFpgaImages , ec2:DescribeSpotFleetInstances , ec2:DescribeSecurityGroupReferences , ec2:DescribeVpcs , ec2:DescribeConversionTasks , ec2:DescribeStaleSecurityGroups ], Resource : * , Condition : { StringEquals : { aws:RequestedRegion : [ us-east-1 , us-west-1 ] } } } ] } Click Review policy . Enter the name of ec2-list-read and any description to help you identify the policy, verify the summary and then click Create policy .","title":"1.1 Create policy named ec2-list-read"},{"location":"Security/300 - IAM Tag Based Access Control for EC2/Lab Guide/#12-create-policy-named-ec2-create-tags","text":"This policy allows the creation of tags for EC2, with a condition of the action being RunInstances , which is launching an instance. 1. Create a managed policy using the JSON policy below and name of ec2-create-tags . { Version : 2012-10-17 , Statement : [ { Sid : ec2createtags , Effect : Allow , Action : ec2:CreateTags , Resource : * , Condition : { StringEquals : { ec2:CreateAction : RunInstances } } } ] }","title":"1.2 Create policy named ec2-create-tags"},{"location":"Security/300 - IAM Tag Based Access Control for EC2/Lab Guide/#13-create-policy-named-ec2-create-tags-existing","text":"This policy allows creation (and overwriting) of EC2 tags only if the resources are already tagged Team / Alpha . 1. Create a managed policy using the JSON policy below and name of ec2-create-tags-existing . { Version : 2012-10-17 , Statement : [ { Sid : ec2createtagsexisting , Effect : Allow , Action : ec2:CreateTags , Resource : * , Condition : { StringEquals : { ec2:ResourceTag/Team : Alpha }, ForAllValues:StringEquals : { aws:TagKeys : [ Team , Name ] }, StringEqualsIfExists : { aws:RequestTag/Team : Alpha } } } ] }","title":"1.3 Create policy named ec2-create-tags-existing"},{"location":"Security/300 - IAM Tag Based Access Control for EC2/Lab Guide/#14-create-policy-named-ec2-run-instances","text":"This first section of this policy allows instances to be launched, only if the conditions of region and specific tag keys are matched. The second section allows other resources to be created at instance launch time with region condition. 1. Create a managed policy using the JSON policy below and name of ec2-run-instances . { Version : 2012-10-17 , Statement : [ { Sid : ec2runinstances , Effect : Allow , Action : ec2:RunInstances , Resource : arn:aws:ec2:*:*:instance/* , Condition : { StringEquals : { aws:RequestedRegion : [ us-east-1 , us-west-1 ], aws:RequestTag/Team : Alpha }, ForAllValues:StringEquals : { aws:TagKeys : [ Name , Team ] } } }, { Sid : ec2runinstancesother , Effect : Allow , Action : ec2:RunInstances , Resource : [ arn:aws:ec2:*:*:subnet/* , arn:aws:ec2:*:*:key-pair/* , arn:aws:ec2:*::snapshot/* , arn:aws:ec2:*:*:launch-template/* , arn:aws:ec2:*:*:volume/* , arn:aws:ec2:*:*:security-group/* , arn:aws:ec2:*:*:placement-group/* , arn:aws:ec2:*:*:network-interface/* , arn:aws:ec2:*::image/* ], Condition : { StringEquals : { aws:RequestedRegion : [ us-east-1 , us-west-1 ] } } } ] }","title":"1.4 Create policy named ec2-run-instances"},{"location":"Security/300 - IAM Tag Based Access Control for EC2/Lab Guide/#15-create-policy-named-ec2-manage-instances","text":"This policy allows reboot, terminate, start and stop of instances, with a condition of the key Team is Alpha and region. 1. Create a managed policy using the JSON policy below and name of ec2-manage-instances . { Version : 2012-10-17 , Statement : [ { Sid : ec2manageinstances , Effect : Allow , Action : [ ec2:RebootInstances , ec2:TerminateInstances , ec2:StartInstances , ec2:StopInstances ], Resource : * , Condition : { StringEquals : { ec2:ResourceTag/Team : Alpha , aws:RequestedRegion : [ us-east-1 , us-west-1 ] } } } ] }","title":"1.5 Create policy named ec2-manage-instances"},{"location":"Security/300 - IAM Tag Based Access Control for EC2/Lab Guide/#2-create-role","text":"Create a role for EC2 administrators, and attach the managed policies previously created. 1. Sign in to the AWS Management Console as an IAM user with MFA enabled that can assume roles in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . 2. In the navigation pane, click Roles and then click Create role . 3. Click Another AWS account, then enter the account ID of the account you are using now and tick Require MFA, then click Next: Permissions . We enforce MFA here as it is a best practice. 4. In the search field start typing ec2- then check the box next to the policies you just created: ec2-create-tags , ec2-create-tags-existing , ec2-list-read , ec2-manage-instances , ec2-run-instances . and then click Next: Tags . 5. For this lab we will not use IAM tags, click Next: Review . 6. Enter the name of ec2-admin-team-alpha for the Role name and click Create role . 8. Check the role you have created by clicking on ec2-admin-team-alpha in the list. Record both the Role ARN and the link to the console. 9. The role is now created, ready to test!","title":"2. Create Role "},{"location":"Security/300 - IAM Tag Based Access Control for EC2/Lab Guide/#3-test-role","text":"","title":"3. Test Role"},{"location":"Security/300 - IAM Tag Based Access Control for EC2/Lab Guide/#31-assume-ec2-admin-team-alpha-role","text":"Now you will use an existing IAM user with MFA enabled to assume the new ec2-admin-team-alpha role. 1. Sign in to the AWS Management Console as an IAM user with MFA enabled. https://console.aws.amazon.com . 2. In the console, click your user name on the navigation bar in the upper right. It typically looks like this: username@account_ID_number_or_alias then click Switch Role . Alternatively you can paste the link in your browser that you recorded earlier. 3. On the Switch Role page, type the account ID number or the account alias and the name of the role ec2-admin-team-alpha that you created in the previous step. (Optional) Type text that you want to appear on the navigation bar in place of your user name when this role is active. A name is suggested, based on the account and role information, but you can change it to whatever has meaning for you. You can also select a color to highlight the display name. 4. Click Switch Role . If this is the first time choosing this option, a page appears with more information. After reading it, click Switch Role. If you clear your browser cookies, this page can appear again. 5. The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you replacing the permission that you had as the IAM user. **Tip** The last several roles that you used appear on the menu. The next time you need to switch to one of those roles, you can simply click the role you want. You only need to type the account and role information manually if the role is not displayed on the Identity menu.","title":"3.1 Assume ec2-admin-team-alpha Role"},{"location":"Security/300 - IAM Tag Based Access Control for EC2/Lab Guide/#32-launch-instance-with-without-tags","text":"Navigate to the EC2 Management Console in the us-east-2 (Ohio) region https://us-east-2.console.aws.amazon.com/ec2/v2/home?region=us-east-2 . The EC2 Dashboard should display a list of errors including You are not authorized . This is the first test passed, as us-east-2 region is not allowed. Navigate to the EC2 Management Console in the us-east-1 (North Virginia) region https://us-east-1.console.aws.amazon.com/ec2/v2/home?region=us-east-1 . The EC2 Dashboard should display a summary list of resources with the only error being Error retrieving resource count from Elastic Load Balancing as that requires additional permissions. Click Launch Instance button to start the wizard. Click Select next to the first Amazon Linux 2 Amazon Machine Image to launch. Accept the default instance size by clicking Next: Configure Instance Details . Accept default details by clicking Next: Add Storage . Accept default storage options by clicking Next: Add Tags . Lets add an incorrect tag now that will fail to launch. Click Add Tag enter Key of Name and Value of Example . Repeat to add Key of Team and Value of Beta . Note: Keys and values are case sensitive! Click Next: Configure Security Group . Click Select an existing security group , click the check box next to security group with name default , then click Review and Launch . Click Launch then click the option to Proceed without a key pair . Tick the I acknowledge box then click Launch Instances . The launch should fail, if it succeeded verify the role you are using and the managed roles you have attached as per previous steps. Click Back to Review Screen then click Edit tags to modify the tags. Change the Team key to a value of Alpha which matches the IAM policy previously created then click Review and Launch . On the review launch page once again click Launch then click the option to Proceed without a key pair . Tick the I acknowledge box then click Launch Instances . You should see a message that the instance is now launching. Click View Instances and do not terminate it just yet.","title":"3.2 Launch Instance With &amp; Without Tags"},{"location":"Security/300 - IAM Tag Based Access Control for EC2/Lab Guide/#33-modify-tags-on-instances","text":"Continuing from 3.2 in the EC2 Management Console instances view, click the check box next to the instance named Example then the Tags tab. Click Add/Edit Tags , try changing the Team key to a value of Test then click Save . An error message should appear. Change the Team key back to Alpha, and edit the Name key to a value of Test and click Save . The request should succeed.","title":"3.3 Modify Tags On Instances"},{"location":"Security/300 - IAM Tag Based Access Control for EC2/Lab Guide/#34-manage-instances","text":"Continuing from 3.3 in the EC2 Management Console instances view, click the check box next to the instance named Test . Click Actions button then expand out Instance State then Terminate . Check the instance is the one you wish to terminate by it's name and click Yes, Terminate . The instance should now terminate. Congratulations! You have now learnt about IAM permission boundaries and have one working!","title":"3.4 Manage Instances"},{"location":"Security/300 - IAM Tag Based Access Control for EC2/Lab Guide/#4-tear-down-this-lab","text":"Please note that the changes you made to the policies and roles have no charges associated with them. 1. Using the original IAM user, select the ec2-admin-team-alpha role in the IAM console at https://console.aws.amazon.com/iam/ and click Delete role . 2. For each of the policies you created, one at a time select the radio button then Policy actions drop down menu then Delete . The policies created are: ec2-create-tags ec2-create-tags-existing ec2-list-read ec2-manage-instances ec2-run-instances","title":"4. Tear down this lab "},{"location":"Security/300 - IAM Tag Based Access Control for EC2/Lab Guide/#references-useful-resources","text":"AWS Identity and Access Management User Guide IAM Best Practices and Use Cases Become an IAM Policy Master in 60 Minutes or Less Actions, Resources, and Condition Keys for Identity And Access Management","title":"References &amp; useful resources:"},{"location":"Security/300 - IAM Tag Based Access Control for EC2/Lab Guide/#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"}]}